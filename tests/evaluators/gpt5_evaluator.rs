use anyhow::{Result, Context};
use serde::{Deserialize, Serialize};
use reqwest::Client;
use tokio::time::{timeout, Duration};
use super::super::integration::human_like_testing::{TestResult, TestScenario};

/// GPT-5 nano API Evaluator - –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ MAGRAY CLI —á–µ—Ä–µ–∑ OpenAI API
pub struct Gpt5Evaluator {
    client: Client,
    api_key: String,
    model: String,
    base_url: String,
    timeout_duration: Duration,
}

/// –†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ü–µ–Ω–∫–∏ GPT-5 nano
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EvaluationResult {
    pub scenario_id: String,
    pub overall_score: f64,
    pub scores: EvaluationScores,
    pub feedback: EvaluationFeedback,
    pub metadata: EvaluationMetadata,
}

/// –î–µ—Ç–∞–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EvaluationScores {
    pub relevance: f64,           // –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ (1-10)
    pub technical_accuracy: f64,  // –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (1-10)
    pub completeness: f64,        // –ü–æ–ª–Ω–æ—Ç–∞ —Ä–µ—à–µ–Ω–∏—è (1-10)
    pub practicality: f64,        // –ü—Ä–∞–∫—Ç–∏—á–Ω–æ—Å—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π (1-10)
    pub overall_quality: f64,     // –û–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ (1-10)
}

/// –†–∞–∑–≤–µ—Ä–Ω—É—Ç–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –æ—Ç GPT-5
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EvaluationFeedback {
    pub strengths: Vec<String>,    // –°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –æ—Ç–≤–µ—Ç–∞
    pub weaknesses: Vec<String>,   // –°–ª–∞–±—ã–µ –º–µ—Å—Ç–∞
    pub suggestions: Vec<String>,  // –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
    pub summary: String,           // –ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –æ—Ü–µ–Ω–∫–∏
}

/// –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EvaluationMetadata {
    pub evaluation_time_ms: u64,
    pub model_used: String,
    pub prompt_tokens: Option<u32>,
    pub completion_tokens: Option<u32>,
    pub timestamp: String,
}

/// –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è API –∑–∞–ø—Ä–æ—Å–∞ –∫ OpenAI
#[derive(Serialize)]
struct OpenAIRequest {
    model: String,
    messages: Vec<OpenAIMessage>,
    temperature: f64,
    max_tokens: u32,
    response_format: OpenAIResponseFormat,
}

#[derive(Serialize, Deserialize)]
struct OpenAIMessage {
    role: String,
    content: String,
}

#[derive(Serialize)]
struct OpenAIResponseFormat {
    #[serde(rename = "type")]
    format_type: String,
}

/// –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ –æ—Ç OpenAI API
#[derive(Deserialize)]
struct OpenAIResponse {
    choices: Vec<OpenAIChoice>,
    usage: Option<OpenAIUsage>,
}

#[derive(Deserialize)]
struct OpenAIChoice {
    message: OpenAIMessage,
}

#[derive(Deserialize)]
struct OpenAIUsage {
    prompt_tokens: u32,
    completion_tokens: u32,
    total_tokens: u32,
}

/// –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ—Ç–≤–µ—Ç–∞ –æ—Ç GPT-5
#[derive(Deserialize)]
struct GPTEvaluationResponse {
    relevance: f64,
    technical_accuracy: f64,
    completeness: f64,
    practicality: f64,
    overall_quality: f64,
    strengths: Vec<String>,
    weaknesses: Vec<String>,
    suggestions: Vec<String>,
    summary: String,
}

impl Gpt5Evaluator {
    /// –°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π GPT-5 Evaluator
    pub fn new(api_key: String) -> Self {
        Self {
            client: Client::new(),
            api_key,
            model: "gpt-4".to_string(), // –ò—Å–ø–æ–ª—å–∑—É–µ–º GPT-4 –∫–∞–∫ –ª—É—á—à—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å
            base_url: "https://api.openai.com/v1/chat/completions".to_string(),
            timeout_duration: Duration::from_secs(60),
        }
    }

    /// –ó–∞–≥—Ä—É–∂–∞–µ—Ç API –∫–ª—é—á –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
    pub fn from_env() -> Result<Self> {
        let api_key = std::env::var("OPENAI_API_KEY")
            .context("OPENAI_API_KEY environment variable not found")?;
        
        Ok(Self::new(api_key))
    }

    /// –ó–∞–≥—Ä—É–∂–∞–µ—Ç API –∫–ª—é—á –∏–∑ .env —Ñ–∞–π–ª–∞
    pub fn from_env_file<P: AsRef<std::path::Path>>(env_path: P) -> Result<Self> {
        let content = std::fs::read_to_string(env_path)?;
        
        for line in content.lines() {
            if line.starts_with("OPENAI_API_KEY=") {
                let api_key = line.trim_start_matches("OPENAI_API_KEY=").trim();
                return Ok(Self::new(api_key.to_string()));
            }
        }
        
        Err(anyhow::anyhow!("OPENAI_API_KEY not found in .env file"))
    }

    /// –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    pub fn with_model(mut self, model: String) -> Self {
        self.model = model;
        self
    }

    /// –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç timeout –¥–ª—è API –∑–∞–ø—Ä–æ—Å–æ–≤
    pub fn with_timeout(mut self, duration: Duration) -> Self {
        self.timeout_duration = duration;
        self
    }

    /// –û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å –ø–æ–º–æ—â—å—é GPT-5 nano
    pub async fn evaluate_response(
        &self, 
        scenario: &TestScenario, 
        test_result: &TestResult
    ) -> Result<EvaluationResult> {
        
        println!("ü§ñ Evaluating response for scenario: {}", scenario.name);
        
        let start_time = std::time::Instant::now();
        
        // –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        let evaluation_prompt = self.create_evaluation_prompt(scenario, test_result);
        
        // –í—ã–ø–æ–ª–Ω—è–µ–º API –∑–∞–ø—Ä–æ—Å –∫ OpenAI
        let gpt_response = self.call_openai_api(&evaluation_prompt).await?;
        
        // –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
        let evaluation_data: GPTEvaluationResponse = serde_json::from_str(&gpt_response)
            .context("Failed to parse GPT evaluation response as JSON")?;

        let execution_time = start_time.elapsed().as_millis() as u64;
        
        // –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â—É—é –æ—Ü–µ–Ω–∫—É –∫–∞–∫ —Å—Ä–µ–¥–Ω–µ–µ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–µ
        let overall_score = (
            evaluation_data.relevance +
            evaluation_data.technical_accuracy +
            evaluation_data.completeness +
            evaluation_data.practicality +
            evaluation_data.overall_quality
        ) / 5.0;

        let result = EvaluationResult {
            scenario_id: scenario.id.clone(),
            overall_score,
            scores: EvaluationScores {
                relevance: evaluation_data.relevance,
                technical_accuracy: evaluation_data.technical_accuracy,
                completeness: evaluation_data.completeness,
                practicality: evaluation_data.practicality,
                overall_quality: evaluation_data.overall_quality,
            },
            feedback: EvaluationFeedback {
                strengths: evaluation_data.strengths,
                weaknesses: evaluation_data.weaknesses,
                suggestions: evaluation_data.suggestions,
                summary: evaluation_data.summary,
            },
            metadata: EvaluationMetadata {
                evaluation_time_ms: execution_time,
                model_used: self.model.clone(),
                prompt_tokens: None, // TODO: extract from API response
                completion_tokens: None,
                timestamp: chrono::Utc::now().to_rfc3339(),
            },
        };

        println!("‚úÖ Evaluation completed. Overall score: {:.1}/10", overall_score);
        
        Ok(result)
    }

    /// –°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ—Ç–≤–µ—Ç–∞
    fn create_evaluation_prompt(&self, scenario: &TestScenario, test_result: &TestResult) -> String {
        format!(r#"
–í—ã —è–≤–ª—è–µ—Ç–µ—Å—å —ç–∫—Å–ø–µ—Ä—Ç–æ–º –ø–æ –æ—Ü–µ–Ω–∫–µ –∫–∞—á–µ—Å—Ç–≤–∞ AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤. –û—Ü–µ–Ω–∏—Ç–µ –æ—Ç–≤–µ—Ç MAGRAY CLI –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å.

## –ö–û–ù–¢–ï–ö–°–¢ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø:
–°—Ü–µ–Ω–∞—Ä–∏–π: {scenario_name}
–¢–∏–ø –∑–∞–¥–∞—á–∏: {scenario_type}
–ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏: {criteria}

## –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–¨–°–ö–ò–ô –ó–ê–ü–†–û–°:
{user_input}

## –û–¢–í–ï–¢ MAGRAY CLI:
{cli_output}

## –†–ï–ó–£–õ–¨–¢–ê–¢ –í–´–ü–û–õ–ù–ï–ù–ò–Ø:
–£—Å–ø–µ—à–Ω–æ: {success}
–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {execution_time}–º—Å
{error_info}

## –ò–ù–°–¢–†–£–ö–¶–ò–ò –ü–û –û–¶–ï–ù–ö–ï:

–û—Ü–µ–Ω–∏—Ç–µ –æ—Ç–≤–µ—Ç –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º (—à–∫–∞–ª–∞ 1-10, –≥–¥–µ 10 - –æ—Ç–ª–∏—á–Ω–æ):

1. **–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å** (relevance): –ù–∞—Å–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
2. **–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å** (technical_accuracy): –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ç–∞–ª–µ–π –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
3. **–ü–æ–ª–Ω–æ—Ç–∞ —Ä–µ—à–µ–Ω–∏—è** (completeness): –ü–æ–∫—Ä—ã–≤–∞–µ—Ç –ª–∏ –æ—Ç–≤–µ—Ç –≤—Å–µ –∞—Å–ø–µ–∫—Ç—ã –∑–∞–ø—Ä–æ—Å–∞
4. **–ü—Ä–∞–∫—Ç–∏—á–Ω–æ—Å—Ç—å** (practicality): –ú–æ–∂–Ω–æ –ª–∏ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ
5. **–û–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ** (overall_quality): –û–±—â–µ–µ –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏–µ –æ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞

–¢–∞–∫–∂–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ:
- **–°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã** (strengths): –ß—Ç–æ —Å–¥–µ–ª–∞–Ω–æ —Ö–æ—Ä–æ—à–æ
- **–°–ª–∞–±—ã–µ –º–µ—Å—Ç–∞** (weaknesses): –ß—Ç–æ –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å
- **–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è** (suggestions): –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
- **–†–µ–∑—é–º–µ** (summary): –ö—Ä–∞—Ç–∫–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ

–û—Ç–≤–µ—Ç—å—Ç–µ —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
    "relevance": 0.0,
    "technical_accuracy": 0.0,
    "completeness": 0.0,
    "practicality": 0.0,
    "overall_quality": 0.0,
    "strengths": ["—Å–∏–ª—å–Ω–∞—è —Å—Ç–æ—Ä–æ–Ω–∞ 1", "—Å–∏–ª—å–Ω–∞—è —Å—Ç–æ—Ä–æ–Ω–∞ 2"],
    "weaknesses": ["—Å–ª–∞–±–æ—Å—Ç—å 1", "—Å–ª–∞–±–æ—Å—Ç—å 2"],
    "suggestions": ["–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ 1", "–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ 2"],
    "summary": "–ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –æ—Ü–µ–Ω–∫–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ"
}}
"#,
            scenario_name = scenario.name,
            scenario_type = scenario.expected_type,
            criteria = scenario.evaluation_criteria.join(", "),
            user_input = scenario.input,
            cli_output = test_result.output,
            success = test_result.success,
            execution_time = test_result.execution_time_ms,
            error_info = test_result.error_message.as_ref()
                .map(|err| format!("–û—à–∏–±–∫–∞: {}", err))
                .unwrap_or_default()
        )
    }

    /// –í—ã–ø–æ–ª–Ω—è–µ—Ç API –∑–∞–ø—Ä–æ—Å –∫ OpenAI
    async fn call_openai_api(&self, prompt: &str) -> Result<String> {
        let request = OpenAIRequest {
            model: self.model.clone(),
            messages: vec![
                OpenAIMessage {
                    role: "system".to_string(),
                    content: "–í—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ—Ü–µ–Ω–∫–µ –∫–∞—á–µ—Å—Ç–≤–∞ AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤. –û—Ç–≤–µ—á–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON.".to_string(),
                },
                OpenAIMessage {
                    role: "user".to_string(),
                    content: prompt.to_string(),
                },
            ],
            temperature: 0.3,
            max_tokens: 2000,
            response_format: OpenAIResponseFormat {
                format_type: "json_object".to_string(),
            },
        };

        let response = timeout(
            self.timeout_duration,
            self.client
                .post(&self.base_url)
                .header("Authorization", format!("Bearer {}", self.api_key))
                .header("Content-Type", "application/json")
                .json(&request)
                .send()
        ).await
            .context("OpenAI API request timeout")?
            .context("Failed to send request to OpenAI API")?;

        if !response.status().is_success() {
            let error_text = response.text().await.unwrap_or_default();
            return Err(anyhow::anyhow!("OpenAI API error: {}", error_text));
        }

        let api_response: OpenAIResponse = response.json().await
            .context("Failed to parse OpenAI API response")?;

        if api_response.choices.is_empty() {
            return Err(anyhow::anyhow!("No choices in OpenAI API response"));
        }

        Ok(api_response.choices[0].message.content.clone())
    }

    /// –û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–∞—Ç—á–∞ —Ç–µ—Å—Ç–æ–≤
    pub async fn evaluate_test_batch(
        &self,
        scenarios: &[TestScenario],
        test_results: &[TestResult]
    ) -> Result<Vec<EvaluationResult>> {
        
        println!("üîç Starting batch evaluation of {} test results", test_results.len());
        
        let mut evaluation_results = Vec::new();
        
        for (i, test_result) in test_results.iter().enumerate() {
            // –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Å—Ü–µ–Ω–∞—Ä–∏–π
            let scenario = scenarios.iter()
                .find(|s| s.id == test_result.scenario_id)
                .context(format!("Scenario not found for result: {}", test_result.scenario_id))?;

            println!("üìä Evaluating {}/{}: {}", i + 1, test_results.len(), scenario.name);
            
            match self.evaluate_response(scenario, test_result).await {
                Ok(evaluation) => {
                    evaluation_results.push(evaluation);
                }
                Err(e) => {
                    eprintln!("‚ùå Failed to evaluate scenario '{}': {}", scenario.name, e);
                    // –°–æ–∑–¥–∞–µ–º fallback —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ
                    evaluation_results.push(EvaluationResult {
                        scenario_id: scenario.id.clone(),
                        overall_score: 0.0,
                        scores: EvaluationScores {
                            relevance: 0.0,
                            technical_accuracy: 0.0,
                            completeness: 0.0,
                            practicality: 0.0,
                            overall_quality: 0.0,
                        },
                        feedback: EvaluationFeedback {
                            strengths: vec![],
                            weaknesses: vec!["Evaluation failed due to technical error".to_string()],
                            suggestions: vec!["Review evaluation system configuration".to_string()],
                            summary: format!("Evaluation failed: {}", e),
                        },
                        metadata: EvaluationMetadata {
                            evaluation_time_ms: 0,
                            model_used: self.model.clone(),
                            prompt_tokens: None,
                            completion_tokens: None,
                            timestamp: chrono::Utc::now().to_rfc3339(),
                        },
                    });
                }
            }

            // –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –¥–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è rate limits
            tokio::time::sleep(Duration::from_millis(1000)).await;
        }

        println!("‚úÖ Batch evaluation completed: {}/{} successful", 
               evaluation_results.iter().filter(|r| r.overall_score > 0.0).count(),
               evaluation_results.len());

        Ok(evaluation_results)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_evaluator_creation() {
        let evaluator = Gpt5Evaluator::new("test_key".to_string());
        assert_eq!(evaluator.api_key, "test_key");
        assert_eq!(evaluator.model, "gpt-4");
    }

    #[test]
    fn test_prompt_creation() {
        let evaluator = Gpt5Evaluator::new("test_key".to_string());
        
        let scenario = TestScenario {
            id: "test".to_string(),
            name: "Test Scenario".to_string(),
            input: "test input".to_string(),
            expected_type: "simple_response".to_string(),
            timeout_seconds: 30,
            evaluation_criteria: vec!["test_criterion".to_string()],
        };
        
        let test_result = TestResult {
            scenario_id: "test".to_string(),
            input: "test input".to_string(),
            output: "test output".to_string(),
            execution_time_ms: 1000,
            success: true,
            error_message: None,
            timestamp: "2025-01-01T00:00:00Z".to_string(),
        };
        
        let prompt = evaluator.create_evaluation_prompt(&scenario, &test_result);
        assert!(prompt.contains("test input"));
        assert!(prompt.contains("test output"));
        assert!(prompt.contains("Test Scenario"));
    }
}