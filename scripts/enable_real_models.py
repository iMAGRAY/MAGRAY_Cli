#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö ONNX –º–æ–¥–µ–ª–µ–π –≤–º–µ—Å—Ç–æ –º–æ–∫–æ–≤ –≤ MAGRAY CLI.
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏—Ç –∏ –∞–∫—Ç–∏–≤–∏—Ä—É–µ—Ç –∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥ –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏.
"""

import os
import re
import sys
from pathlib import Path
from typing import List, Tuple

class ModelEnabler:
    def __init__(self, verbose: bool = True):
        self.verbose = verbose
        self.modified_files = []
        
    def log(self, message: str):
        """–í—ã–≤–æ–¥–∏—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω verbose —Ä–µ–∂–∏–º."""
        if self.verbose:
            print(message)
    
    def uncomment_onnx_code(self, file_path: Path) -> bool:
        """–†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–µ—Ç –∫–æ–¥ —Å–≤—è–∑–∞–Ω–Ω—ã–π —Å ONNX Runtime."""
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        modified = False
        new_lines = []
        in_commented_block = False
        
        for i, line in enumerate(lines):
            # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ ONNX –∫–æ–¥–∞
            if '// Real ONNX' in line or '// TODO: Enable real' in line:
                in_commented_block = True
            
            # –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫–∏ —Å ONNX –≤—ã–∑–æ–≤–∞–º–∏
            if in_commented_block and line.strip().startswith('//'):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –Ω–µ –æ–±—ã—á–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
                uncommented = line.replace('//', '', 1)
                if any(keyword in uncommented for keyword in 
                       ['onnx', 'OrtSession', 'run(', 'tensor', 'ndarray']):
                    new_lines.append(uncommented)
                    modified = True
                    self.log(f"  Uncommented line {i+1}: {uncommented.strip()}")
                else:
                    new_lines.append(line)
            else:
                new_lines.append(line)
                
            if in_commented_block and line.strip() == '':
                in_commented_block = False
        
        if modified:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.writelines(new_lines)
            self.modified_files.append(file_path)
            
        return modified
    
    def fix_mock_returns(self, file_path: Path) -> bool:
        """–ó–∞–º–µ–Ω—è–µ—Ç mock –≤–æ–∑–≤—Ä–∞—Ç—ã –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –≤—ã–∑–æ–≤—ã."""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        original_content = content
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∑–∞–º–µ–Ω—ã
        replacements = [
            # Mock embeddings
            (r'vec!\[0\.5; 1024\]', 'self.generate_real_embedding(text).await?'),
            (r'vec!\[0\.1; 1024\]', 'self.generate_real_embedding(text).await?'),
            (r'// Mock embedding generation', '// Real embedding generation'),
            
            # Mock reranking
            (r'scores: vec!\[0\.9, 0\.7, 0\.5\]', 'scores: self.rerank_real(query, documents).await?'),
            (r'// Mock reranking', '// Real reranking'),
            
            # Enable ONNX session creation
            (r'// let session = .*OrtSession', 'let session = OrtSession'),
            (r'//\s*self\.session = Some\(', 'self.session = Some('),
            
            # Remove skip attributes from tests
            (r'#\[ignore\].*// Requires real model', ''),
            (r'#\[cfg\(feature = "real_models"\)\]', ''),
        ]
        
        for pattern, replacement in replacements:
            content = re.sub(pattern, replacement, content)
        
        if content != original_content:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            self.modified_files.append(file_path)
            return True
            
        return False
    
    def update_cargo_toml(self, file_path: Path) -> bool:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç Cargo.toml –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π."""
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        new_lines = []
        modified = False
        
        for line in lines:
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Ä—Å–∏—é onnxruntime –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
            if 'onnxruntime' in line and '#' in line:
                # –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–µ–º
                new_lines.append(line.split('#')[0].rstrip() + '\n')
                modified = True
                self.log(f"  Enabled onnxruntime dependency")
            elif 'onnxruntime' in line and '0.0.14' in line:
                # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Ä—Å–∏—é
                new_lines.append(line.replace('0.0.14', '0.1.16'))
                modified = True
                self.log(f"  Updated onnxruntime version to 0.1.16")
            else:
                new_lines.append(line)
        
        # –î–æ–±–∞–≤–ª—è–µ–º feature flags –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        if modified:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Å–µ–∫—Ü–∏—è features
            has_features = any('[features]' in line for line in lines)
            if not has_features:
                new_lines.append('\n[features]\n')
                new_lines.append('default = ["cpu"]\n')
                new_lines.append('cpu = ["onnxruntime"]\n')
                new_lines.append('gpu = ["onnxruntime/cuda"]\n')
                self.log("  Added feature flags for CPU/GPU modes")
        
        if modified:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.writelines(new_lines)
            self.modified_files.append(file_path)
            
        return modified
    
    def create_model_loader_fixes(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–¥ –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è model_loader.rs"""
        return '''use anyhow::{Result, Context};
use std::path::{Path, PathBuf};
use onnxruntime::{environment::Environment, session::Session, GraphOptimizationLevel};
use std::sync::Arc;

pub struct ModelLoader {
    env: Arc<Environment>,
    models_dir: PathBuf,
}

impl ModelLoader {
    pub fn new(models_dir: impl AsRef<Path>) -> Result<Self> {
        let env = Environment::builder()
            .with_name("magray")
            .with_log_level(onnxruntime::LoggingLevel::Warning)
            .build()
            .context("Failed to create ONNX Runtime environment")?;
            
        Ok(Self {
            env: Arc::new(env),
            models_dir: models_dir.as_ref().to_path_buf(),
        })
    }
    
    pub fn load_embedding_model(&self, model_name: &str) -> Result<Session> {
        let model_path = self.models_dir.join(model_name).join("model.onnx");
        
        if !model_path.exists() {
            anyhow::bail!("Model file not found: {:?}. Run download_models.ps1 first", model_path);
        }
        
        Session::builder()
            .with_env(&self.env)
            .with_optimization_level(GraphOptimizationLevel::All)
            .with_intra_threads(4)
            .with_model_from_file(&model_path)
            .context(format!("Failed to load model: {}", model_name))
    }
    
    pub fn load_reranker_model(&self, model_name: &str) -> Result<Session> {
        self.load_embedding_model(model_name) // Same loading logic
    }
}
'''
    
    def fix_embedding_service(self, file_path: Path) -> bool:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è embeddings_bge_m3.rs"""
        if not file_path.exists():
            return False
            
        content = self.create_real_embedding_code()
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        self.modified_files.append(file_path)
        self.log(f"‚úÖ Replaced {file_path} with real implementation")
        return True
    
    def create_real_embedding_code(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–π –∫–æ–¥ –¥–ª—è embedding service."""
        return '''use anyhow::{Result, Context};
use ndarray::{Array1, Array2, ArrayView1};
use onnxruntime::{session::Session, tensor::OrtOwnedTensor};
use std::sync::Arc;
use tokio::sync::RwLock;
use tokenizers::Tokenizer;

use crate::{AiError, ModelLoader};

pub struct BgeM3EmbeddingService {
    session: Arc<Session>,
    tokenizer: Arc<Tokenizer>,
    cache: Arc<RwLock<lru::LruCache<String, Vec<f32>>>>,
}

impl BgeM3EmbeddingService {
    pub async fn new(model_path: &str) -> Result<Self> {
        // –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        let loader = ModelLoader::new("models")?;
        let session = Arc::new(loader.load_embedding_model("bge-m3")?);
        
        // –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        let tokenizer_path = format!("{}/tokenizer.json", model_path);
        let tokenizer = Tokenizer::from_file(&tokenizer_path)
            .map_err(|e| AiError::ModelLoadError(e.to_string()))?;
        
        // –°–æ–∑–¥–∞–µ–º –∫—ç—à
        let cache = Arc::new(RwLock::new(lru::LruCache::new(
            std::num::NonZeroUsize::new(1000).unwrap()
        )));
        
        Ok(Self {
            session,
            tokenizer: Arc::new(tokenizer),
            cache,
        })
    }
    
    pub async fn embed(&self, text: &str) -> Result<Vec<f32>> {
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        {
            let cache = self.cache.read().await;
            if let Some(embedding) = cache.get(text) {
                return Ok(embedding.clone());
            }
        }
        
        // –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        let encoding = self.tokenizer
            .encode(text, true)
            .map_err(|e| AiError::TokenizationError(e.to_string()))?;
        
        let input_ids = encoding.get_ids();
        let attention_mask = encoding.get_attention_mask();
        
        // –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–Ω–∑–æ—Ä–æ–≤
        let input_ids_array = Array2::from_shape_vec(
            (1, input_ids.len()),
            input_ids.iter().map(|&id| id as i64).collect(),
        )?;
        
        let attention_mask_array = Array2::from_shape_vec(
            (1, attention_mask.len()),
            attention_mask.iter().map(|&m| m as i64).collect(),
        )?;
        
        // –ó–∞–ø—É—Å–∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏
        let outputs = self.session.run(vec![
            input_ids_array.into(),
            attention_mask_array.into(),
        ])?;
        
        // –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        let output: OrtOwnedTensor<f32, _> = outputs[0].try_extract()?;
        let embedding = output.view().as_slice()
            .ok_or_else(|| AiError::ProcessingError("Failed to extract embedding".into()))?
            .to_vec();
        
        // –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        let norm = embedding.iter().map(|x| x * x).sum::<f32>().sqrt();
        let normalized = embedding.iter().map(|x| x / norm).collect::<Vec<_>>();
        
        // –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        {
            let mut cache = self.cache.write().await;
            cache.put(text.to_string(), normalized.clone());
        }
        
        Ok(normalized)
    }
    
    pub async fn embed_batch(&self, texts: &[String]) -> Result<Vec<Vec<f32>>> {
        let mut results = Vec::with_capacity(texts.len());
        
        // TODO: Implement proper batching for efficiency
        for text in texts {
            results.push(self.embed(text).await?);
        }
        
        Ok(results)
    }
}
'''
    
    def process_ai_crate(self):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç crate ai/ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
        ai_path = Path('crates/ai/src')
        
        if not ai_path.exists():
            self.log(f"‚ùå AI crate not found at {ai_path}")
            return
            
        # –§–∞–π–ª—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        files_to_process = [
            'embeddings_bge_m3.rs',
            'embeddings_cpu.rs', 
            'embeddings_gpu.rs',
            'reranking.rs',
            'reranker_qwen3.rs',
            'model_loader.rs',
        ]
        
        for file_name in files_to_process:
            file_path = ai_path / file_name
            if file_path.exists():
                self.log(f"\nüìÑ Processing {file_path}")
                
                if 'model_loader' in file_name:
                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π model_loader
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(self.create_model_loader_fixes())
                    self.modified_files.append(file_path)
                    self.log("  ‚úÖ Created real model loader implementation")
                    
                elif 'embeddings_bge_m3' in file_name:
                    # –ó–∞–º–µ–Ω—è–µ–º –Ω–∞ —Ä–µ–∞–ª—å–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é
                    self.fix_embedding_service(file_path)
                    
                else:
                    # –û–±—ã—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
                    if self.uncomment_onnx_code(file_path):
                        self.log(f"  ‚úÖ Uncommented ONNX code")
                    if self.fix_mock_returns(file_path):
                        self.log(f"  ‚úÖ Fixed mock returns")
    
    def update_all_cargo_tomls(self):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –≤—Å–µ Cargo.toml —Ñ–∞–π–ª—ã –≤ –ø—Ä–æ–µ–∫—Ç–µ."""
        for root, dirs, files in os.walk('crates'):
            for file in files:
                if file == 'Cargo.toml':
                    cargo_path = Path(root) / file
                    self.log(f"\nüì¶ Checking {cargo_path}")
                    if self.update_cargo_toml(cargo_path):
                        self.log(f"  ‚úÖ Updated dependencies")
    
    def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –≤—Å–µ—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π."""
        print("üöÄ Enabling real ONNX models in MAGRAY CLI\n")
        
        # 1. –û–±—Ä–∞–±–æ—Ç–∫–∞ AI crate
        print("Step 1: Processing AI crate...")
        self.process_ai_crate()
        
        # 2. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Cargo.toml —Ñ–∞–π–ª–æ–≤
        print("\nStep 2: Updating Cargo.toml files...")
        self.update_all_cargo_tomls()
        
        # 3. –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\n‚úÖ Summary:")
        print(f"  Modified {len(self.modified_files)} files:")
        for file in self.modified_files:
            print(f"    - {file}")
        
        print("\nüìù Next steps:")
        print("  1. Run: ./scripts/download_models.ps1")
        print("  2. Run: cargo build --release --features cpu")
        print("  3. Run: cargo test --features cpu")

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='Enable real ONNX models in MAGRAY CLI')
    parser.add_argument('--dry-run', action='store_true', 
                       help='Show what would be changed without modifying files')
    parser.add_argument('--verbose', '-v', action='store_true', default=True,
                       help='Verbose output')
    
    args = parser.parse_args()
    
    if args.dry_run:
        print("DRY RUN MODE - No files will be modified\n")
    
    enabler = ModelEnabler(verbose=args.verbose)
    enabler.run()

if __name__ == '__main__':
    main()