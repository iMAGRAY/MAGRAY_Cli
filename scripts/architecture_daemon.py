#!/usr/bin/env python3
"""
–£–ª—å—Ç—Ä–∞–∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π –¥–µ–º–æ–Ω –¥–ª—è –∞–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–∏–Ω–∏–º–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π Mermaid –¥–∏–∞–≥—Ä–∞–º–º—ã

–¶–ï–õ–¨: –ó–∞–º–µ–Ω–∏—Ç—å –æ–≥—Ä–æ–º–Ω—ã–µ CTL –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –æ–¥–Ω–æ–π –∫—Ä–∞—Ç–∫–æ–π, –Ω–æ —Ç–æ—á–Ω–æ–π Mermaid –¥–∏–∞–≥—Ä–∞–º–º–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã.

–ê–≤—Ç–æ—Ä: AI Architecture Daemon
–í–µ—Ä—Å–∏—è: 1.0
"""

import os
import json
import time
import argparse
import toml
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Set, Tuple, Optional
import re
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import threading
import sys
import ast
import hashlib
from collections import defaultdict, Counter
import math

# Tree-sitter imports –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ AST –ø–∞—Ä—Å–∏–Ω–≥–∞
try:
    import tree_sitter
    from tree_sitter_rust import language as rust_language
    TREE_SITTER_AVAILABLE = True
except ImportError:
    TREE_SITTER_AVAILABLE = False
    print("[WARNING] tree-sitter –∏–ª–∏ tree-sitter-rust –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback –Ω–∞ regex –ø–∞—Ä—Å–∏–Ω–≥.")

class ArchitectureDaemon:
    """–£–ª—å—Ç—Ä–∞–∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –¥–µ–º–æ–Ω –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Mermaid –¥–∏–∞–≥—Ä–∞–º–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.crates_dir = self.project_root / "crates"
        self.claude_md = self.project_root / "CLAUDE.md"
        self.dependencies: Dict[str, Set[str]] = {}
        self.features: Dict[str, List[str]] = {}
        self.file_structures: Dict[str, Dict] = {}  # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–æ–≤
        self.imports_map: Dict[str, Set[str]] = {}  # –ö–∞—Ä—Ç–∞ –∏–º–ø–æ—Ä—Ç–æ–≤ –º–µ–∂–¥—É —Ñ–∞–π–ª–∞–º–∏
        self.duplicates: Dict[str, List[Tuple[str, str]]] = {}  # –ö–∞—Ä—Ç–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        self.mocks_registry: Dict[str, List[str]] = {}  # –†–µ–µ—Å—Ç—Ä –≤—Å–µ—Ö –º–æ–∫–æ–≤
        self.test_utilities: Dict[str, List[str]] = {}  # Test helpers –∏ builders
        self.complexity_metrics: Dict[str, Dict] = {}  # –ú–µ—Ç—Ä–∏–∫–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        self.tech_debt: List[Dict] = []  # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –¥–æ–ª–≥
        self.dependency_graph: Dict[str, Set[str]] = {}  # –ì—Ä–∞—Ñ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        self.file_cache: Dict[str, Dict] = {}  # –ö—ç—à –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        self.file_hashes: Dict[str, str] = {}  # –•—ç—à–∏ —Ñ–∞–π–ª–æ–≤
        self.architectural_issues: Dict[str, List[Dict]] = {}  # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
        self.ast_cache: Dict[str, tuple] = {}  # –ö—ç—à AST –¥–µ—Ä–µ–≤—å–µ–≤ (hash -> (tree, result))
        self.circular_deps: List[List[str]] = []  # –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º tree-sitter –ø–∞—Ä—Å–µ—Ä –¥–ª—è Rust
        self.rust_parser = None
        if TREE_SITTER_AVAILABLE:
            try:
                # –°–æ–∑–¥–∞–µ–º Language –æ–±—ä–µ–∫—Ç –∏ –ø–∞—Ä—Å–µ—Ä
                lang = tree_sitter.Language(rust_language())
                self.rust_parser = tree_sitter.Parser(lang)
                print("[INFO] Tree-sitter –ø–∞—Ä—Å–µ—Ä –¥–ª—è Rust –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                print(f"[WARNING] –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å tree-sitter: {e}")
                self.rust_parser = None
        
    def scan_architecture(self) -> Dict[str, any]:
        """–°–∫–∞–Ω–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞ –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–µ–∞–ª—å–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É"""
        print("[INFO] –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞...")
        
        architecture = {
            "crates": {},
            "dependencies": {},
            "features": {},
            "file_structures": {},
            "imports": {},
            "mermaid": ""
        }
        
        # –°–∫–∞–Ω–∏—Ä—É–µ–º –≤—Å–µ crates
        for cargo_toml in self.crates_dir.rglob("Cargo.toml"):
            crate_name = cargo_toml.parent.name
            try:
                with open(cargo_toml, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # –£–±–∏—Ä–∞–µ–º BOM –µ—Å–ª–∏ –µ—Å—Ç—å
                    if content.startswith('\ufeff'):
                        content = content[1:]
                    cargo_data = toml.loads(content)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
                deps = set()
                if 'dependencies' in cargo_data:
                    for dep_name, dep_config in cargo_data['dependencies'].items():
                        if isinstance(dep_config, dict) and 'path' in dep_config:
                            # –õ–æ–∫–∞–ª—å–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å
                            path_parts = dep_config['path'].split('/')
                            if len(path_parts) >= 2 and path_parts[-2] == '..':
                                deps.add(path_parts[-1])
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º features
                features = []
                if 'features' in cargo_data:
                    features = list(cargo_data['features'].keys())
                
                # –°–∫–∞–Ω–∏—Ä—É–µ–º Rust —Ñ–∞–π–ª—ã –≤ –∫—Ä–µ–π—Ç–µ
                rust_files = self._scan_rust_files(cargo_toml.parent)
                
                architecture["crates"][crate_name] = {
                    "path": str(cargo_toml.parent),
                    "description": self._get_crate_description(crate_name),
                    "files": rust_files
                }
                architecture["dependencies"][crate_name] = list(deps)
                architecture["features"][crate_name] = features
                architecture["file_structures"][crate_name] = rust_files
                
                print(f"  [OK] {crate_name}: {len(deps)} deps, {len(features)} features, {len(rust_files)} files")
                
            except Exception as e:
                print(f"  [ERROR] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {cargo_toml}: {e}")
                continue
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º Mermaid –¥–∏–∞–≥—Ä–∞–º–º—É
        architecture["mermaid"] = self._generate_mermaid(architecture)
        
        return architecture
    
    def _get_crate_description(self, crate_name: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ crate"""
        descriptions = {
            "cli": "CLI Agent & Commands",
            "memory": "3-Layer HNSW Memory",
            "ai": "AI/ONNX Models & GPU",
            "llm": "Multi-Provider LLM",
            "router": "Smart Task Router",
            "tools": "Tools Registry",
            "common": "Common Utilities",
            "todo": "Task DAG System"
        }
        return descriptions.get(crate_name, f"{crate_name.title()} Crate")
    
    def _calculate_tech_debt(self) -> List[Dict]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –¥–æ–ª–≥ –≤ —á–µ–ª–æ–≤–µ–∫–æ-—á–∞—Å–∞—Ö"""
        debt_items = []
        
        for file_path, metrics in self.complexity_metrics.items():
            cyclomatic = metrics.get('cyclomatic', 0)
            cognitive = metrics.get('cognitive', 0)
            god_score = metrics.get('god_object_score', 0)
            
            # –í—ã—Å–æ–∫–∞—è —Ü–∏–∫–ª–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
            if cyclomatic > 20:
                hours = min((cyclomatic - 20) * 0.5, 16)  # –ú–∞–∫—Å–∏–º—É–º 16 —á–∞—Å–æ–≤ –Ω–∞ —Ñ–∞–π–ª
                debt_items.append({
                    'file': file_path,
                    'type': 'high_cyclomatic_complexity',
                    'severity': 'critical' if cyclomatic > 30 else 'high',
                    'current_value': cyclomatic,
                    'target_value': 10,
                    'estimated_hours': hours,
                    'description': f'–¶–∏–∫–ª–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å {cyclomatic} (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å < 10)'
                })
            
            # –í—ã—Å–æ–∫–∞—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å  
            if cognitive > 30:
                hours = min((cognitive - 30) * 0.25, 12)  # –ú–∞–∫—Å–∏–º—É–º 12 —á–∞—Å–æ–≤ –Ω–∞ —Ñ–∞–π–ª
                debt_items.append({
                    'file': file_path,
                    'type': 'high_cognitive_complexity',
                    'severity': 'high' if cognitive > 50 else 'medium',
                    'current_value': cognitive,
                    'target_value': 15,
                    'estimated_hours': hours,
                    'description': f'–ö–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å {cognitive} (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å < 15)'
                })
            
            # God Object
            if god_score > 0.7:
                debt_items.append({
                    'file': file_path,
                    'type': 'god_object',
                    'severity': 'critical',
                    'current_value': god_score,
                    'target_value': 0.3,
                    'estimated_hours': 16,  # 2 –¥–Ω—è –Ω–∞ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—é
                    'description': f'God Object –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å {god_score:.0%}'
                })
        
        # –î—É–±–ª–∏–∫–∞—Ç—ã –∫–æ–¥–∞
        duplicates = self._analyze_duplicates()
        for sig, locations in duplicates.items():
            if len(locations) > 3:
                debt_items.append({
                    'file': ', '.join([loc[0] for loc in locations[:3]]),
                    'type': 'code_duplication',
                    'severity': 'medium',
                    'current_value': len(locations),
                    'target_value': 1,
                    'estimated_hours': len(locations) * 2,
                    'description': f'–î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ "{sig}" –≤ {len(locations)} –º–µ—Å—Ç–∞—Ö'
                })
        
        return sorted(debt_items, key=lambda x: ('critical', 'high', 'medium').index(x['severity']))
    
    def _should_analyze_file(self, file_path: Path) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª (–∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑)"""
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º —Ö—ç—à —Ñ–∞–π–ª–∞
            with open(file_path, 'rb') as f:
                current_hash = hashlib.md5(f.read()).hexdigest()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
            file_key = str(file_path)
            if file_key in self.file_hashes:
                if self.file_hashes[file_key] == current_hash:
                    # –§–∞–π–ª –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à
                    return False
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–π —Ö—ç—à
            self.file_hashes[file_key] = current_hash
            return True
            
        except Exception:
            return True  # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º
    
    def _build_dependency_graph(self, crate_name: str, uses: List[str], file_path: str = None):
        """–°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –º–µ–∂–¥—É –º–æ–¥—É–ª—è–º–∏ —Å —É—á–µ—Ç–æ–º AST –¥–∞–Ω–Ω—ã—Ö"""
        source = file_path if file_path else f"{crate_name}"
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –∏ –≤–Ω–µ—à–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
        local_deps = set()
        external_deps = set()
        
        for use_stmt in uses:
            use_clean = use_stmt.strip()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º—ã–π –º–æ–¥—É–ª—å
            if 'crate::' in use_clean:
                parts = use_clean.replace('crate::', '').split('::')
                if parts:
                    # –õ–æ–∫–∞–ª—å–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –≤–Ω—É—Ç—Ä–∏ –∫—Ä–µ–π—Ç–∞
                    local_deps.add(parts[0])
                    target = f"{crate_name}/{parts[0]}"
                    if source not in self.dependency_graph:
                        self.dependency_graph[source] = set()
                    self.dependency_graph[source].add(target)
                    
            elif 'super::' in use_clean:
                # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –º–æ–¥—É–ª—è
                parts = use_clean.replace('super::', '').split('::')
                if parts:
                    local_deps.add(f"../{parts[0]}")
                    
            elif 'self::' in use_clean:
                # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ –º–æ–¥—É–ª—è
                parts = use_clean.replace('self::', '').split('::')
                if parts:
                    local_deps.add(f"./{parts[0]}")
                    
            elif '::' in use_clean:
                # –í–Ω–µ—à–Ω—è—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å
                parts = use_clean.split('::')
                if parts:
                    crate_name_dep = parts[0]
                    external_deps.add(crate_name_dep)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –≥—Ä–∞—Ñ
                    if source not in self.dependency_graph:
                        self.dependency_graph[source] = set()
                    self.dependency_graph[source].add(crate_name_dep)
            else:
                # –ü—Ä–æ—Å—Ç–æ–π –∏–º–ø–æ—Ä—Ç (std –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω—ã–π –º–æ–¥—É–ª—å)
                if use_clean in ['std', 'core', 'alloc']:
                    external_deps.add(use_clean)
                else:
                    local_deps.add(use_clean)
        
        # –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
        self._detect_circular_deps(source)
        
        return local_deps, external_deps
    
    def _detect_circular_deps(self, source: str):
        """–î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        if source not in self.dependency_graph:
            return
        
        visited = set()
        stack = []
        
        def dfs(node, path):
            if node in path:
                # –ù–∞—à–ª–∏ —Ü–∏–∫–ª
                cycle = path[path.index(node):] + [node]
                if len(cycle) > 1:
                    self.circular_deps.append(cycle)
                return
            
            if node in visited:
                return
                
            visited.add(node)
            path.append(node)
            
            if node in self.dependency_graph:
                for neighbor in self.dependency_graph[node]:
                    dfs(neighbor, path[:])
        
        dfs(source, [])
    
    def _detect_circular_dependencies(self) -> List[List[str]]:
        """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤ –≥—Ä–∞—Ñ–µ"""
        cycles = []
        visited = set()
        rec_stack = set()
        
        def dfs(node, path):
            if node in rec_stack:
                # –ù–∞—à–ª–∏ —Ü–∏–∫–ª
                cycle_start = path.index(node)
                cycles.append(path[cycle_start:])
                return
            
            if node in visited:
                return
                
            visited.add(node)
            rec_stack.add(node)
            path.append(node)
            
            if node in self.dependency_graph:
                for neighbor in self.dependency_graph[node]:
                    dfs(neighbor, path[:])
            
            rec_stack.remove(node)
        
        for node in self.dependency_graph:
            if node not in visited:
                dfs(node, [])
        
        return cycles
    
    def _parse_rust_with_ast(self, content: str, file_path: str) -> Dict:
        """–ü–∞—Ä—Å–∏—Ç Rust –∫–æ–¥ –∏—Å–ø–æ–ª—å–∑—É—è tree-sitter AST –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        if not self.rust_parser or not TREE_SITTER_AVAILABLE:
            # Fallback –Ω–∞ regex –ø–∞—Ä—Å–∏–Ω–≥
            return self._parse_rust_with_regex(content, file_path)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à AST
        import hashlib
        content_hash = hashlib.md5(content.encode()).hexdigest()
        
        if content_hash in self.ast_cache:
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            _, cached_result = self.ast_cache[content_hash]
            return cached_result
        
        try:
            # –ü–∞—Ä—Å–∏–º AST –¥–µ—Ä–µ–≤–æ
            tree = self.rust_parser.parse(content.encode('utf8'))
            root_node = tree.root_node
            
            # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞
            result = {
                'structs': [],
                'enums': [],
                'traits': [],
                'functions': [],
                'methods': [],
                'impl_blocks': [],
                'macros': [],
                'type_aliases': [],
                'constants': [],
                'statics': [],
                'uses': [],
                'mods': [],
                'tests': [],
                'mocks': [],
                'async_functions': [],
                'generics': [],
                'lifetimes': [],
                'attributes': []
            }
            
            # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –æ–±—Ö–æ–¥ AST –¥–µ—Ä–µ–≤–∞
            self._traverse_ast_node(root_node, content.encode('utf8'), result)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –º–æ–∫–æ–≤ –∏ —Ç–µ—Å—Ç–æ–≤
            self._analyze_mocks_from_ast(result)
            self._analyze_tests_from_ast(result)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            self.ast_cache[content_hash] = (tree, result)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
            if len(self.ast_cache) > 100:
                # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ (–ø—Ä–æ—Å—Ç–∞—è FIFO —Å—Ç—Ä–∞—Ç–µ–≥–∏—è)
                oldest_key = next(iter(self.ast_cache))
                del self.ast_cache[oldest_key]
            
            return result
            
        except Exception as e:
            print(f"[WARNING] –û—à–∏–±–∫–∞ AST –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–ª—è {file_path}: {e}")
            # Fallback –Ω–∞ regex
            return self._parse_rust_with_regex(content, file_path)
    
    def _traverse_ast_node(self, node, source_code: bytes, result: Dict, depth=0):
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ö–æ–¥–∏—Ç AST —É–∑–ª—ã –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é"""
        if depth > 100:  # –ó–∞—â–∏—Ç–∞ –æ—Ç —Å–ª–∏—à–∫–æ–º –≥–ª—É–±–æ–∫–æ–π —Ä–µ–∫—É—Ä—Å–∏–∏
            return
            
        node_type = node.type
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä—ã
        if node_type == 'struct_item':
            name_node = self._find_child_by_type(node, 'type_identifier')
            if name_node:
                struct_name = source_code[name_node.start_byte:name_node.end_byte].decode('utf8')
                result['structs'].append(struct_name)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ God Object (–º–Ω–æ–≥–æ –ø–æ–ª–µ–π)
                field_count = len([c for c in node.children if c.type == 'field_declaration'])
                if field_count > 10:
                    result.setdefault('god_objects', []).append((struct_name, field_count))
        
        # –ü–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è
        elif node_type == 'enum_item':
            name_node = self._find_child_by_type(node, 'type_identifier')
            if name_node:
                result['enums'].append(source_code[name_node.start_byte:name_node.end_byte].decode('utf8'))
        
        # –¢—Ä–µ–π—Ç—ã
        elif node_type == 'trait_item':
            name_node = self._find_child_by_type(node, 'type_identifier')
            if name_node:
                result['traits'].append(source_code[name_node.start_byte:name_node.end_byte].decode('utf8'))
        
        # –§—É–Ω–∫—Ü–∏–∏
        elif node_type == 'function_item':
            name_node = self._find_child_by_type(node, 'identifier')
            if name_node:
                func_name = source_code[name_node.start_byte:name_node.end_byte].decode('utf8')
                result['functions'].append(func_name)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º async
                if self._has_child_type(node, 'async'):
                    result['async_functions'].append(func_name)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ç–µ—Å—Ç
                if self._has_test_attribute(node, source_code):
                    result['tests'].append(func_name)
        
        # Impl –±–ª–æ–∫–∏
        elif node_type == 'impl_item':
            type_node = self._find_child_by_type(node, 'type_identifier')
            if type_node:
                impl_type = source_code[type_node.start_byte:type_node.end_byte].decode('utf8')
                result['impl_blocks'].append(impl_type)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–æ–¥—ã –∏–∑ impl –±–ª–æ–∫–∞
                decl_list = self._find_child_by_type(node, 'declaration_list')
                if decl_list:
                    for child in decl_list.children:
                        if child.type == 'function_item':
                            method_name_node = self._find_child_by_type(child, 'identifier')
                            if method_name_node:
                                method_name = source_code[method_name_node.start_byte:method_name_node.end_byte].decode('utf8')
                                result['methods'].append(f"{impl_type}::{method_name}")
        
        # –ú–∞–∫—Ä–æ—Å—ã
        elif node_type == 'macro_definition':
            name_node = self._find_child_by_type(node, 'identifier')
            if name_node:
                result['macros'].append(source_code[name_node.start_byte:name_node.end_byte].decode('utf8'))
        
        # Type aliases
        elif node_type == 'type_alias':
            name_node = self._find_child_by_type(node, 'type_identifier')
            if name_node:
                result['type_aliases'].append(source_code[name_node.start_byte:name_node.end_byte].decode('utf8'))
        
        # –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
        elif node_type == 'const_item':
            name_node = self._find_child_by_type(node, 'identifier')
            if name_node:
                result['constants'].append(source_code[name_node.start_byte:name_node.end_byte].decode('utf8'))
        
        # –°—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        elif node_type == 'static_item':
            name_node = self._find_child_by_type(node, 'identifier')
            if name_node:
                result['statics'].append(source_code[name_node.start_byte:name_node.end_byte].decode('utf8'))
        
        # Use statements
        elif node_type == 'use_declaration':
            use_text = source_code[node.start_byte:node.end_byte].decode('utf8')
            use_text = use_text.replace('use ', '').replace(';', '').strip()
            result['uses'].append(use_text)
        
        # –ú–æ–¥—É–ª–∏
        elif node_type == 'mod_item':
            name_node = self._find_child_by_type(node, 'identifier')
            if name_node:
                result['mods'].append(source_code[name_node.start_byte:name_node.end_byte].decode('utf8'))
        
        # –ê—Ç—Ä–∏–±—É—Ç—ã (–¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è mockall –∏ –¥—Ä—É–≥–∏—Ö)
        elif node_type == 'attribute_item':
            attr_text = source_code[node.start_byte:node.end_byte].decode('utf8')
            result['attributes'].append(attr_text)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ mock –∞—Ç—Ä–∏–±—É—Ç—ã
            if 'mock' in attr_text.lower() or 'automock' in attr_text.lower():
                result.setdefault('mock_attributes', []).append(attr_text)
        
        # Unsafe –±–ª–æ–∫–∏
        elif node_type == 'unsafe_block':
            result.setdefault('unsafe_blocks', []).append({
                'line': node.start_point[0] + 1,
                'code': source_code[node.start_byte:node.end_byte].decode('utf8')[:50] + '...'
            })
        
        # Associated types –≤ trait
        elif node_type == 'associated_type':
            name_node = self._find_child_by_type(node, 'type_identifier')
            if name_node:
                result.setdefault('associated_types', []).append(
                    source_code[name_node.start_byte:name_node.end_byte].decode('utf8')
                )
        
        # Generic –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        elif node_type == 'type_parameters':
            params_text = source_code[node.start_byte:node.end_byte].decode('utf8')
            result.setdefault('generics', []).append(params_text)
        
        # Lifetime –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        elif node_type == 'lifetime':
            lifetime_text = source_code[node.start_byte:node.end_byte].decode('utf8')
            result.setdefault('lifetimes', []).append(lifetime_text)
        
        # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –º–æ–∫–æ–≤/—Å—Ç–∞–±–æ–≤/—Ñ–µ–π–∫–æ–≤ –ø–æ –∏–º–µ–Ω–∏
        if node_type in ['struct_item', 'function_item']:
            name_node = self._find_child_by_type(node, 'identifier') or self._find_child_by_type(node, 'type_identifier')
            if name_node:
                name = source_code[name_node.start_byte:name_node.end_byte].decode('utf8')
                name_lower = name.lower()
                
                # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –º–æ–∫–æ–≤/—Å—Ç–∞–±–æ–≤
                if any(pattern in name_lower for pattern in ['mock', 'stub', 'fake', 'dummy', 'spy']):
                    result.setdefault('test_doubles', []).append({
                        'name': name,
                        'type': 'mock' if 'mock' in name_lower else 
                                'stub' if 'stub' in name_lower else
                                'fake' if 'fake' in name_lower else
                                'dummy' if 'dummy' in name_lower else 'spy',
                        'node_type': node_type
                    })
        
        # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ö–æ–¥–∏–º –¥–æ—á–µ—Ä–Ω–∏–µ —É–∑–ª—ã
        for child in node.children:
            self._traverse_ast_node(child, source_code, result, depth + 1)
    
    def _find_child_by_type(self, node, type_name: str):
        """–ù–∞—Ö–æ–¥–∏—Ç –¥–æ—á–µ—Ä–Ω–∏–π —É–∑–µ–ª –ø–æ —Ç–∏–ø—É"""
        for child in node.children:
            if child.type == type_name:
                return child
        return None
    
    def _has_child_type(self, node, type_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –¥–æ—á–µ—Ä–Ω–µ–≥–æ —É–∑–ª–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞"""
        return any(child.type == type_name for child in node.children)
    
    def _has_test_attribute(self, node, source_code: bytes) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ —É —Ñ—É–Ω–∫—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤—ã–π –∞—Ç—Ä–∏–±—É—Ç"""
        # –ò—â–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã –ø–µ—Ä–µ–¥ —Ñ—É–Ω–∫—Ü–∏–µ–π
        for child in node.children:
            if child.type == 'attribute_item':
                attr_text = source_code[child.start_byte:child.end_byte].decode('utf8')
                if '#[test]' in attr_text or '#[tokio::test]' in attr_text:
                    return True
        return False
    
    def _analyze_mocks_from_ast(self, ast_result: Dict):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç AST —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –º–æ–∫–æ–≤"""
        mocks = []
        
        # –ò—â–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å Mock/Fake/Stub –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏
        for struct_name in ast_result.get('structs', []):
            if any(pattern in struct_name for pattern in ['Mock', 'Fake', 'Stub', 'Dummy', 'Test']):
                mocks.append(struct_name)
        
        # –ò—â–µ–º impl –±–ª–æ–∫–∏ –¥–ª—è –º–æ–∫–æ–≤
        for impl_type in ast_result.get('impl_blocks', []):
            if any(pattern in impl_type for pattern in ['Mock', 'Fake', 'Stub']):
                mocks.append(f"impl {impl_type}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã mockall
        for attr in ast_result.get('attributes', []):
            if 'mockall' in attr or 'automock' in attr:
                mocks.append("[uses mockall]")
        
        ast_result['mocks'] = mocks
    
    def _analyze_tests_from_ast(self, ast_result: Dict):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç AST –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤"""
        test_count = len(ast_result.get('tests', []))
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ–º —Ç–µ—Å—Ç—ã
        unit_tests = [t for t in ast_result.get('tests', []) if not t.startswith('test_integration')]
        integration_tests = [t for t in ast_result.get('tests', []) if t.startswith('test_integration')]
        
        ast_result['test_stats'] = {
            'total': test_count,
            'unit': len(unit_tests),
            'integration': len(integration_tests)
        }
    
    def _parse_rust_with_regex(self, content: str, file_path: str) -> Dict:
        """Fallback –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º regex (—Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥)"""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å –ø–æ–º–æ—â—å—é regex
        return {
            'structs': re.findall(r'^(?:pub\s+)?struct (\w+)', content, re.MULTILINE),
            'enums': re.findall(r'^(?:pub\s+)?enum (\w+)', content, re.MULTILINE),
            'traits': re.findall(r'^(?:pub\s+)?trait (\w+)', content, re.MULTILINE),
            'functions': re.findall(r'^(?:pub\s+)?(?:async\s+)?fn\s+(\w+)', content, re.MULTILINE),
            'methods': [],  # –ë—É–¥—É—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω—ã –æ—Ç–¥–µ–ª—å–Ω–æ
            'impl_blocks': [],
            'macros': re.findall(r'^macro_rules!\s+(\w+)', content, re.MULTILINE),
            'type_aliases': re.findall(r'^(?:pub\s+)?type\s+(\w+)', content, re.MULTILINE),
            'constants': re.findall(r'^(?:pub\s+)?const\s+(\w+):', content, re.MULTILINE),
            'statics': re.findall(r'^(?:pub\s+)?static\s+(\w+):', content, re.MULTILINE),
            'uses': re.findall(r'^use\s+([^;]+);', content, re.MULTILINE),
            'mods': re.findall(r'^(?:pub\s+)?mod\s+(\w+)', content, re.MULTILINE),
            'tests': re.findall(r'#\[test\]\s*(?:async\s+)?fn\s+(\w+)', content),
            'mocks': [],
            'async_functions': re.findall(r'async\s+fn\s+(\w+)', content)
        }
    
    def _scan_rust_files(self, crate_path: Path) -> Dict[str, Dict]:
        """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –≤—Å–µ Rust —Ñ–∞–π–ª—ã –≤ –∫—Ä–µ–π—Ç–µ –∏—Å–ø–æ–ª—å–∑—É—è AST –ø–∞—Ä—Å–∏–Ω–≥"""
        files_info = {}
        
        for rust_file in crate_path.rglob("*.rs"):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º target
            if 'target' in rust_file.parts:
                continue
                
            relative_path = rust_file.relative_to(crate_path)
            file_key = str(relative_path).replace('\\', '/')
            crate_name = crate_path.name
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞
            is_test = 'test' in file_key or file_key.startswith('tests/')
            is_example = file_key.startswith('examples/')
            is_bench = file_key.startswith('benches/')
            is_mock = 'mock' in file_key.lower()
            is_common = 'common' in file_key or 'utils' in file_key or 'helpers' in file_key
            
            # –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            if not self._should_analyze_file(rust_file) and file_key in self.file_cache:
                files_info[file_key] = self.file_cache[file_key]
                continue
            
            try:
                with open(rust_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º AST –ø–∞—Ä—Å–∏–Ω–≥ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
                full_file_path = f"{crate_name}/{file_key}"
                ast_result = self._parse_rust_with_ast(content, full_file_path)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–∑ AST —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                structs = ast_result.get('structs', [])
                enums = ast_result.get('enums', [])
                traits = ast_result.get('traits', [])
                functions = ast_result.get('functions', [])
                methods = ast_result.get('methods', [])
                uses = ast_result.get('uses', [])
                mocks = ast_result.get('mocks', [])
                tests = ast_result.get('tests', [])
                async_fns = ast_result.get('async_functions', [])
                macros = ast_result.get('macros', [])
                type_aliases = ast_result.get('type_aliases', [])
                consts = ast_result.get('constants', [])
                statics = ast_result.get('statics', [])
                
                # –ù–æ–≤—ã–µ –ø–æ–ª—è –∏–∑ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ AST
                unsafe_blocks = ast_result.get('unsafe_blocks', [])
                associated_types = ast_result.get('associated_types', [])
                test_doubles = ast_result.get('test_doubles', [])
                god_objects = ast_result.get('god_objects', [])
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö —É—Ç–∏–ª–∏—Ç –∏ –º–æ–∫–æ–≤
                test_builders = []
                mock_impls = []
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ mock impl –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã —á–µ—Ä–µ–∑ AST
                if not mocks and ('mock' in file_key.lower() or 'fake' in file_key.lower()):
                    mock_impl_pattern = r'impl(?:<[^>]+>)?\s+\w+\s+for\s+(Mock\w+|Fake\w+|Stub\w+)'
                    mock_impls = re.findall(mock_impl_pattern, content)
                    mocks.extend(mock_impls)
                
                # –ò—â–µ–º Test builders –∏ helpers
                if is_test or is_common:
                    test_builders = re.findall(r'(?:pub\s+)?struct\s+(\w*(?:Builder|Helper|Generator|Factory|Fixture)\w*)', content)
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —É—Ç–∏–ª–∏—Ç—ã –≥–ª–æ–±–∞–ª—å–Ω–æ
                    if test_builders:
                        full_path = f"{crate_name}/{file_key}"
                        if crate_name not in self.test_utilities:
                            self.test_utilities[crate_name] = []
                        self.test_utilities[crate_name].extend([(full_path, tb) for tb in test_builders])
                
                # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –º–æ–∫–∏ –≥–ª–æ–±–∞–ª—å–Ω–æ
                if mocks or mock_impls:
                    full_path = f"{crate_name}/{file_key}"
                    if crate_name not in self.mocks_registry:
                        self.mocks_registry[crate_name] = []
                    self.mocks_registry[crate_name].extend([(full_path, m) for m in mocks + mock_impls])
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ AST —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                test_fns = tests[:3] if tests else []
                
                # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
                local_deps, external_deps = self._build_dependency_graph(crate_name, uses, full_file_path)
                
                # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
                cyclomatic = self._calculate_cyclomatic_complexity(content)
                cognitive = self._calculate_cognitive_complexity(content)
                
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–æ–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä
                fields_count = len(re.findall(r'^\s+(pub\s+)?\w+:\s+', content, re.MULTILINE))
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º God Object
                god_score = self._detect_god_object(structs, methods, fields_count)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                full_file_path = f"{crate_name}/{file_key}"
                self.complexity_metrics[full_file_path] = {
                    'cyclomatic': cyclomatic,
                    'cognitive': cognitive,
                    'god_object_score': god_score,
                    'loc': len(content.splitlines()),
                    'methods': len(methods),
                    'fields': fields_count
                }
                
                # –ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ - —Å–æ–±–∏—Ä–∞–µ–º —Å–∏–≥–Ω–∞—Ç—É—Ä—ã impl –±–ª–æ–∫–æ–≤
                impl_signatures = []
                for trait_or_type in re.findall(r'impl(?:<[^>]+>)?\s+(?:(\w+)\s+for\s+)?(\w+)', content):
                    if trait_or_type[0]:  # trait impl
                        sig = f"impl {trait_or_type[0]} for {trait_or_type[1]}"
                    else:  # direct impl
                        sig = f"impl {trait_or_type[1]}"
                    impl_signatures.append(sig)
                    
                    # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
                    if sig not in self.duplicates:
                        self.duplicates[sig] = []
                    self.duplicates[sig].append((f"{crate_name}/{file_key}", trait_or_type[1]))
                
                file_info = {
                    "structs": structs[:4],
                    "enums": enums[:3],
                    "traits": traits[:3],
                    "methods": methods[:4],
                    "functions": functions[:4],
                    "async_fns": async_fns[:3],
                    "consts": consts[:2],
                    "statics": statics[:2],
                    "macros": macros[:2],
                    "types": type_aliases[:2],
                    "mocks": mocks[:3],
                    "test_fns": test_fns[:3],
                    "test_builders": test_builders[:2],
                    "test_doubles": test_doubles[:3],
                    "unsafe_blocks": len(unsafe_blocks),
                    "associated_types": associated_types[:2],
                    "god_objects": god_objects[:2],
                    "impl_sigs": impl_signatures[:3],
                    "uses": [u.strip() for u in uses[:3]],
                    "loc": len(content.splitlines()),
                    "is_test": is_test,
                    "is_example": is_example,
                    "is_bench": is_bench,
                    "is_mock": is_mock or len(mocks) > 0 or len(test_doubles) > 0,
                    "is_common": is_common,
                    "complexity": {
                        "cyclomatic": cyclomatic,
                        "cognitive": cognitive,
                        "god_score": god_score
                    }
                }
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                files_info[file_key] = file_info
                self.file_cache[file_key] = file_info
                
            except Exception as e:
                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ —á—Ç–µ–Ω–∏—è
                pass
                
        return files_info
    
    def _calculate_cyclomatic_complexity(self, content: str) -> int:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ü–∏–∫–ª–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∫–æ–¥–∞"""
        complexity = 1  # –ë–∞–∑–æ–≤–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        
        # –ü–æ–¥—Å—á–µ—Ç —Ç–æ—á–µ–∫ –≤–µ—Ç–≤–ª–µ–Ω–∏—è
        complexity += len(re.findall(r'\bif\b', content))
        complexity += len(re.findall(r'\belse\s+if\b', content))
        complexity += len(re.findall(r'\bmatch\b', content))
        complexity += len(re.findall(r'\bfor\b', content))
        complexity += len(re.findall(r'\bwhile\b', content))
        complexity += len(re.findall(r'\b\?\b', content))  # –¢–µ—Ä–Ω–∞—Ä–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä
        complexity += len(re.findall(r'\b&&\b', content))
        complexity += len(re.findall(r'\b\|\|\b', content))
        complexity += len(re.findall(r'=>', content)) // 2  # match arms
        
        return complexity
    
    def _calculate_cognitive_complexity(self, content: str) -> int:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å (–±–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞)"""
        cognitive = 0
        nesting_level = 0
        
        lines = content.split('\n')
        for line in lines:
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —É—Ä–æ–≤–µ–Ω—å –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏
            if re.search(r'\b(if|for|while|match)\b.*\{', line):
                nesting_level += 1
                cognitive += nesting_level
            elif '{' in line:
                nesting_level += 1
            elif '}' in line:
                nesting_level = max(0, nesting_level - 1)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç—å –¥–ª—è –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤
            cognitive += len(re.findall(r'\b(&&|\|\|)\b', line)) * (nesting_level + 1)
            
        return cognitive
    
    def _detect_god_object(self, structs: List[str], methods: List[str], fields_count: int = 0) -> float:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å God Object –∞–Ω—Ç–∏–ø–∞—Ç—Ç–µ—Ä–Ω–∞"""
        if not structs:
            return 0.0
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        method_count = len(methods)
        struct_count = len(structs)
        
        # God Object –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        god_score = 0.0
        
        if method_count > 20:
            god_score += 0.3
        elif method_count > 15:
            god_score += 0.2
        elif method_count > 10:
            god_score += 0.1
            
        if fields_count > 15:
            god_score += 0.3
        elif fields_count > 10:
            god_score += 0.2
        elif fields_count > 7:
            god_score += 0.1
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–º–µ–Ω–∞ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ God Object
        for struct in structs:
            if any(word in struct.lower() for word in ['manager', 'controller', 'handler', 'service', 'unified']):
                god_score += 0.2
                break
        
        # –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–µ–π
        if method_count > 10 and fields_count > 5:
            god_score += 0.2
            
        return min(1.0, god_score)
    
    def _analyze_duplicates(self) -> Dict[str, List]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç—á–µ—Ç"""
        duplicate_report = {}
        
        for sig, locations in self.duplicates.items():
            if len(locations) > 1:
                # –ï—Å—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã
                duplicate_report[sig] = locations
        
        return duplicate_report
    
    def _generate_analysis_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ–± –∞–Ω–∞–ª–∏–∑–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, –º–æ–∫–æ–≤ –∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö —É—Ç–∏–ª–∏—Ç"""
        report_lines = []
        
        # –ê–Ω–∞–ª–∏–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–ª–≥–∞
        self.tech_debt = self._calculate_tech_debt()
        if self.tech_debt:
            report_lines.append("## üí∏ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –¥–æ–ª–≥\n")
            
            total_hours = sum(item['estimated_hours'] for item in self.tech_debt)
            critical_count = sum(1 for item in self.tech_debt if item['severity'] == 'critical')
            high_count = sum(1 for item in self.tech_debt if item['severity'] == 'high')
            
            report_lines.append(f"**–û–±—â–∏–π –¥–æ–ª–≥**: {total_hours:.1f} —á–∞—Å–æ–≤ ({total_hours/8:.1f} –¥–Ω–µ–π)")
            report_lines.append(f"**–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º**: {critical_count}")
            report_lines.append(f"**–í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç**: {high_count}\n")
            
            # –¢–æ–ø-5 –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º
            for item in self.tech_debt[:5]:
                report_lines.append(f"- [{item['severity'].upper()}] {item['description']}")
                report_lines.append(f"  - –§–∞–π–ª: `{item['file']}`")
                report_lines.append(f"  - –û—Ü–µ–Ω–∫–∞: {item['estimated_hours']:.1f} —á–∞—Å–æ–≤")
            report_lines.append("")
        
        # –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        if self.complexity_metrics:
            report_lines.append("## üìä –ú–µ—Ç—Ä–∏–∫–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏\n")
            
            # –§–∞–π–ª—ã —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é
            complex_files = sorted(
                [(path, m) for path, m in self.complexity_metrics.items()],
                key=lambda x: x[1]['cyclomatic'],
                reverse=True
            )[:5]
            
            report_lines.append("### –°–∞–º—ã–µ —Å–ª–æ–∂–Ω—ã–µ —Ñ–∞–π–ª—ã:")
            for path, metrics in complex_files:
                if metrics['cyclomatic'] > 10:
                    report_lines.append(f"- `{path}`:")
                    report_lines.append(f"  - –¶–∏–∫–ª–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è: {metrics['cyclomatic']}")
                    report_lines.append(f"  - –ö–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è: {metrics['cognitive']}")
                    if metrics['god_object_score'] > 0.5:
                        report_lines.append(f"  - ‚ö†Ô∏è God Object: {metrics['god_object_score']:.0%}")
            report_lines.append("")
        
        # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
        cycles = self._detect_circular_dependencies()
        if cycles:
            report_lines.append("## üîÑ –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏\n")
            for cycle in cycles[:5]:
                report_lines.append(f"- {' ‚Üí '.join(cycle)} ‚Üí {cycle[0]}")
            report_lines.append("")
        
        # –ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        duplicates = self._analyze_duplicates()
        if duplicates:
            report_lines.append("## üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã\n")
            for sig, locations in sorted(duplicates.items())[:10]:  # –¢–æ–ø-10 –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                report_lines.append(f"- **{sig}** –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è {len(locations)} —Ä–∞–∑:")
                for path, name in locations[:3]:
                    report_lines.append(f"  - `{path}` ({name})")
                if len(locations) > 3:
                    report_lines.append(f"  - ...–∏ –µ—â–µ {len(locations)-3} –º–µ—Å—Ç")
            report_lines.append("")
        
        # –ê–Ω–∞–ª–∏–∑ –º–æ–∫–æ–≤
        if self.mocks_registry:
            report_lines.append("## üé≠ –†–µ–µ—Å—Ç—Ä –º–æ–∫–æ–≤ –∏ –∑–∞–≥–ª—É—à–µ–∫\n")
            total_mocks = sum(len(mocks) for mocks in self.mocks_registry.values())
            report_lines.append(f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –º–æ–∫–æ–≤: **{total_mocks}**\n")
            
            for crate, mocks in sorted(self.mocks_registry.items()):
                if mocks:
                    report_lines.append(f"### {crate}")
                    unique_mocks = {}
                    for path, mock_name in mocks:
                        if mock_name not in unique_mocks:
                            unique_mocks[mock_name] = []
                        unique_mocks[mock_name].append(path)
                    
                    for mock_name, paths in sorted(unique_mocks.items())[:5]:
                        report_lines.append(f"- `{mock_name}` –≤ {paths[0]}")
            report_lines.append("")
        
        # –ê–Ω–∞–ª–∏–∑ —Ç–µ—Å—Ç–æ–≤—ã—Ö —É—Ç–∏–ª–∏—Ç
        if self.test_utilities:
            report_lines.append("## üõ†Ô∏è –¢–µ—Å—Ç–æ–≤—ã–µ —É—Ç–∏–ª–∏—Ç—ã –∏ –±–∏–ª–¥–µ—Ä—ã\n")
            for crate, utilities in sorted(self.test_utilities.items()):
                if utilities:
                    report_lines.append(f"### {crate}")
                    unique_utils = {}
                    for path, util_name in utilities:
                        if util_name not in unique_utils:
                            unique_utils[util_name] = path
                    
                    for util_name, path in sorted(unique_utils.items())[:5]:
                        report_lines.append(f"- `{util_name}` –≤ {path}")
            report_lines.append("")
        
        return "\n".join(report_lines) if report_lines else ""
    
    def _generate_mermaid(self, arch: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—É—é Mermaid –¥–∏–∞–≥—Ä–∞–º–º—É"""
        lines = [
            "```mermaid",
            "graph TB",
            ""
        ]
        
        # –°–æ–∑–¥–∞–µ–º subgraph –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫—Ä–µ–π—Ç–∞
        for crate_name, crate_info in arch["crates"].items():
            crate_id = crate_name.upper()
            lines.append(f"    subgraph {crate_id}[{crate_info['description']}]")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã –∫—Ä–µ–π—Ç–∞
            if "files" in crate_info and crate_info["files"]:
                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è–º
                files_by_dir = {}
                for file_path, file_info in crate_info["files"].items():
                    dir_name = os.path.dirname(file_path) if '/' in file_path else 'root'
                    if dir_name not in files_by_dir:
                        files_by_dir[dir_name] = []
                    files_by_dir[dir_name].append((file_path, file_info))
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–π–ª—ã —Å –∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º–∏
                for dir_name, files in files_by_dir.items():
                    for file_path, file_info in files[:8]:  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç –¥–ª—è –±–æ–ª—å—à–µ–π –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏
                        file_name = os.path.basename(file_path).replace('.rs', '')
                        file_id = f"{crate_id}_{file_name.replace('-', '_').replace('/', '_')}"
                        
                        # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
                        components = []
                        
                        # –ú–∞—Ä–∫–∏—Ä—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã
                        if file_info.get('is_test'):
                            components.append("TEST")
                        if file_info.get('is_mock'):
                            components.append("MOCK")
                        if file_info.get('is_example'):
                            components.append("EXAMPLE")
                        if file_info.get('is_bench'):
                            components.append("BENCH")
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
                        if file_info.get('structs'):
                            components.append(f"S:{','.join(file_info['structs'][:2])}")
                        if file_info.get('traits'):
                            components.append(f"T:{','.join(file_info['traits'][:2])}")
                        if file_info.get('enums'):
                            components.append(f"E:{','.join(file_info['enums'][:2])}")
                        if file_info.get('functions'):
                            components.append(f"fn:{','.join(file_info['functions'][:2])}")
                        if file_info.get('methods'):
                            components.append(f"m:{','.join(file_info['methods'][:2])}")
                        if file_info.get('async_fns'):
                            components.append(f"async:{','.join(file_info['async_fns'][:2])}")
                        if file_info.get('macros'):
                            components.append(f"macro:{','.join(file_info['macros'][:1])}")
                        if file_info.get('mocks'):
                            mock_str = ','.join([m if isinstance(m, str) else str(m) for m in file_info['mocks'][:2]])
                            components.append(f"Mock:{mock_str}")
                        if file_info.get('test_doubles'):
                            test_doubles_str = ','.join([td['name'] for td in file_info['test_doubles'][:2]])
                            components.append(f"TestDouble:{test_doubles_str}")
                        if file_info.get('unsafe_blocks', 0) > 0:
                            components.append(f"unsafe:{file_info['unsafe_blocks']}")
                        if file_info.get('god_objects'):
                            god_str = ','.join([f"{go[0]}({go[1]})" for go in file_info['god_objects'][:1]])
                            components.append(f"GOD:{god_str}")
                        if file_info.get('test_fns'):
                            components.append(f"tests:{len(file_info['test_fns'])}")
                        
                        # –§–æ—Ä–º–∏—Ä—É–µ–º label
                        if components:
                            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
                            if len(components) > 3:
                                label = f"{file_name}<br/>{'<br/>'.join(components[:3])}<br/>...+{len(components)-3}"
                            else:
                                label = f"{file_name}<br/>{'<br/>'.join(components)}"
                        else:
                            label = file_name
                        
                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∏–ª—å —É–∑–ª–∞
                        node_style = ""
                        if file_info.get('is_test'):
                            node_style = ":::testFile"
                        elif file_info.get('is_mock'):
                            node_style = ":::mockFile"
                        elif file_info.get('is_example'):
                            node_style = ":::exampleFile"
                        elif file_info.get('is_bench'):
                            node_style = ":::benchFile"
                            
                        lines.append(f"        {file_id}[{label}]{node_style}")
                        
            lines.append("    end")
            lines.append("")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –∫—Ä–µ–π—Ç–∞–º–∏
        lines.append("    %% –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –∫—Ä–µ–π—Ç–∞–º–∏")
        for crate_name, deps in arch["dependencies"].items():
            crate_id = crate_name.upper()
            for dep in deps:
                dep_id = dep.upper()
                if dep_id in [c.upper() for c in arch["crates"].keys()]:
                    lines.append(f"    {crate_id} -.->|uses| {dep_id}")
        
        lines.append("")
        
        # –°—Ç–∏–ª–∏–∑–∞—Ü–∏—è
        lines.extend([
            "    classDef crate fill:#e3f2fd,stroke:#1976d2,stroke-width:2px",
            "    classDef file fill:#fff9c4,stroke:#f57c00,stroke-width:1px",
            "    classDef testFile fill:#ffebee,stroke:#c62828,stroke-width:1px,stroke-dasharray: 5 5",
            "    classDef mockFile fill:#fce4ec,stroke:#ad1457,stroke-width:1px,stroke-dasharray: 3 3",
            "    classDef exampleFile fill:#e8f5e9,stroke:#2e7d32,stroke-width:1px",
            "    classDef benchFile fill:#fff3e0,stroke:#e65100,stroke-width:1px",
            "    classDef trait fill:#f3e5f5,stroke:#7b1fa2,stroke-width:1px",
            "    classDef struct fill:#e8f5e9,stroke:#388e3c,stroke-width:1px"
        ])
        
        lines.append("```")
        return "\n".join(lines)
    
    def update_claude_md(self, architecture: Dict):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å–µ–∫—Ü–∏—é AUTO-GENERATED ARCHITECTURE –≤ CLAUDE.md"""
        print("[INFO] –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ CLAUDE.md...")
        
        if not self.claude_md.exists():
            print("[ERROR] CLAUDE.md –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return
        
        with open(self.claude_md, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # –ù–∞—Ö–æ–¥–∏–º —Å–µ–∫—Ü–∏—é AUTO-GENERATED ARCHITECTURE –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –µ—ë
        start_marker = "# AUTO-GENERATED ARCHITECTURE"
        
        start_idx = content.find(start_marker)
        if start_idx == -1:
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–µ–∫—Ü–∏—é –≤ –∫–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞
            print("[INFO] –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π —Å–µ–∫—Ü–∏–∏ AUTO-GENERATED ARCHITECTURE...")
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S UTC")
            new_section = f"""

---

# AUTO-GENERATED ARCHITECTURE

*Last updated: {timestamp}*

## –ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ MAGRAY CLI

{architecture['mermaid']}

## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞

- **–í—Å–µ–≥–æ crates**: {len(architecture['crates'])}
- **–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤**: {sum(len(c.get('files', {})) for c in architecture['crates'].values())}
- **–ê–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏**: {sum(len(deps) for deps in architecture['dependencies'].values())}
- **–û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã**: CLI, Memory (3-Layer HNSW), AI/ONNX, LLM Multi-Provider
- **GPU –ø–æ–¥–¥–µ—Ä–∂–∫–∞**: CUDA + TensorRT —á–µ—Ä–µ–∑ feature flags
- **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å**: HNSW O(log n) search, SIMD –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

## –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

- **–ï–¥–∏–Ω—ã–π –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–π —Ñ–∞–π–ª**: `magray` (target ~16MB)
- **Conditional compilation**: cpu/gpu/minimal variants
- **Memory —Å–∏—Å—Ç–µ–º–∞**: 3 —Å–ª–æ—è (Interact/Insights/Assets) —Å HNSW –∏–Ω–¥–µ–∫—Å–∞–º–∏  
- **AI –º–æ–¥–µ–ª–∏**: Qwen3 embeddings (1024D), BGE-M3 legacy support
- **LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã**: OpenAI/Anthropic/Local
- **Production –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å**: Circuit breakers, health checks, metrics

{self._generate_analysis_report()}
"""
            
            new_content = content + new_section
            
            with open(self.claude_md, 'w', encoding='utf-8') as f:
                f.write(new_content)
            
            print(f"[OK] CLAUDE.md –æ–±–Ω–æ–≤–ª–µ–Ω ({timestamp})")
            return
        
        # –ï—Å–ª–∏ —Å–µ–∫—Ü–∏—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –æ–±–Ω–æ–≤–ª—è–µ–º –µ—ë
        # –ò—â–µ–º —Å–ª–µ–¥—É—é—â–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ # –∏–ª–∏ –∫–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞
        next_section_idx = content.find("\n# ", start_idx + len(start_marker))
        if next_section_idx == -1:
            end_idx = len(content)
        else:
            end_idx = next_section_idx
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é —Å–µ–∫—Ü–∏—é
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S UTC")
        new_section = f"""# AUTO-GENERATED ARCHITECTURE

*Last updated: {timestamp}*

## –ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ MAGRAY CLI

{architecture['mermaid']}

## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞

- **–í—Å–µ–≥–æ crates**: {len(architecture['crates'])}
- **–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤**: {sum(len(c.get('files', {})) for c in architecture['crates'].values())}
- **–ê–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏**: {sum(len(deps) for deps in architecture['dependencies'].values())}
- **–û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã**: CLI, Memory (3-Layer HNSW), AI/ONNX, LLM Multi-Provider
- **GPU –ø–æ–¥–¥–µ—Ä–∂–∫–∞**: CUDA + TensorRT —á–µ—Ä–µ–∑ feature flags
- **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å**: HNSW O(log n) search, SIMD –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

## –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

- **–ï–¥–∏–Ω—ã–π –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–π —Ñ–∞–π–ª**: `magray` (target ~16MB)
- **Conditional compilation**: cpu/gpu/minimal variants
- **Memory —Å–∏—Å—Ç–µ–º–∞**: 3 —Å–ª–æ—è (Interact/Insights/Assets) —Å HNSW –∏–Ω–¥–µ–∫—Å–∞–º–∏  
- **AI –º–æ–¥–µ–ª–∏**: Qwen3 embeddings (1024D), BGE-M3 legacy support
- **LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã**: OpenAI/Anthropic/Local
- **Production –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å**: Circuit breakers, health checks, metrics

{self._generate_analysis_report()}
"""
        
        # –ó–∞–º–µ–Ω—è–µ–º —Å–µ–∫—Ü–∏—é
        before_section = content[:start_idx]
        after_section = content[end_idx:]
        new_content = before_section + new_section + after_section
        
        with open(self.claude_md, 'w', encoding='utf-8') as f:
            f.write(new_content)
        
        print(f"[OK] CLAUDE.md –æ–±–Ω–æ–≤–ª–µ–Ω ({timestamp})")
    
    def watch_mode(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç watchdog —Ä–µ–∂–∏–º –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        print("[INFO] –ó–∞–ø—É—Å–∫ watch —Ä–µ–∂–∏–º–∞...")
        
        class ArchitectureHandler(FileSystemEventHandler):
            def __init__(self, daemon):
                self.daemon = daemon
                self.last_update = 0
                
            def on_modified(self, event):
                if event.is_directory:
                    return
                    
                if event.src_path.endswith('Cargo.toml'):
                    # Debounce - –æ–±–Ω–æ–≤–ª—è–µ–º –Ω–µ —á–∞—â–µ —Ä–∞–∑–∞ –≤ 5 —Å–µ–∫—É–Ω–¥
                    now = time.time()
                    if now - self.last_update < 5:
                        return
                        
                    self.last_update = now
                    print(f"[WATCH] –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ {event.src_path}")
                    
                    # –û—Ç–ª–æ–∂–µ–Ω–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ 2 —Å–µ–∫—É–Ω–¥—ã
                    threading.Timer(2.0, self._delayed_update).start()
                    
            def _delayed_update(self):
                try:
                    arch = self.daemon.scan_architecture()
                    self.daemon.update_claude_md(arch)
                    print("[OK] –ê–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
                except Exception as e:
                    print(f"[ERROR] –û—à–∏–±–∫–∞ –∞–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {e}")
        
        observer = Observer()
        observer.schedule(
            ArchitectureHandler(self),
            str(self.crates_dir), 
            recursive=True
        )
        
        observer.start()
        print(f"[WATCH] –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ {self.crates_dir} –∑–∞–ø—É—â–µ–Ω")
        
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            observer.stop()
            print("\n[INFO] Watch —Ä–µ–∂–∏–º –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
        observer.join()
    
    def run_once(self):
        """–ï–¥–∏–Ω–æ–∫—Ä–∞—Ç–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∏–∞–≥—Ä–∞–º–º—ã"""
        print("[INFO] –ó–∞–ø—É—Å–∫ –µ–¥–∏–Ω–æ–∫—Ä–∞—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞...")
        arch = self.scan_architecture()
        self.update_claude_md(arch)
        print("[OK] –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω")
        
        return arch

def main():
    parser = argparse.ArgumentParser(
        description="–£–ª—å—Ç—Ä–∞–∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π –¥–µ–º–æ–Ω –¥–ª—è MAGRAY CLI"
    )
    parser.add_argument(
        '--watch', '-w', 
        action='store_true',
        help='–ó–∞–ø—É—Å—Ç–∏—Ç—å –≤ watch —Ä–µ–∂–∏–º–µ –¥–ª—è –∞–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–π'
    )
    parser.add_argument(
        '--project-root', '-p',
        default='.',
        help='–ü—É—Ç—å –∫ –∫–æ—Ä–Ω—é –ø—Ä–æ–µ–∫—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: —Ç–µ–∫—É—â–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è)'
    )
    
    args = parser.parse_args()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞
    project_root = Path(args.project_root).resolve()
    if not (project_root / "Cargo.toml").exists():
        print("[ERROR] –ù–µ –Ω–∞–π–¥–µ–Ω Cargo.toml –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞")
        print(f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å: {project_root}")
        sys.exit(1)
    
    print(f"[INFO] –ü—Ä–æ–µ–∫—Ç: {project_root}")
    
    daemon = ArchitectureDaemon(str(project_root))
    
    if args.watch:
        daemon.watch_mode()
    else:
        daemon.run_once()

if __name__ == "__main__":
    main()