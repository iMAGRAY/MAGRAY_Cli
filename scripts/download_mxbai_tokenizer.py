#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ MXBai reranker –º–æ–¥–µ–ª–∏
"""

import os
import sys
import json
import requests
from pathlib import Path

# MXBai reranker model –Ω–∞ Hugging Face
MODEL_ID = "mixedbread-ai/mxbai-rerank-base-v1"
BASE_URL = f"https://huggingface.co/{MODEL_ID}/resolve/main"

# –§–∞–π–ª—ã –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ —Å–∫–∞—á–∞—Ç—å
FILES_TO_DOWNLOAD = [
    "tokenizer.json",
    "tokenizer_config.json", 
    "vocab.txt",
    "special_tokens_map.json",
    "merges.txt"  # –µ—Å–ª–∏ —ç—Ç–æ BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
]

def download_file(url: str, dest_path: Path) -> bool:
    """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º"""
    try:
        print(f"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ {dest_path.name}...")
        
        response = requests.get(url, stream=True)
        response.raise_for_status()
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        dest_path.parent.mkdir(parents=True, exist_ok=True)
        
        total_size = int(response.headers.get('content-length', 0))
        
        with open(dest_path, 'wb') as f:
            downloaded = 0
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    
                    if total_size > 0:
                        percent = (downloaded / total_size) * 100
                        print(f"  Progress: {percent:.1f}%", end='\r')
        
        print(f"  ‚úÖ {dest_path.name} —Å–∫–∞—á–∞–Ω ({downloaded:,} bytes)")
        return True
        
    except Exception as e:
        print(f"  ‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {dest_path.name}: {e}")
        return False

def main():
    print("ü§ñ MXBai Tokenizer Downloader")
    print("=" * 50)
    
    # –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
    model_dir = Path(__file__).parent.parent / "crates" / "memory" / "models" / "mxbai_rerank_base_v2"
    
    if not model_dir.exists():
        print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_dir}")
        return 1
    
    print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –º–æ–¥–µ–ª–∏: {model_dir}")
    
    successful_downloads = 0
    
    for filename in FILES_TO_DOWNLOAD:
        url = f"{BASE_URL}/{filename}"
        dest_path = model_dir / filename
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if dest_path.exists():
            print(f"‚è≠Ô∏è {filename} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            successful_downloads += 1
            continue
        
        if download_file(url, dest_path):
            successful_downloads += 1
    
    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç: {successful_downloads}/{len(FILES_TO_DOWNLOAD)} —Ñ–∞–π–ª–æ–≤ —Å–∫–∞—á–∞–Ω–æ")
    
    if successful_downloads == len(FILES_TO_DOWNLOAD):
        print("üéâ –í—Å–µ —Ñ–∞–π–ª—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω—ã!")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ tokenizer.json –≤–∞–ª–∏–¥–Ω—ã–π JSON
        tokenizer_path = model_dir / "tokenizer.json"
        if tokenizer_path.exists():
            try:
                with open(tokenizer_path, 'r', encoding='utf-8') as f:
                    tokenizer_data = json.load(f)
                print(f"‚úÖ tokenizer.json –≤–∞–ª–∏–¥–Ω—ã–π (vocab_size: {len(tokenizer_data.get('model', {}).get('vocab', {}))})")
            except Exception as e:
                print(f"‚ö†Ô∏è tokenizer.json –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–º: {e}")
        
        return 0
    else:
        print("‚ö†Ô∏è –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ–∞–π–ª—ã –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å")
        return 1

if __name__ == "__main__":
    sys.exit(main())