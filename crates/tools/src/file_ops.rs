use crate::{Tool, ToolInput, ToolOutput, ToolSpec, UsageGuide};
use anyhow::{anyhow, Result};
use std::collections::HashMap;
use std::fs;
use std::path::{Path, PathBuf};
use walkdir::WalkDir;

// ===== Filesystem Sandbox (env-driven) =====
fn fs_sandbox_enabled() -> bool {
    common::sandbox_config::SandboxConfig::from_env().fs.enabled
}

fn fs_sandbox_roots() -> Vec<PathBuf> {
    let cfg = common::sandbox_config::SandboxConfig::from_env();
    if cfg.fs.roots.is_empty() { return Vec::new(); }
    cfg.fs
        .roots
        .iter()
        .filter(|s| !s.trim().is_empty())
        .filter_map(|s| {
            let p = PathBuf::from(s);
            std::fs::canonicalize(&p).ok()
        })
        .collect()
}

fn err_outside_sandbox(path: &str) -> anyhow::Error {
    anyhow!(
        "–î–æ—Å—Ç—É–ø –∫ –ø—É—Ç–∏ –≤–Ω–µ –ø–µ—Å–æ—á–Ω–∏—Ü—ã –∑–∞–ø—Ä–µ—â—ë–Ω: {} (–≤–∫–ª—é—á–∏—Ç–µ –∫–æ—Ä–Ω–∏ —á–µ—Ä–µ–∑ MAGRAY_FS_ROOTS)",
        path
    )
}

fn ensure_read_allowed(path: &str) -> Result<()> {
    if !fs_sandbox_enabled() { return Ok(()); }
    let roots = fs_sandbox_roots();
    if roots.is_empty() { return Err(anyhow!("FS –ø–µ—Å–æ—á–Ω–∏—Ü–∞ –≤–∫–ª—é—á–µ–Ω–∞, –Ω–æ MAGRAY_FS_ROOTS –Ω–µ –∑–∞–¥–∞–Ω")); }
    let canon = std::fs::canonicalize(Path::new(path))
        .map_err(|e| anyhow!("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å '{}': {}", path, e))?;
    if roots.iter().any(|r| canon.starts_with(r)) { Ok(()) } else { Err(err_outside_sandbox(path)) }
}

fn ensure_write_allowed(path: &str) -> Result<()> {
    if !fs_sandbox_enabled() { return Ok(()); }
    let roots = fs_sandbox_roots();
    if roots.is_empty() { return Err(anyhow!("FS –ø–µ—Å–æ—á–Ω–∏—Ü–∞ –≤–∫–ª—é—á–µ–Ω–∞, –Ω–æ MAGRAY_FS_ROOTS –Ω–µ –∑–∞–¥–∞–Ω")); }
    let p = Path::new(path);
    let check_path = if p.exists() {
        std::fs::canonicalize(p).map_err(|e| anyhow!("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å '{}': {}", path, e))?
    } else {
        let parent = p.parent().unwrap_or(Path::new("."));
        let parent_canon = std::fs::canonicalize(parent)
            .map_err(|e| anyhow!("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å —Ä–æ–¥–∏—Ç–µ–ª—è '{}': {}", parent.display(), e))?;
        parent_canon.join(p.file_name().unwrap_or_default())
    };
    if roots.iter().any(|r| check_path.starts_with(r)) { Ok(()) } else { Err(err_outside_sandbox(path)) }
}

// FileReader - —á—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ —Å –ø—Ä–æ—Å—Ç—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
pub struct FileReader;

impl FileReader {
    pub fn new() -> Self {
        FileReader
    }
}

impl Default for FileReader {
    fn default() -> Self {
        Self::new()
    }
}

#[async_trait::async_trait]
impl Tool for FileReader {
    fn spec(&self) -> ToolSpec {
        ToolSpec {
            name: "file_read".to_string(),
            description: "–ß–∏—Ç–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–æ–≤ —Å –∫—Ä–∞—Å–∏–≤–æ–π –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞".to_string(),
            usage: "file_read <–ø—É—Ç—å>".to_string(),
            examples: vec![
                "file_read src/main.rs".to_string(),
                "file_read README.md".to_string(),
                "–ø–æ–∫–∞–∑–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ config.toml".to_string(),
            ],
            input_schema: r#"{"path": "string"}"#.to_string(),
            usage_guide: None,
            permissions: None,
            supports_dry_run: false,
        }
    }

    async fn execute(&self, input: ToolInput) -> Result<ToolOutput> {
        let path = input
            .args
            .get("path")
            .ok_or_else(|| anyhow!("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä 'path'"))?;

        // Enforce sandbox for read
        ensure_read_allowed(path)?;

        let content = fs::read_to_string(path)?;

        // –ü—Ä–æ—Å—Ç–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
        let mut formatted = String::new();
        formatted.push_str(&format!("\nüìÑ –§–∞–π–ª: {}\n", path));
        formatted.push_str(&"‚îÄ".repeat(60));
        formatted.push('\n');
        formatted.push_str(&content);
        formatted.push('\n');
        formatted.push_str(&"‚îÄ".repeat(60));
        formatted.push('\n');

        Ok(ToolOutput {
            success: true,
            result: content,
            formatted_output: Some(formatted),
            metadata: HashMap::new(),
        })
    }

    async fn parse_natural_language(&self, query: &str) -> Result<ToolInput> {
        let mut args = HashMap::new();

        // –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—É—Ç—å –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        if let Some(path) = extract_path_from_query(query) {
            args.insert("path".to_string(), path);
        } else {
            args.insert("path".to_string(), query.to_string());
        }

        Ok(ToolInput {
            command: "file_read".to_string(),
            args,
            context: Some(query.to_string()),
            dry_run: false,
            timeout_ms: None,
        })
    }
}

// FileWriter - –∑–∞–ø–∏—Å—å —Ñ–∞–π–ª–æ–≤
pub struct FileWriter;

impl FileWriter {
    pub fn new() -> Self {
        FileWriter
    }
}

impl Default for FileWriter {
    fn default() -> Self {
        Self::new()
    }
}

#[async_trait::async_trait]
impl Tool for FileWriter {
    fn spec(&self) -> ToolSpec {
        ToolSpec {
            name: "file_write".to_string(),
            description: "–°–æ–∑–¥–∞—ë—Ç –∏–ª–∏ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Ñ–∞–π–ª —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º —Å–æ–¥–µ—Ä–∂–∏–º—ã–º".to_string(),
            usage: "file_write <–ø—É—Ç—å> <—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ>".to_string(),
            examples: vec![
                "file_write test.txt Hello World".to_string(),
                "—Å–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª config.json —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º {...}".to_string(),
            ],
            input_schema: r#"{"path": "string", "content": "string"}"#.to_string(),
            usage_guide: None,
            permissions: None,
            supports_dry_run: true,
        }
    }

    async fn execute(&self, input: ToolInput) -> Result<ToolOutput> {
        let path = input
            .args
            .get("path")
            .ok_or_else(|| anyhow!("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä 'path'"))?;
        let content = input.args.get("content").map(|s| s.as_str()).unwrap_or("");

        // Dry-run: –ø–æ–∫–∞–∑–∞—Ç—å —á—Ç–æ –±—É–¥–µ—Ç –∑–∞–ø–∏—Å–∞–Ω–æ
        if input.dry_run {
            let mut meta = HashMap::new();
            meta.insert("dry_run".into(), "true".into());
            meta.insert("bytes".into(), content.len().to_string());
            return Ok(ToolOutput {
                success: true,
                result: format!("[dry-run] write {} bytes to {}", content.len(), path),
                formatted_output: Some(format!("$ echo '<content:{} bytes>' > {}\n[dry-run: no side effects]", content.len(), path)),
                metadata: meta,
            });
        }

        // Enforce sandbox for write
        ensure_write_allowed(path)?;
        fs::write(path, content)?;

        // Publish fs.diff event for timeline/observability
        let path_for_evt = path.clone();
        let bytes_for_evt = content.len();
        tokio::spawn(async move {
            let evt = serde_json::json!({
                "path": path_for_evt,
                "bytes": bytes_for_evt,
                "op": "write",
            });
            common::events::publish(common::topics::TOPIC_FS_DIFF, evt).await;
        });

        Ok(ToolOutput {
            success: true,
            result: format!("‚úÖ –§–∞–π–ª '{}' —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω", path),
            formatted_output: None,
            metadata: HashMap::from([("bytes".into(), content.len().to_string())]),
        })
    }

    async fn parse_natural_language(&self, query: &str) -> Result<ToolInput> {
        let mut args = HashMap::new();

        // –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∞–π–ª–∞
        let parts: Vec<&str> = query.split(" —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º ").collect();
        if parts.len() == 2 {
            args.insert("path".to_string(), parts[0].trim().to_string());
            args.insert("content".to_string(), parts[1].trim().to_string());
        } else {
            return Err(anyhow!("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∑–∞–ø—Ä–æ—Å –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞"));
        }

        Ok(ToolInput {
            command: "file_write".to_string(),
            args,
            context: Some(query.to_string()),
            dry_run: false,
            timeout_ms: None,
        })
    }
}

// DirLister - –ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
pub struct DirLister;

impl DirLister {
    pub fn new() -> Self {
        DirLister
    }
}

impl Default for DirLister {
    fn default() -> Self {
        Self::new()
    }
}

#[async_trait::async_trait]
impl Tool for DirLister {
    fn spec(&self) -> ToolSpec {
        ToolSpec {
            name: "dir_list".to_string(),
            description: "–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤ –≤–∏–¥–µ –∫—Ä–∞—Å–∏–≤–æ–≥–æ –¥–µ—Ä–µ–≤–∞".to_string(),
            usage: "dir_list <–ø—É—Ç—å>".to_string(),
            examples: vec![
                "dir_list .".to_string(),
                "dir_list src/".to_string(),
                "–ø–æ–∫–∞–∑–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–∞–ø–∫–∏".to_string(),
            ],
            input_schema: r#"{"path": "string"}"#.to_string(),
            usage_guide: None,
            permissions: None,
            supports_dry_run: false,
        }
    }

    async fn execute(&self, input: ToolInput) -> Result<ToolOutput> {
        let path = input
            .args
            .get("path")
            .ok_or_else(|| anyhow!("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä 'path'"))?;

        let path = Path::new(path);
        // Enforce sandbox for read/list
        ensure_read_allowed(&path.to_string_lossy())?;
        if !path.is_dir() {
            return Err(anyhow!("'{}' –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–µ–π", path.display()));
        }

        let mut output = String::new();
        output.push_str(&format!("\nüìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {}\n", path.display()));
        output.push_str(&"‚îÄ".repeat(60));
        output.push('\n');

        // –°–æ–±–∏—Ä–∞–µ–º entries
        let mut entries: Vec<_> = fs::read_dir(path)?.filter_map(|e| e.ok()).collect();

        // –°–æ—Ä—Ç–∏—Ä—É–µ–º: —Å–Ω–∞—á–∞–ª–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, –ø–æ—Ç–æ–º —Ñ–∞–π–ª—ã
        entries.sort_by(|a, b| {
            let a_is_dir = a.file_type().map(|t| t.is_dir()).unwrap_or(false);
            let b_is_dir = b.file_type().map(|t| t.is_dir()).unwrap_or(false);
            b_is_dir
                .cmp(&a_is_dir)
                .then_with(|| a.file_name().cmp(&b.file_name()))
        });

        for entry in entries {
            let entry_path = entry.path();
            let name = entry.file_name();
            let name_str = name.to_string_lossy();

            if entry_path.is_dir() {
                output.push_str(&format!("üìÅ {}/\n", name_str));
            } else {
                let icon = "üìÑ";
                let size = entry
                    .metadata()
                    .map(|m| format_size(m.len()))
                    .unwrap_or_else(|_| "?".to_string());
                output.push_str(&format!("{} {} ({})\n", icon, name_str, size));
            }
        }

        output.push_str(&"‚îÄ".repeat(60));
        output.push('\n');

        Ok(ToolOutput {
            success: true,
            result: output.clone(),
            formatted_output: Some(output),
            metadata: HashMap::new(),
        })
    }

    async fn parse_natural_language(&self, query: &str) -> Result<ToolInput> {
        let mut args = HashMap::new();

        // –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—É—Ç—å –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        if let Some(path) = extract_path_from_query(query) {
            args.insert("path".to_string(), path);
        } else {
            args.insert("path".to_string(), ".".to_string());
        }

        Ok(ToolInput {
            command: "dir_list".to_string(),
            args,
            context: Some(query.to_string()),
            dry_run: false,
            timeout_ms: None,
        })
    }
}

// FileSearcher - –ø–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤
pub struct FileSearcher;

impl FileSearcher {
    pub fn new() -> Self {
        FileSearcher
    }
}

impl Default for FileSearcher {
    fn default() -> Self {
        Self::new()
    }
}

#[async_trait::async_trait]
impl Tool for FileSearcher {
    fn spec(&self) -> ToolSpec {
        ToolSpec {
            name: "file_search".to_string(),
            description: "–ò—â–µ—Ç —Ñ–∞–π–ª—ã –ø–æ –∏–º–µ–Ω–∏ –∏–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é".to_string(),
            usage: "file_search <–ø–∞—Ç—Ç–µ—Ä–Ω> [–ø—É—Ç—å]".to_string(),
            examples: vec![
                "file_search *.rs".to_string(),
                "file_search main.rs src/".to_string(),
                "–Ω–∞–π—Ç–∏ –≤—Å–µ —Ñ–∞–π–ª—ã .toml".to_string(),
            ],
            input_schema: r#"{"pattern": "string", "path": "string?"}"#.to_string(),
            usage_guide: None,
            permissions: None,
            supports_dry_run: false,
        }
    }

    async fn execute(&self, input: ToolInput) -> Result<ToolOutput> {
        let pattern = input
            .args
            .get("pattern")
            .ok_or_else(|| anyhow!("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä 'pattern'"))?;
        let search_path = input.args.get("path").map(|s| s.as_str()).unwrap_or(".");

        // Enforce sandbox for traversal/search root
        ensure_read_allowed(search_path)?;

        let mut results = Vec::new();
        let pattern_lower = pattern.to_lowercase();

        for entry in WalkDir::new(search_path)
            .max_depth(10)
            .into_iter()
            .filter_map(|e| e.ok())
        {
            let path = entry.path();
            let file_name = path.file_name().and_then(|n| n.to_str()).unwrap_or("");

            // –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É
            let matches = if pattern.contains('*') {
                // –ü—Ä–æ—Å—Ç–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ wildcard
                let pattern_parts: Vec<&str> = pattern.split('*').collect();
                if pattern_parts.len() == 2 {
                    if pattern.starts_with('*') {
                        file_name
                            .to_lowercase()
                            .ends_with(&pattern_parts[1].to_lowercase())
                    } else if pattern.ends_with('*') {
                        file_name
                            .to_lowercase()
                            .starts_with(&pattern_parts[0].to_lowercase())
                    } else {
                        file_name
                            .to_lowercase()
                            .starts_with(&pattern_parts[0].to_lowercase())
                            && file_name
                                .to_lowercase()
                                .ends_with(&pattern_parts[1].to_lowercase())
                    }
                } else {
                    file_name.to_lowercase().contains(&pattern_lower)
                }
            } else {
                file_name.to_lowercase().contains(&pattern_lower)
            };

            if matches {
                results.push(path.display().to_string());
            }
        }

        let mut output = String::new();
        output.push_str(&format!("\nüîç –ü–æ–∏—Å–∫: {} –≤ {}\n", pattern, search_path));
        output.push_str(&"‚îÄ".repeat(60));
        output.push('\n');

        if results.is_empty() {
            output.push_str("–§–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\n");
        } else {
            output.push_str(&format!("–ù–∞–π–¥–µ–Ω–æ {} —Ñ–∞–π–ª–æ–≤:\n", results.len()));
            for result in results.iter().take(100) {
                output.push_str(&format!("  üìÑ {}\n", result));
            }
            if results.len() > 100 {
                output.push_str(&format!("  ... –∏ –µ—â—ë {} —Ñ–∞–π–ª–æ–≤\n", results.len() - 100));
            }
        }

        output.push_str(&"‚îÄ".repeat(60));
        output.push('\n');

        Ok(ToolOutput {
            success: true,
            result: output.clone(),
            formatted_output: Some(output),
            metadata: HashMap::new(),
        })
    }

    async fn parse_natural_language(&self, query: &str) -> Result<ToolInput> {
        let mut args = HashMap::new();

        // –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω –ø–æ–∏—Å–∫–∞
        if query.contains("—Ñ–∞–π–ª—ã") || query.contains("—Ñ–∞–π–ª") {
            // –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
            if let Some(ext_start) = query.find('.') {
                let ext_end = query[ext_start..]
                    .find(' ')
                    .unwrap_or(query.len() - ext_start);
                let pattern = format!("*{}", &query[ext_start..ext_start + ext_end]);
                args.insert("pattern".to_string(), pattern);
            } else {
                args.insert("pattern".to_string(), "*".to_string());
            }
        } else {
            args.insert("pattern".to_string(), query.to_string());
        }

        Ok(ToolInput {
            command: "file_search".to_string(),
            args,
            context: Some(query.to_string()),
            dry_run: false,
            timeout_ms: None,
        })
    }
}

// FileDeleter - —É–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ —Å –ø—É–±–ª–∏–∫–∞—Ü–∏–µ–π fs.diff
pub struct FileDeleter;

impl FileDeleter {
    pub fn new() -> Self { FileDeleter }
}

impl Default for FileDeleter {
    fn default() -> Self { Self::new() }
}

#[async_trait::async_trait]
impl Tool for FileDeleter {
    fn spec(&self) -> ToolSpec {
        let mut spec = ToolSpec {
            name: "file_delete".to_string(),
            description: "–£–¥–∞–ª—è–µ—Ç —Ñ–∞–π–ª –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –ø—É—Ç–∏".to_string(),
            usage: "file_delete <–ø—É—Ç—å>".to_string(),
            examples: vec![
                "file_delete tmp/test.txt".to_string(),
                "—É–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª build.log".to_string(),
            ],
            input_schema: r#"{"path": "string"}"#.to_string(),
            usage_guide: None,
            permissions: None,
            supports_dry_run: true,
        };
        // Mark as high risk and with side effects for policy dynamic Ask
        spec.usage_guide = Some(UsageGuide {
            usage_title: "file_delete".into(),
            usage_summary: "–£–¥–∞–ª—è–µ—Ç —Ñ–∞–π–ª –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –ø—É—Ç–∏".into(),
            preconditions: vec!["–§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤–æ–≤–∞—Ç—å".into()],
            arguments_brief: HashMap::from([(String::from("path"), String::from("–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É"))]),
            good_for: vec!["cleanup".into(), "io".into()],
            not_for: vec!["sensitive".into()],
            constraints: vec!["–ù–µ–æ–±—Ä–∞—Ç–∏–º–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è".into()],
            examples: vec!["file_delete /tmp/file.txt".into()],
            platforms: vec!["linux".into(), "mac".into(), "win".into()],
            cost_class: "free".into(),
            latency_class: "fast".into(),
            side_effects: vec!["–£–¥–∞–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö".into()],
            risk_score: 5,
            capabilities: vec!["delete".into(), "fs".into()],
            tags: vec!["danger".into(), "destructive".into()],
        });
        spec
    }

    async fn execute(&self, input: ToolInput) -> Result<ToolOutput> {
        let path = input
            .args
            .get("path")
            .ok_or_else(|| anyhow!("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä 'path'"))?;

        // Dry-run: –ø–æ–∫–∞–∑–∞—Ç—å, —á—Ç–æ –±—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω–æ
        if input.dry_run {
            let mut meta = HashMap::new();
            meta.insert("dry_run".into(), "true".into());
            return Ok(ToolOutput {
                success: true,
                result: format!("[dry-run] rm {}", path),
                formatted_output: Some(format!("$ rm {}\n[dry-run: no side effects]", path)),
                metadata: meta,
            });
        }

        // Enforce sandbox for delete
        ensure_write_allowed(path)?;

        // –°—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –¥–æ —É–¥–∞–ª–µ–Ω–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å)
        let bytes = std::fs::metadata(path).map(|m| m.len()).unwrap_or(0) as usize;

        // –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª
        fs::remove_file(path)?;

        // –ü—É–±–ª–∏–∫—É–µ–º —Å–æ–±—ã—Ç–∏–µ fs.diff
        let path_for_evt = path.clone();
        tokio::spawn(async move {
            let evt = serde_json::json!({
                "path": path_for_evt,
                "bytes": bytes,
                "op": "delete",
            });
            common::events::publish(common::topics::TOPIC_FS_DIFF, evt).await;
        });

        Ok(ToolOutput {
            success: true,
            result: format!("‚úÖ –§–∞–π–ª '{}' —É–¥–∞–ª—ë–Ω", path),
            formatted_output: None,
            metadata: HashMap::from([("bytes".into(), bytes.to_string())]),
        })
    }

    async fn parse_natural_language(&self, query: &str) -> Result<ToolInput> {
        let mut args = HashMap::new();
        // –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –ø—É—Ç—å –∫–∞–∫ –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏
        if let Some(p) = extract_path_from_query(query) {
            args.insert("path".to_string(), p);
        } else {
            args.insert("path".to_string(), query.to_string());
        }
        Ok(ToolInput { command: "file_delete".to_string(), args, context: Some(query.to_string()), dry_run: false, timeout_ms: None })
    }
}

// –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
fn format_size(size: u64) -> String {
    const UNITS: &[&str] = &["B", "KB", "MB", "GB"];
    let mut size = size as f64;
    let mut unit_index = 0;

    while size >= 1024.0 && unit_index < UNITS.len() - 1 {
        size /= 1024.0;
        unit_index += 1;
    }

    if unit_index == 0 {
        format!("{} {}", size as u64, UNITS[unit_index])
    } else {
        format!("{:.1} {}", size, UNITS[unit_index])
    }
}

// –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—É—Ç–∏ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
fn extract_path_from_query(query: &str) -> Option<String> {
    // –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø—É—Ç–µ–π –≤ –∑–∞–ø—Ä–æ—Å–µ
    let words: Vec<&str> = query.split_whitespace().collect();
    for word in words {
        if word.contains('/')
            || word.contains('\\')
            || word.ends_with(".rs")
            || word.ends_with(".md")
            || word.ends_with(".toml")
        {
            return Some(word.to_string());
        }
    }
    None
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::fs;
    use tempfile::TempDir;
    use common::{events, topics};

    #[tokio::test]
    async fn test_file_reader_creation() {
        let reader = FileReader::new();
        let spec = reader.spec();

        assert_eq!(spec.name, "file_read");
        assert!(spec.description.contains("–ß–∏—Ç–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–æ–≤"));
        assert!(!spec.examples.is_empty());
    }

    #[tokio::test]
    async fn test_file_reader_default() {
        let reader1 = FileReader::new();
        let reader2 = FileReader::new();

        assert_eq!(reader1.spec().name, reader2.spec().name);
    }

    #[tokio::test]
    async fn test_file_reader_nonexistent_file() {
        let reader = FileReader::new();
        let mut input_args = HashMap::new();
        input_args.insert(
            "path".to_string(),
            "/definitely/nonexistent/file.txt".to_string(),
        );

        let input = ToolInput {
            command: "file_read".to_string(),
            args: input_args,
            context: None,
            dry_run: false,
            timeout_ms: None,
        };

        let result = reader.execute(input).await;
        assert!(result.is_err());
    }

    #[tokio::test]
    async fn test_file_reader_existing_file() -> Result<()> {
        let temp_dir = TempDir::new().unwrap();
        let file_path = temp_dir.path().join("test.txt");
        let test_content = "Hello, World!";

        fs::write(&file_path, test_content).unwrap();

        let reader = FileReader::new();
        let mut input_args = HashMap::new();
        input_args.insert("path".to_string(), file_path.to_string_lossy().to_string());

        let input = ToolInput {
            command: "file_read".to_string(),
            args: input_args,
            context: None,
            dry_run: false,
            timeout_ms: None,
        };

        let result = reader.execute(input).await?;
        assert!(result.success);
        assert_eq!(result.result, test_content);
        assert!(result.formatted_output.is_some());

        Ok(())
    }

    #[tokio::test]
    async fn test_file_reader_natural_language() -> Result<()> {
        let reader = FileReader::new();

        // Test with path in query
        let input = reader
            .parse_natural_language("–ø–æ–∫–∞–∑–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ src/main.rs")
            .await?;
        assert_eq!(input.command, "file_read");
        assert_eq!(input.args.get("path").unwrap(), "src/main.rs");

        // Test without recognizable path
        let input = reader.parse_natural_language("–ø–æ–∫–∞–∑–∞—Ç—å —Ñ–∞–π–ª").await?;
        assert_eq!(input.args.get("path").unwrap(), "–ø–æ–∫–∞–∑–∞—Ç—å —Ñ–∞–π–ª");

        Ok(())
    }

    #[tokio::test]
    async fn test_file_writer_creation() {
        let writer = FileWriter::new();
        let spec = writer.spec();

        assert_eq!(spec.name, "file_write");
        assert!(spec.description.contains("–°–æ–∑–¥–∞—ë—Ç –∏–ª–∏ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Ñ–∞–π–ª"));
        assert!(!spec.examples.is_empty());
    }

    #[tokio::test]
    async fn test_file_writer_default() {
        let writer1 = FileWriter::new();
        let writer2 = FileWriter::new();

        assert_eq!(writer1.spec().name, writer2.spec().name);
    }

    #[tokio::test]
    async fn test_file_writer_write_file() -> Result<()> {
        let temp_dir = TempDir::new().unwrap();
        let file_path = temp_dir.path().join("test_output.txt");
        let test_content = "Test content";

        let writer = FileWriter::new();
        let mut input_args = HashMap::new();
        input_args.insert("path".to_string(), file_path.to_string_lossy().to_string());
        input_args.insert("content".to_string(), test_content.to_string());

        let input = ToolInput {
            command: "file_write".to_string(),
            args: input_args,
            context: None,
            dry_run: false,
            timeout_ms: None,
        };

        let result = writer.execute(input).await?;
        assert!(result.success);
        assert!(result.result.contains("—É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω"));

        // Verify file was actually created
        let written_content = fs::read_to_string(&file_path).unwrap();
        assert_eq!(written_content, test_content);

        Ok(())
    }

    #[tokio::test]
    async fn test_file_writer_missing_path() {
        let writer = FileWriter::new();
        let input_args = HashMap::new(); // Missing path

        let input = ToolInput {
            command: "file_write".to_string(),
            args: input_args,
            context: None,
            dry_run: false,
            timeout_ms: None,
        };

        let result = writer.execute(input).await;
        assert!(result.is_err());
    }

    #[tokio::test]
    async fn test_file_writer_natural_language() -> Result<()> {
        let writer = FileWriter::new();

        let input = writer
            .parse_natural_language("—Å–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª test.txt —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º Hello World")
            .await?;
        assert_eq!(input.command, "file_write");
        assert_eq!(input.args.get("path").unwrap(), "—Å–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª test.txt");
        assert_eq!(input.args.get("content").unwrap(), "Hello World");

        // Test invalid format
        let result = writer.parse_natural_language("—Å–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª test.txt").await;
        assert!(result.is_err());

        Ok(())
    }

    #[test]
    fn test_extract_path_from_query() {
        // Test with .rs file
        let path = extract_path_from_query("–ø–æ–∫–∞–∑–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ src/main.rs");
        assert_eq!(path, Some("src/main.rs".to_string()));

        // Test with .md file
        let path = extract_path_from_query("–ø—Ä–æ—á–∏—Ç–∞—Ç—å README.md");
        assert_eq!(path, Some("README.md".to_string()));

        // Test with path containing slash
        let path = extract_path_from_query("–æ—Ç–∫—Ä—ã—Ç—å file/path.txt");
        assert_eq!(path, Some("file/path.txt".to_string()));

        // Test with backslash (Windows style)
        let path = extract_path_from_query("–ø–æ–∫–∞–∑–∞—Ç—å file\\path.txt");
        assert_eq!(path, Some("file\\path.txt".to_string()));

        // Test with .toml file
        let path = extract_path_from_query("config.toml file");
        assert_eq!(path, Some("config.toml".to_string()));

        // Test with no recognizable path
        let path = extract_path_from_query("–ø–æ–∫–∞–∑–∞—Ç—å —Ñ–∞–π–ª");
        assert_eq!(path, None);
    }

    #[test]
    fn test_format_size() {
        assert_eq!(format_size(100), "100 B");
        assert_eq!(format_size(1024), "1.0 KB");
        assert_eq!(format_size(1536), "1.5 KB");
        assert_eq!(format_size(1024 * 1024), "1.0 MB");
        assert_eq!(format_size(1024 * 1024 * 1024), "1.0 GB");
        assert_eq!(format_size(2048 * 1024 * 1024), "2.0 GB");
    }

    #[tokio::test]
    async fn test_file_delete_removes_and_emits_event() -> Result<()> {
        let temp_dir = TempDir::new().unwrap();
        let file_path = temp_dir.path().join("to_remove.txt");
        let content = "bye";
        fs::write(&file_path, content).unwrap();

        // –ü–æ–¥–ø–∏—à–µ–º—Å—è –Ω–∞ —Å–æ–±—ã—Ç–∏—è fs.diff –¥–æ –¥–µ–π—Å—Ç–≤–∏—è
        let mut rx = events::subscribe(topics::TOPIC_FS_DIFF).await;

        let deleter = FileDeleter::new();
        let input = ToolInput {
            command: "file_delete".to_string(),
            args: HashMap::from([("path".to_string(), file_path.to_string_lossy().to_string())]),
            context: None,
            dry_run: false,
            timeout_ms: None,
        };

        let out = deleter.execute(input).await?;
        assert!(out.success);
        assert!(!file_path.exists());

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–±—ã—Ç–∏–µ
        let evt = tokio::time::timeout(std::time::Duration::from_millis(500), rx.recv()).await??;
        assert_eq!(evt.topic.0, "fs.diff");
        assert_eq!(evt.payload["op"], "delete");
        assert!(evt.payload["bytes"].as_u64().unwrap_or(0) >= content.len() as u64);
        Ok(())
    }
}
