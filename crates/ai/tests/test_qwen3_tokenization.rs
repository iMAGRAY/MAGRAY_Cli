use ai::tokenization::OptimizedTokenizer;
use anyhow::Result;
use std::path::PathBuf;

/// –¢–µ—Å—Ç –ø–æ–ª–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ Qwen3 –º–æ–¥–µ–ª–µ–π
#[tokio::test]
async fn test_qwen3_tokenization_validation() -> Result<()> {
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ tokenizer.json —Ñ–∞–π–ª—ã –¥–æ—Å—Ç—É–ø–Ω—ã
    let qwen3emb_tokenizer = PathBuf::from("crates/memory/models/qwen3emb/tokenizer.json");
    let qwen3_reranker_tokenizer =
        PathBuf::from("crates/memory/models/qwen3_reranker/tokenizer.json");

    if !qwen3emb_tokenizer.exists() {
        println!(
            "‚ö†Ô∏è Qwen3 embedding tokenizer not found at: {:?}",
            qwen3emb_tokenizer
        );
        return Ok(()); // Skip test if model not available
    }

    if !qwen3_reranker_tokenizer.exists() {
        println!(
            "‚ö†Ô∏è Qwen3 reranker tokenizer not found at: {:?}",
            qwen3_reranker_tokenizer
        );
        return Ok(()); // Skip test if model not available
    }

    println!("üöÄ Testing Qwen3 tokenization...");

    // –¢–µ—Å—Ç embedding tokenizer
    let emb_tokenizer = OptimizedTokenizer::new(&qwen3emb_tokenizer, 512)?;

    let test_texts = [
        "Hello world".to_string(),
        "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä".to_string(),
        "ËøôÊòØ‰∏≠ÊñáÊµãËØï".to_string(),
        "Mixed language test: –∞–Ω–≥–ª–∏–π—Å–∫–∏–π, ‰∏≠Êñá, —Ä—É—Å—Å–∫–∏–π".to_string(),
    ];

    println!("Testing embedding tokenizer:");
    for text in &test_texts {
        let tokenized = emb_tokenizer.encode(text)?;
        println!("  Text: '{}' -> {} tokens", text, tokenized.input_ids.len());

        // –ë–∞–∑–æ–≤—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        assert!(
            !tokenized.input_ids.is_empty(),
            "Token IDs should not be empty"
        );
        assert_eq!(
            tokenized.input_ids.len(),
            tokenized.attention_mask.len(),
            "Input IDs and attention mask should have same length"
        );
        assert_eq!(
            tokenized.input_ids.len(),
            tokenized.token_type_ids.len(),
            "Input IDs and token type IDs should have same length"
        );

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π vocab size –¥–ª—è Qwen3
        let vocab_size = emb_tokenizer.vocab_size();
        println!("    Vocab size: {}", vocab_size);
        assert_eq!(vocab_size, 151669, "Qwen3 should have vocab size 151669");
    }

    // –¢–µ—Å—Ç reranker tokenizer
    let rerank_tokenizer = OptimizedTokenizer::new(&qwen3_reranker_tokenizer, 512)?;

    println!("Testing reranker tokenizer:");
    let query = "What is machine learning?";
    let document = "Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed.";
    let combined_text = format!("{}\n{}", query, document);

    let tokenized = rerank_tokenizer.encode(&combined_text)?;
    println!("  Query + Document -> {} tokens", tokenized.input_ids.len());

    // –ü—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è reranker
    assert!(
        !tokenized.input_ids.is_empty(),
        "Reranker tokens should not be empty"
    );
    assert!(
        tokenized.input_ids.len() > 10,
        "Combined text should produce substantial tokens"
    );

    let vocab_size = rerank_tokenizer.vocab_size();
    println!("    Reranker vocab size: {}", vocab_size);
    assert_eq!(
        vocab_size, 151669,
        "Qwen3 reranker should have same vocab size"
    );

    // –¢–µ—Å—Ç batch —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
    println!("Testing batch tokenization:");
    let batch_texts = [
        "First document".to_string(),
        "Second document".to_string(),
        "Third document".to_string(),
    ];

    let batch_str_refs: Vec<&str> = batch_texts.iter().map(|s| s.as_str()).collect();
    let batch_result = emb_tokenizer.encode_batch(&batch_str_refs)?;
    assert_eq!(
        batch_result.input_ids.len(),
        batch_texts.len(),
        "Batch should return same number of results"
    );

    for (i, text) in batch_texts.iter().enumerate() {
        println!(
            "  Batch[{}]: '{}' -> {} tokens",
            i,
            text,
            batch_result.input_ids[i].len()
        );
    }

    // –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback
    println!("‚úÖ All Qwen3 tokenization tests passed!");
    println!("‚úÖ Full tokenizer.json loaded (no simplified fallback)");
    println!("‚úÖ Proper vocab size: 151669");
    println!("‚úÖ Batch processing works");
    println!("‚úÖ Multi-language support verified");

    Ok(())
}

/// –¢–µ—Å—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è Qwen3
#[tokio::test]
async fn test_qwen3_special_tokens() -> Result<()> {
    let tokenizer_path = PathBuf::from("crates/memory/models/qwen3emb/tokenizer.json");

    if !tokenizer_path.exists() {
        println!("‚ö†Ô∏è Skipping special tokens test - tokenizer not found");
        return Ok(());
    }

    let tokenizer = OptimizedTokenizer::new(&tokenizer_path, 512)?;

    // –¢–µ—Å—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ Qwen3
    let special_texts = [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>",
    ];

    println!("Testing Qwen3 special tokens:");
    for text in special_texts {
        let tokenized = tokenizer.encode(text)?;
        println!(
            "  Special: '{}' -> {} tokens",
            text,
            tokenized.input_ids.len()
        );

        // –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–æ–ª–∂–Ω—ã —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å—Å—è
        assert!(
            !tokenized.input_ids.is_empty(),
            "Special tokens should be tokenized"
        );
    }

    println!("‚úÖ Qwen3 special tokens test passed!");
    Ok(())
}
