use ai::{
    auto_device_selector::SmartEmbeddingFactory, gpu_detector::GpuDetector,
    gpu_fallback::GpuFallbackManager, EmbeddingConfig, GpuConfig,
};
use anyhow::Result;
use tracing_subscriber;

/// –¢–µ—Å—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ fallback —Å GPU –Ω–∞ CPU
#[tokio::test]
async fn test_gpu_cpu_fallback() -> Result<()> {
    let _ = tracing_subscriber::fmt::try_init();

    let config = EmbeddingConfig {
        model_name: "qwen3emb".to_string(),
        max_length: 128,
        use_gpu: true,
        batch_size: 8,
        gpu_config: Some(GpuConfig::auto_optimized()),
        ..Default::default()
    };

    // –°–æ–∑–¥–∞—ë–º fallback manager
    let manager = GpuFallbackManager::new(config).await?;

    // –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã
    let test_texts = vec![
        "Short text".to_string(),
        "This is a medium length text for testing embedding generation".to_string(),
        "This is a much longer text that should test the embedding model's ability to handle various sequence lengths and maintain consistent performance across different input sizes".to_string(),
    ];

    // –í—ã–ø–æ–ª–Ω—è–µ–º embedding —Å fallback
    let embeddings = manager
        .embed_batch_with_fallback(test_texts.clone())
        .await?;

    assert_eq!(embeddings.len(), test_texts.len());
    for embedding in &embeddings {
        assert!(!embedding.is_empty());
        assert!(embedding.len() >= 512); // –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è embedding

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é
        let norm: f32 = embedding.iter().map(|x| x * x).sum::<f32>().sqrt();
        assert!(
            (norm - 1.0).abs() < 0.01,
            "Embedding –Ω–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω: norm={}",
            norm
        );
    }

    // –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    let stats = manager.get_stats();
    println!(
        "üìä Fallback stats: GPU successes: {}, CPU fallbacks: {}, success rate: {:.2}%",
        stats.gpu_success_count,
        stats.cpu_fallback_count,
        stats.gpu_success_rate() * 100.0
    );

    Ok(())
}

/// –¢–µ—Å—Ç SmartEmbeddingFactory
#[tokio::test]
async fn test_smart_embedding_factory() -> Result<()> {
    let _ = tracing_subscriber::fmt::try_init();

    let base_config = EmbeddingConfig {
        model_name: "qwen3emb".to_string(),
        max_length: 256,
        ..Default::default()
    };

    // –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã
    let optimized_config = SmartEmbeddingFactory::optimize_config_for_system(base_config.clone());

    println!(
        "üéØ Optimized config: GPU={}, batch_size={}",
        optimized_config.use_gpu, optimized_config.batch_size
    );

    // –°–æ–∑–¥–∞—ë–º —É–º–Ω—ã–π —Å–µ—Ä–≤–∏—Å
    let (service, decision) = SmartEmbeddingFactory::create_optimized(optimized_config).await?;

    println!(
        "‚úÖ Device decision: use_gpu={}, reason: {}",
        decision.use_gpu, decision.reason
    );

    // –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å –Ω–µ–±–æ–ª—å—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    let test_texts = vec![
        "Test embedding generation".to_string(),
        "Another test text".to_string(),
    ];

    let embeddings = service.embed_batch(test_texts).await?;
    assert_eq!(embeddings.len(), 2);

    Ok(())
}

/// –¢–µ—Å—Ç high-throughput pipeline
#[tokio::test]
async fn test_high_throughput_pipeline() -> Result<()> {
    let _ = tracing_subscriber::fmt::try_init();

    let base_config = EmbeddingConfig {
        model_name: "qwen3emb".to_string(),
        max_length: 128,
        batch_size: 32,
        ..Default::default()
    };

    // –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU
    let detector = GpuDetector::detect();
    if !detector.available {
        println!("‚ö†Ô∏è GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º pipeline —Ç–µ—Å—Ç");
        return Ok(());
    }

    // –°–æ–∑–¥–∞—ë–º pipeline
    let pipeline =
        SmartEmbeddingFactory::create_high_throughput_pipeline(base_config, Some(2)).await?;

    // –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–æ–ª—å—à–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞
    let large_batch: Vec<String> = (0..100)
        .map(|i| format!("Test text number {} for pipeline processing", i))
        .collect();

    println!("üöÄ Processing {} texts through pipeline", large_batch.len());

    let start_time = std::time::Instant::now();
    let embeddings = pipeline.process_texts_optimized(large_batch).await?;
    let elapsed = start_time.elapsed();

    println!(
        "‚ö° Pipeline processed {} embeddings in {:?} ({:.1} texts/sec)",
        embeddings.len(),
        elapsed,
        embeddings.len() as f32 / elapsed.as_secs_f32()
    );

    assert_eq!(embeddings.len(), 100);

    // –ü–µ—á–∞—Ç–∞–µ–º –ø–æ–¥—Ä–æ–±–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    pipeline.print_detailed_stats().await;

    Ok(())
}

/// –¢–µ—Å—Ç circuit breaker functionality
#[tokio::test]
async fn test_circuit_breaker_behavior() -> Result<()> {
    let _ = tracing_subscriber::fmt::try_init();

    let config = EmbeddingConfig {
        model_name: "qwen3emb".to_string(),
        max_length: 64,
        use_gpu: true,
        ..Default::default()
    };

    let manager = GpuFallbackManager::new(config).await?;

    // –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –≤ CPU —Ä–µ–∂–∏–º
    manager.force_cpu_mode();

    let test_texts = vec!["Forced CPU mode test".to_string()];
    let embeddings = manager.embed_batch_with_fallback(test_texts).await?;

    assert_eq!(embeddings.len(), 1);

    let stats = manager.get_stats();
    println!(
        "üî¥ Forced CPU stats: fallbacks={}",
        stats.cpu_fallback_count
    );

    // –°–±—Ä–∞—Å—ã–≤–∞–µ–º circuit breaker
    manager.reset_circuit_breaker();

    let test_texts2 = vec!["After circuit breaker reset".to_string()];
    let embeddings2 = manager.embed_batch_with_fallback(test_texts2).await?;

    assert_eq!(embeddings2.len(), 1);

    Ok(())
}

/// –¢–µ—Å—Ç —Ä–∞–∑–ª–∏—á–Ω—ã—Ö GPU providers
#[tokio::test]
async fn test_gpu_providers() -> Result<()> {
    let _ = tracing_subscriber::fmt::try_init();

    let detector = GpuDetector::detect();
    if !detector.available {
        println!("‚ö†Ô∏è GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º provider —Ç–µ—Å—Ç—ã");
        return Ok(());
    }

    // –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã providers
    let provider_types = vec![
        ai::gpu_config::GpuProviderType::Auto,
        ai::gpu_config::GpuProviderType::CUDA,
    ];

    // –î–æ–±–∞–≤–ª—è–µ–º DirectML —Ç–æ–ª—å–∫–æ –Ω–∞ Windows
    #[cfg(windows)]
    let provider_types = {
        let mut types = provider_types;
        types.push(ai::gpu_config::GpuProviderType::DirectML);
        types
    };

    for provider_type in provider_types {
        println!("üß™ Testing provider: {:?}", provider_type);

        let mut gpu_config = GpuConfig::auto_optimized();
        gpu_config.preferred_provider = provider_type.clone();

        let config = EmbeddingConfig {
            model_name: "qwen3emb".to_string(),
            max_length: 64,
            use_gpu: true,
            gpu_config: Some(gpu_config),
            batch_size: 4,
            ..Default::default()
        };

        match GpuFallbackManager::new(config).await {
            Ok(manager) => {
                let test_texts = vec![format!("Test for provider {:?}", provider_type)];
                match manager.embed_batch_with_fallback(test_texts).await {
                    Ok(embeddings) => {
                        println!(
                            "‚úÖ Provider {:?} successful: {} embeddings",
                            provider_type,
                            embeddings.len()
                        );
                    }
                    Err(e) => {
                        println!("‚ö†Ô∏è Provider {:?} embedding failed: {}", provider_type, e);
                    }
                }
            }
            Err(e) => {
                println!("‚ö†Ô∏è Provider {:?} unavailable: {}", provider_type, e);
            }
        }
    }

    Ok(())
}

/// Benchmark —Ç–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
#[tokio::test]
async fn test_performance_benchmark() -> Result<()> {
    let _ = tracing_subscriber::fmt::try_init();

    let config = EmbeddingConfig {
        model_name: "qwen3emb".to_string(),
        max_length: 256,
        batch_size: 16,
        ..Default::default()
    };

    // –°–æ–∑–¥–∞—ë–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å
    let (service, decision) = SmartEmbeddingFactory::create_optimized(config).await?;

    println!(
        "üìä Benchmark running on: {}",
        if decision.use_gpu { "GPU" } else { "CPU" }
    );

    // –¢–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    let benchmark_texts: Vec<String> = (0..50)
        .map(|i| {
            format!(
                "Benchmark text #{} with variable length content to test performance",
                i
            )
        })
        .collect();

    let start = std::time::Instant::now();
    let embeddings = service.embed_batch(benchmark_texts.clone()).await?;
    let elapsed = start.elapsed();

    let throughput = benchmark_texts.len() as f32 / elapsed.as_secs_f32();

    println!("üöÄ Performance results:");
    println!("  üìù Texts processed: {}", embeddings.len());
    println!("  ‚è±Ô∏è Time taken: {:?}", elapsed);
    println!("  üéØ Throughput: {:.1} texts/sec", throughput);
    println!(
        "  üíæ Avg embedding size: {:.0}",
        embeddings.iter().map(|e| e.len()).sum::<usize>() as f32 / embeddings.len() as f32
    );

    assert_eq!(embeddings.len(), benchmark_texts.len());
    assert!(throughput > 0.0);

    Ok(())
}
