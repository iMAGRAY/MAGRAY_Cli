use ai::{
    errors::Result,
    models::ModelType,
    tokenization::simple_qwen3::{SimpleQwen3Tokenizer, TokenizationResult},
    tokenizer::{TokenizeConfig, TokenizerManager, TokenizerType},
};
use arbitrary::{Arbitrary, Unstructured};
use mockall::{mock, predicate::*};
use proptest::prelude::*;
use quickcheck::{quickcheck, TestResult};
use rstest::*;
use serial_test::serial;
use std::{collections::HashSet, sync::Arc};

// Mock –¥–ª—è –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
mock! {
    ExternalTokenizer {}

    #[async_trait::async_trait]
    impl ai::tokenizer::TokenizerTrait for ExternalTokenizer {
        fn tokenize(&self, text: &str) -> Result<Vec<u32>>;
        fn decode(&self, tokens: &[u32]) -> Result<String>;
        fn encode(&self, text: &str) -> Result<Vec<u32>>;
        fn get_vocab_size(&self) -> usize;
    }
}

#[derive(Debug, Clone, Arbitrary)]
struct FuzzInput {
    text: String,
    max_length: Option<usize>,
    truncation: bool,
    padding: bool,
}

#[fixture]
fn tokenize_config() -> TokenizeConfig {
    TokenizeConfig {
        max_length: Some(512),
        truncation: true,
        padding: false,
        add_special_tokens: true,
        return_attention_mask: true,
        return_token_type_ids: false,
    }
}

#[fixture]
fn qwen3_tokenizer() -> SimpleQwen3Tokenizer {
    SimpleQwen3Tokenizer::new().expect("Failed to create Qwen3 tokenizer")
}

#[rstest]
#[tokio::test]
#[serial]
async fn test_tokenizer_manager_creation() -> Result<()> {
    // Arrange - –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤
    let test_cases = vec![
        (TokenizerType::Qwen3, true),
        (TokenizerType::BgeM3, true),
        (TokenizerType::Custom("test".to_string()), false), // –º–æ–∂–µ—Ç –Ω–µ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞—Ç—å
    ];

    for (tokenizer_type, should_succeed) in test_cases {
        // Act - —Å–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        let result = TokenizerManager::new(tokenizer_type.clone()).await;

        // Assert
        if should_succeed {
            assert!(
                result.is_ok(),
                "–°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ {:?} –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —É—Å–ø–µ—à–Ω—ã–º",
                tokenizer_type
            );
        } else {
            // –î–ª—è –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–∏–ø–æ–≤ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—à–∏–±–∫–∞
            match result {
                Ok(_) => {
                    // –ï—Å–ª–∏ –≤—Å–µ –∂–µ —Å–æ–∑–¥–∞–ª—Å—è, –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –æ–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç
                }
                Err(e) => {
                    assert!(
                        e.to_string().contains("not found")
                            || e.to_string().contains("unsupported"),
                        "–û—à–∏–±–∫–∞ –¥–æ–ª–∂–Ω–∞ —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø–æ–¥–¥–µ—Ä–∂–∫–∏: {}",
                        e
                    );
                }
            }
        }
    }

    Ok(())
}

#[rstest]
#[tokio::test]
async fn test_qwen3_tokenizer_basic_functionality(
    qwen3_tokenizer: SimpleQwen3Tokenizer,
) -> Result<()> {
    // Arrange - —Ç–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã —Ä–∞–∑–ª–∏—á–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    let test_cases = vec![
        ("Hello, world!", true),
        ("", true),                          // –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç
        ("–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä!", true),              // unicode
        ("üöÄ Emoji test üéâ", true),          // emoji
        ("Multiple\nlines\nof\ntext", true), // –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        ("Very long ".repeat(100), true),    // –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    ];

    for (text, should_succeed) in test_cases {
        // Act - —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        let result = qwen3_tokenizer.tokenize_with_config(text, None);

        // Assert
        if should_succeed {
            assert!(
                result.is_ok(),
                "–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è '{}' –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —É—Å–ø–µ—à–Ω–æ–π",
                text
            );

            let tokenization = result?;

            // –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–∑–æ–≤—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞
            assert!(
                !tokenization.tokens.is_empty() || text.is_empty(),
                "–¢–æ–∫–µ–Ω—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è –Ω–µ–ø—É—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"
            );

            if let Some(attention_mask) = &tokenization.attention_mask {
                assert_eq!(
                    attention_mask.len(),
                    tokenization.tokens.len(),
                    "–†–∞–∑–º–µ—Ä attention mask –¥–æ–ª–∂–µ–Ω —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ç–æ–∫–µ–Ω–æ–≤"
                );
            }

            // –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –≤–∞–ª–∏–¥–Ω—ã
            let vocab_size = qwen3_tokenizer.get_vocab_size();
            for &token in &tokenization.tokens {
                assert!(
                    (token as usize) < vocab_size,
                    "–¢–æ–∫–µ–Ω {} –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö —Å–ª–æ–≤–∞—Ä—è (—Ä–∞–∑–º–µ—Ä: {})",
                    token,
                    vocab_size
                );
            }
        }
    }

    Ok(())
}

#[rstest]
#[tokio::test]
async fn test_tokenizer_encode_decode_roundtrip(
    qwen3_tokenizer: SimpleQwen3Tokenizer,
) -> Result<()> {
    // Arrange - —Ç–µ–∫—Å—Ç—ã –¥–ª—è roundtrip —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    let test_texts = vec![
        "Simple English text",
        "Text with numbers 123 and symbols !@#",
        "Mixed —è–∑—ã–∫–∏ text with —Ä–∞–∑–Ω—ã–º–∏ alphabets",
    ];

    for original_text in test_texts {
        // Act - encode -> decode —Ü–∏–∫–ª
        let encoded = qwen3_tokenizer.encode(original_text)?;
        let decoded = qwen3_tokenizer.decode(&encoded)?;

        // Assert - –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        // –¢–æ—á–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –∏–∑-–∑–∞ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏,
        // –Ω–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–æ–ª–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å—Å—è

        assert!(
            !encoded.is_empty(),
            "–ö–æ–¥–∏—Ä–æ–≤–∫–∞ –Ω–µ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø—É—Å—Ç–æ–π –¥–ª—è –Ω–µ–ø—É—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"
        );
        assert!(!decoded.is_empty(), "–î–µ–∫–æ–¥–∏—Ä–æ–≤–∫–∞ –Ω–µ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø—É—Å—Ç–æ–π");

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ —Å–ª–æ–≤–∞
        let original_words: HashSet<&str> = original_text.split_whitespace().collect();
        let decoded_words: HashSet<&str> = decoded.split_whitespace().collect();

        // –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Å–ª–æ–≤ –¥–æ–ª–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å—Å—è (—É—á–∏—Ç—ã–≤–∞—è –≤–æ–∑–º–æ–∂–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏)
        let preserved_words = original_words.intersection(&decoded_words).count();
        let total_words = original_words.len();

        if total_words > 0 {
            let preservation_ratio = preserved_words as f64 / total_words as f64;
            assert!(
                preservation_ratio >= 0.7,
                "–ú–∏–Ω–∏–º—É–º 70% —Å–ª–æ–≤ –¥–æ–ª–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å—Å—è –ø—Ä–∏ roundtrip: {:.2}% –¥–ª—è '{}'",
                preservation_ratio * 100.0,
                original_text
            );
        }
    }

    Ok(())
}

#[rstest]
#[tokio::test]
async fn test_tokenizer_max_length_handling(
    qwen3_tokenizer: SimpleQwen3Tokenizer,
    mut tokenize_config: TokenizeConfig,
) -> Result<()> {
    // Arrange - –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–ª–∏–Ω—ã
    let long_text =
        "This is a very long text that will definitely exceed the maximum length limit. "
            .repeat(20);

    let length_limits = vec![10, 50, 100, 512];

    for max_length in length_limits {
        tokenize_config.max_length = Some(max_length);
        tokenize_config.truncation = true;

        // Act - —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–ª–∏–Ω—ã
        let result =
            qwen3_tokenizer.tokenize_with_config(&long_text, Some(tokenize_config.clone()))?;

        // Assert - –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–±–ª—é–¥–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
        assert!(
            result.tokens.len() <= max_length,
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ ({}) –Ω–µ –¥–æ–ª–∂–Ω–æ –ø—Ä–µ–≤—ã—à–∞—Ç—å –º–∞–∫—Å–∏–º—É–º ({})",
            result.tokens.len(),
            max_length
        );

        if let Some(attention_mask) = &result.attention_mask {
            assert!(
                attention_mask.len() <= max_length,
                "–†–∞–∑–º–µ—Ä attention mask –Ω–µ –¥–æ–ª–∂–µ–Ω –ø—Ä–µ–≤—ã—à–∞—Ç—å –º–∞–∫—Å–∏–º—É–º"
            );
        }
    }

    Ok(())
}

#[rstest]
#[tokio::test]
async fn test_tokenizer_padding_functionality(
    qwen3_tokenizer: SimpleQwen3Tokenizer,
    mut tokenize_config: TokenizeConfig,
) -> Result<()> {
    // Arrange - –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã –∏ padding
    let short_texts = vec!["Hi", "Hello", "How are you?"];
    let target_length = 20;

    tokenize_config.max_length = Some(target_length);
    tokenize_config.padding = true;

    for text in short_texts {
        // Act - —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å padding
        let result = qwen3_tokenizer.tokenize_with_config(text, Some(tokenize_config.clone()))?;

        // Assert - –ø—Ä–æ–≤–µ—Ä—è–µ–º padding
        if result.tokens.len() < target_length {
            assert_eq!(
                result.tokens.len(),
                target_length,
                "–ü—Ä–∏ –≤–∫–ª—é—á–µ–Ω–Ω–æ–º padding –¥–ª–∏–Ω–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å target_length"
            );

            // –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–æ–±–∞–≤–ª–µ–Ω—ã padding —Ç–æ–∫–µ–Ω—ã
            let padding_token_count = result
                .tokens
                .iter()
                .filter(|&&token| token == qwen3_tokenizer.get_pad_token_id())
                .count();

            assert!(
                padding_token_count > 0,
                "–î–æ–ª–∂–Ω—ã –±—ã—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω—ã padding —Ç–æ–∫–µ–Ω—ã"
            );
        }
    }

    Ok(())
}

#[rstest]
#[tokio::test]
async fn test_tokenizer_special_tokens(qwen3_tokenizer: SimpleQwen3Tokenizer) -> Result<()> {
    // Arrange - —Ç–µ—Å—Ç —Å–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏ –∏ –±–µ–∑ –Ω–∏—Ö
    let test_text = "Hello world";

    let config_with_special = TokenizeConfig {
        add_special_tokens: true,
        ..Default::default()
    };

    let config_without_special = TokenizeConfig {
        add_special_tokens: false,
        ..Default::default()
    };

    // Act - —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –æ–±–µ–∏–º–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏
    let with_special =
        qwen3_tokenizer.tokenize_with_config(test_text, Some(config_with_special))?;
    let without_special =
        qwen3_tokenizer.tokenize_with_config(test_text, Some(config_without_special))?;

    // Assert - –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–∏—è
    if qwen3_tokenizer.has_special_tokens() {
        assert!(
            with_special.tokens.len() >= without_special.tokens.len(),
            "–° —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–æ–ª—å—à–µ –∏–ª–∏ —Ä–∞–≤–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤"
        );

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        let cls_token = qwen3_tokenizer.get_cls_token_id();
        let sep_token = qwen3_tokenizer.get_sep_token_id();

        if with_special.tokens.contains(&cls_token) {
            assert!(
                !without_special.tokens.contains(&cls_token),
                "CLS —Ç–æ–∫–µ–Ω –Ω–µ –¥–æ–ª–∂–µ–Ω –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤"
            );
        }
    }

    Ok(())
}

#[rstest]
#[tokio::test]
async fn test_tokenizer_concurrent_usage(qwen3_tokenizer: SimpleQwen3Tokenizer) -> Result<()> {
    // Arrange - –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    let tokenizer = Arc::new(qwen3_tokenizer);
    let test_texts: Vec<String> = (0..50)
        .map(|i| format!("Concurrent tokenization test text number {}", i))
        .collect();

    // Act - –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    let tasks: Vec<_> = test_texts
        .into_iter()
        .map(|text| {
            let tokenizer = tokenizer.clone();
            tokio::spawn(async move { tokenizer.tokenize_with_config(&text, None) })
        })
        .collect();

    let results = futures::future::try_join_all(tasks).await?;

    // Assert - –≤—Å–µ –∑–∞–¥–∞—á–∏ –¥–æ–ª–∂–Ω—ã –∑–∞–≤–µ—Ä—à–∞—Ç—å—Å—è —É—Å–ø–µ—à–Ω–æ
    for (i, result) in results.into_iter().enumerate() {
        assert!(
            result.is_ok(),
            "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è {} –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —É—Å–ø–µ—à–Ω–æ–π",
            i
        );

        let tokenization = result?;
        assert!(
            !tokenization.tokens.is_empty(),
            "–†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø—É—Å—Ç—ã–º"
        );
    }

    Ok(())
}

#[rstest]
#[tokio::test]
async fn test_tokenizer_error_handling(qwen3_tokenizer: SimpleQwen3Tokenizer) -> Result<()> {
    // Arrange - –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    let problematic_inputs = vec![
        "\u{FEFF}".repeat(1000), // –º–Ω–æ–≥–æ BOM —Å–∏–º–≤–æ–ª–æ–≤
        "\0".repeat(100),        // null —Å–∏–º–≤–æ–ª—ã
        "\u{200B}".repeat(500),  // zero-width –ø—Ä–æ–±–µ–ª—ã
    ];

    for input in problematic_inputs {
        // Act - —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–ª–µ–º–Ω–æ–≥–æ –≤–≤–æ–¥–∞
        let result = qwen3_tokenizer.tokenize_with_config(&input, None);

        // Assert - –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
        match result {
            Ok(tokenization) => {
                // –ï—Å–ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                assert!(
                    tokenization.tokens.len() < 10000,
                    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ä–∞–∑—É–º–Ω—ã–º –¥–∞–∂–µ –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω–æ–≥–æ –≤–≤–æ–¥–∞"
                );
            }
            Err(e) => {
                // –ï—Å–ª–∏ –µ—Å—Ç—å –æ—à–∏–±–∫–∞, –æ–Ω–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–π
                assert!(
                    !e.to_string().is_empty(),
                    "–û—à–∏–±–∫–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ"
                );
            }
        }
    }

    Ok(())
}

// Property-based —Ç–µ—Å—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
proptest! {
    #[test]
    fn test_tokenization_invariants(
        text in prop::string::string_regex("[\\w\\s\\p{P}]{0,200}").unwrap(),
        max_length in prop::option::of(1usize..=512)
    ) {
        tokio_test::block_on(async {
            let tokenizer = SimpleQwen3Tokenizer::new().unwrap();
            let config = TokenizeConfig {
                max_length,
                truncation: true,
                padding: false,
                add_special_tokens: true,
                return_attention_mask: true,
                return_token_type_ids: false,
            };

            if let Ok(result) = tokenizer.tokenize_with_config(&text, Some(config.clone())) {
                // –ò–Ω–≤–∞—Ä–∏–∞–Ω—Ç: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –º–∞–∫—Å–∏–º—É–º
                if let Some(max_len) = max_length {
                    prop_assert!(result.tokens.len() <= max_len, "Tokens should not exceed max_length");
                }

                // –ò–Ω–≤–∞—Ä–∏–∞–Ω—Ç: –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤–∞–ª–∏–¥–Ω—ã–º–∏
                let vocab_size = tokenizer.get_vocab_size();
                for &token in &result.tokens {
                    prop_assert!((token as usize) < vocab_size, "All tokens should be within vocabulary");
                }

                // –ò–Ω–≤–∞—Ä–∏–∞–Ω—Ç: attention mask —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç–æ–∫–µ–Ω–∞–º
                if let Some(attention_mask) = &result.attention_mask {
                    prop_assert_eq!(attention_mask.len(), result.tokens.len(), "Attention mask length should match tokens");

                    // –í—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ attention mask –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å 0 –∏–ª–∏ 1
                    for &mask_value in attention_mask {
                        prop_assert!(mask_value == 0 || mask_value == 1, "Attention mask values should be 0 or 1");
                    }
                }

                // –ò–Ω–≤–∞—Ä–∏–∞–Ω—Ç: –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –º–æ–∂–µ—Ç –¥–∞–≤–∞—Ç—å –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–ª–∏ —Ç–æ–ª—å–∫–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
                if text.trim().is_empty() {
                    prop_assert!(result.tokens.len() <= 2, "Empty text should produce minimal tokens");
                }
            }
        })?;
    }

    #[test]
    fn test_encode_decode_properties(
        text in prop::string::string_regex("[a-zA-Z0-9 .,!?]{1,100}").unwrap()
    ) {
        tokio_test::block_on(async {
            let tokenizer = SimpleQwen3Tokenizer::new().unwrap();

            if let (Ok(encoded), Ok(decoded)) = (tokenizer.encode(&text), tokenizer.encode(&text).and_then(|tokens| tokenizer.decode(&tokens))) {
                // –ò–Ω–≤–∞—Ä–∏–∞–Ω—Ç: –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Å—Ç–∏—á–µ—Å–∫–∏–º
                let encoded2 = tokenizer.encode(&text).unwrap();
                prop_assert_eq!(encoded, encoded2, "Encoding should be deterministic");

                // –ò–Ω–≤–∞—Ä–∏–∞–Ω—Ç: –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ –¥–æ–ª–∂–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞—Ç—å –¥–ª–∏–Ω—É –Ω–µ–ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
                prop_assert!(decoded.len() <= text.len() * 2, "Decoded text should not be excessively longer");

                // –ò–Ω–≤–∞—Ä–∏–∞–Ω—Ç: –¥–≤–æ–π–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ-–¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω—ã–º
                if let Ok(double_encoded) = tokenizer.encode(&decoded) {
                    if let Ok(double_decoded) = tokenizer.decode(&double_encoded) {
                        // –ü–æ—Å–ª–µ –≤—Ç–æ—Ä–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏
                        let similarity = calculate_similarity(&decoded, &double_decoded);
                        prop_assert!(similarity >= 0.8, "Double encode-decode should be stable");
                    }
                }
            }
        })?;
    }
}

// QuickCheck —Ç–µ—Å—Ç—ã –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
quickcheck! {
    fn qc_tokenization_length_bounds(text: String, max_len: Option<u16>) -> TestResult {
        if text.len() > 1000 { return TestResult::discard(); }

        let max_length = max_len.map(|l| l as usize).filter(|&l| l > 0 && l <= 1024);

        tokio_test::block_on(async {
            let tokenizer = match SimpleQwen3Tokenizer::new() {
                Ok(t) => t,
                Err(_) => return TestResult::discard(),
            };

            let config = TokenizeConfig {
                max_length,
                truncation: true,
                ..Default::default()
            };

            match tokenizer.tokenize_with_config(&text, Some(config)) {
                Ok(result) => {
                    if let Some(max_len) = max_length {
                        TestResult::from_bool(result.tokens.len() <= max_len)
                    } else {
                        TestResult::from_bool(result.tokens.len() <= 10000) // —Ä–∞–∑—É–º–Ω—ã–π –º–∞–∫—Å–∏–º—É–º
                    }
                }
                Err(_) => TestResult::passed(), // –æ—à–∏–±–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–æ–ø—É—Å—Ç–∏–º—ã
            }
        })
    }

    fn qc_vocab_bounds(text: String) -> TestResult {
        if text.is_empty() || text.len() > 200 { return TestResult::discard(); }

        tokio_test::block_on(async {
            let tokenizer = match SimpleQwen3Tokenizer::new() {
                Ok(t) => t,
                Err(_) => return TestResult::discard(),
            };

            match tokenizer.tokenize_with_config(&text, None) {
                Ok(result) => {
                    let vocab_size = tokenizer.get_vocab_size();
                    let all_tokens_valid = result.tokens.iter()
                        .all(|&token| (token as usize) < vocab_size);
                    TestResult::from_bool(all_tokens_valid)
                }
                Err(_) => TestResult::passed(),
            }
        })
    }
}

// Fuzzing-–ø–æ–¥–æ–±–Ω—ã–µ —Ç–µ—Å—Ç—ã —Å arbitrary –¥–∞–Ω–Ω—ã–º–∏
#[test]
fn fuzz_tokenizer_with_arbitrary_input() {
    let mut data = vec![0u8; 1000];
    for _ in 0..100 {
        // –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        for byte in &mut data {
            *byte = fastrand::u8(..);
        }

        let mut unstructured = Unstructured::new(&data);
        if let Ok(fuzz_input) = FuzzInput::arbitrary(&mut unstructured) {
            tokio_test::block_on(async {
                let tokenizer = SimpleQwen3Tokenizer::new().unwrap();

                let config = TokenizeConfig {
                    max_length: fuzz_input.max_length,
                    truncation: fuzz_input.truncation,
                    padding: fuzz_input.padding,
                    ..Default::default()
                };

                // –¢–µ—Å—Ç–∏—Ä—É–µ–º —á—Ç–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–µ –ø–∞–Ω–∏–∫—É–µ—Ç –Ω–∞ –ª—é–±—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                let _ = tokenizer.tokenize_with_config(&fuzz_input.text, Some(config));
            });
        }
    }
}

// –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–æ–≤
fn calculate_similarity(s1: &str, s2: &str) -> f64 {
    let words1: HashSet<&str> = s1.split_whitespace().collect();
    let words2: HashSet<&str> = s2.split_whitespace().collect();

    if words1.is_empty() && words2.is_empty() {
        return 1.0;
    }

    let intersection = words1.intersection(&words2).count();
    let union = words1.union(&words2).count();

    if union == 0 {
        0.0
    } else {
        intersection as f64 / union as f64
    }
}

// Benchmark —Ç–µ—Å—Ç—ã
#[tokio::test]
#[ignore]
async fn benchmark_tokenization_performance() -> Result<()> {
    let tokenizer = SimpleQwen3Tokenizer::new()?;

    let test_texts: Vec<String> = (0..1000)
        .map(|i| format!("Performance benchmark text number {} with various content and sufficient length for realistic testing scenarios", i))
        .collect();

    let start = std::time::Instant::now();

    for text in &test_texts {
        let _ = tokenizer.tokenize_with_config(text, None)?;
    }

    let duration = start.elapsed();
    let throughput = test_texts.len() as f64 / duration.as_secs_f64();

    println!("Tokenization Benchmark:");
    println!("  Texts processed: {}", test_texts.len());
    println!("  Time taken: {:?}", duration);
    println!("  Throughput: {:.2} texts/sec", throughput);

    // –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    assert!(
        throughput > 100.0,
        "–ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –±–æ–ª—å—à–µ 100 —Ç–µ–∫—Å—Ç–æ–≤/—Å–µ–∫"
    );

    Ok(())
}
