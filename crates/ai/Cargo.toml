[package]
name = "ai"
version.workspace = true
edition.workspace = true

[features]
default = ["cpu"]
cpu = []  # CPU-only mode (always available)
gpu = ["ort/cuda", "ort/tensorrt"]  # GPU support with CUDA and TensorRT
openai = []  # OpenAI API integration
onnx = []  # Full ONNX Runtime support
minimal = []  # Minimal build without heavy dependencies

[dependencies]
# Core dependencies
anyhow.workspace = true
serde.workspace = true
serde_json.workspace = true
tokio.workspace = true
tracing.workspace = true
async-trait.workspace = true

# ONNX Runtime for real model inference
ort = { version = "2.0.0-rc.10", features = ["load-dynamic"] }
ndarray = "0.15"

# Tokenization
tokenizers = "0.20"

# HTTP client for remote APIs and model downloading
reqwest = { workspace = true, features = ["stream"] }
tokio-stream = { workspace = true }

# Utilities
parking_lot = "0.12"
dashmap = "5.5"
num_cpus = "1.16"
lazy_static = "1.4"
thread_local = "1.1"
rand = "0.8"

[dev-dependencies]
tempfile.workspace = true
tracing-subscriber.workspace = true
tokio = { workspace = true, features = ["rt-multi-thread", "macros"] }

[[example]]
name = "test_qwen3_models"
path = "examples/test_qwen3_models.rs"