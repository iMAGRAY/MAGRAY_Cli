use crate::{EmbeddingConfig, gpu_memory_pool::{GpuMemoryPool, GPU_MEMORY_POOL}};
use crate::embeddings_gpu::GpuEmbeddingService;
use anyhow::Result;
use std::sync::Arc;
use std::time::Instant;
use tokio::sync::{Semaphore, Mutex};
use tracing::{info, debug};

/// @component: {"k":"C","id":"gpu_pipeline_optimized","t":"Optimized GPU pipeline with memory pooling","m":{"cur":95,"tgt":100,"u":"%"}}
pub struct OptimizedGpuPipelineManager {
    services: Vec<Arc<GpuEmbeddingService>>,
    semaphore: Arc<Semaphore>,
    config: PipelineConfig,
    stats: Arc<Mutex<PipelineStats>>,
    memory_pool: Arc<GpuMemoryPool>,
}

#[derive(Debug, Clone)]
pub struct PipelineConfig {
    pub max_concurrent_batches: usize,
    pub optimal_batch_size: usize,
    pub min_batch_size: usize,
    pub prefetch_enabled: bool,
    pub memory_pooling_enabled: bool,
    pub adaptive_batching: bool,
}

impl Default for PipelineConfig {
    fn default() -> Self {
        Self {
            max_concurrent_batches: 4,
            optimal_batch_size: 64,
            min_batch_size: 8,
            prefetch_enabled: true,
            memory_pooling_enabled: true,
            adaptive_batching: true,
        }
    }
}

#[derive(Debug, Default, Clone)]
pub struct PipelineStats {
    pub total_batches_processed: u64,
    pub total_texts_processed: u64,
    pub total_processing_time_ms: u64,
    pub avg_batch_size: f32,
    pub max_concurrent_batches_used: usize,
    pub memory_pool_hits: u64,
    pub memory_pool_misses: u64,
    pub cache_efficiency: f32,
}

impl PipelineStats {
    pub fn throughput_per_second(&self) -> f32 {
        if self.total_processing_time_ms == 0 {
            0.0
        } else {
            (self.total_texts_processed as f32 / self.total_processing_time_ms as f32) * 1000.0
        }
    }
    
    pub fn memory_pool_efficiency(&self) -> f32 {
        let total = self.memory_pool_hits + self.memory_pool_misses;
        if total == 0 {
            0.0
        } else {
            self.memory_pool_hits as f32 / total as f32
        }
    }
}

impl OptimizedGpuPipelineManager {
    pub async fn new(embedding_config: EmbeddingConfig, config: PipelineConfig) -> Result<Self> {
        info!("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OptimizedGpuPipelineManager");
        info!("‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: max_concurrent={}, optimal_batch={}, memory_pooling={}", 
            config.max_concurrent_batches, config.optimal_batch_size, config.memory_pooling_enabled);

        // –°–æ–∑–¥–∞–µ–º –ø—É–ª GPU —Å–µ—Ä–≤–∏—Å–æ–≤ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        let mut services = Vec::new();
        for i in 0..config.max_concurrent_batches {
            debug!("üîß –°–æ–∑–¥–∞–Ω–∏–µ GPU service #{}", i + 1);
            let service = Arc::new(GpuEmbeddingService::new(embedding_config.clone()).await?);
            services.push(service);
        }

        // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º memory pool –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω
        if config.memory_pooling_enabled {
            info!("üíæ Memory pooling –≤–∫–ª—é—á–µ–Ω");
            GPU_MEMORY_POOL.print_stats();
        }

        Ok(Self {
            services,
            semaphore: Arc::new(Semaphore::new(config.max_concurrent_batches)),
            config,
            stats: Arc::new(Mutex::new(PipelineStats::default())),
            memory_pool: Arc::new(GpuMemoryPool::new(2 * 1024 * 1024 * 1024)), // 2GB –ª–æ–∫–∞–ª—å–Ω—ã–π –ø—É–ª
        })
    }

    /// –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø–∞–π–ø–ª–∞–π–Ω–æ–º
    pub async fn process_texts_optimized(&self, texts: Vec<String>) -> Result<Vec<Vec<f32>>> {
        let total_texts = texts.len();
        let start_time = Instant::now();
        
        info!("üè≠ –ù–∞—á–∏–Ω–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É {} —Ç–µ–∫—Å—Ç–æ–≤", total_texts);

        // –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞
        let effective_batch_size = if self.config.adaptive_batching {
            self.calculate_adaptive_batch_size(total_texts).await
        } else {
            self.config.optimal_batch_size
        };

        info!("üìä –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {}", effective_batch_size);

        // –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ –±–∞—Ç—á–∏ —Å —É—á–µ—Ç–æ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        let batches: Vec<Vec<String>> = texts
            .chunks(effective_batch_size)
            .map(|chunk| chunk.to_vec())
            .collect();

        info!("üì¶ –°–æ–∑–¥–∞–Ω–æ {} –±–∞—Ç—á–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏", batches.len());

        // –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π —Å —Å–µ–º–∞—Ñ–æ—Ä–æ–º
        let mut handles = Vec::new();
        let mut all_embeddings = Vec::with_capacity(total_texts);

        // –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∞–ª–ª–æ–∫–∞—Ü–∏—è —Å memory pooling
        if self.config.memory_pooling_enabled {
            let estimated_memory = total_texts * 1024 * std::mem::size_of::<f32>(); // –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            debug!("üíæ –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –≤—ã–¥–µ–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏: {} MB", estimated_memory / 1024 / 1024);
        }

        for (batch_id, batch) in batches.into_iter().enumerate() {
            let permit = self.semaphore.clone().acquire_owned().await?;
            let service = self.services[batch_id % self.services.len()].clone();
            let stats = self.stats.clone();
            let batch_size = batch.len();

            let handle = tokio::spawn(async move {
                let _permit = permit; // –î–µ—Ä–∂–∏–º permit –¥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                let batch_start = Instant::now();
                
                debug!("üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ #{} —Ä–∞–∑–º–µ—Ä–æ–º {}", batch_id, batch_size);
                
                let result = service.embed_batch(batch).await;
                let batch_elapsed = batch_start.elapsed();
                
                // –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                let mut stats_guard = stats.lock().await;
                stats_guard.total_batches_processed += 1;
                stats_guard.total_texts_processed += batch_size as u64;
                stats_guard.total_processing_time_ms += batch_elapsed.as_millis() as u64;
                stats_guard.avg_batch_size = (stats_guard.avg_batch_size * (stats_guard.total_batches_processed - 1) as f32 
                    + batch_size as f32) / stats_guard.total_batches_processed as f32;
                
                if batch_id < stats_guard.max_concurrent_batches_used {
                    stats_guard.max_concurrent_batches_used = batch_id + 1;
                }
                
                debug!("‚úÖ –ë–∞—Ç—á #{} –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {:?}", batch_id, batch_elapsed);
                
                (batch_id, result)
            });
            
            handles.push(handle);
        }

        // –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
        let mut batch_results = Vec::new();
        for handle in handles {
            let (batch_id, result) = handle.await?;
            batch_results.push((batch_id, result?));
        }

        // –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ batch_id –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞
        batch_results.sort_by_key(|(batch_id, _)| *batch_id);

        // –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for (_, embeddings) in batch_results {
            all_embeddings.extend(embeddings);
        }

        let total_elapsed = start_time.elapsed();
        
        // –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        let stats = self.get_stats().await;
        
        info!("üéØ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:");
        info!("  üìä –í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤: {}", total_texts);
        info!("  ‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {:?}", total_elapsed);
        info!("  üöÄ –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {:.1} —Ç–µ–∫—Å—Ç–æ–≤/—Å–µ–∫", total_texts as f32 / total_elapsed.as_secs_f32());
        info!("  üìà –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {:.1}", stats.avg_batch_size);
        info!("  üíæ Memory pool efficiency: {:.1}%", stats.memory_pool_efficiency() * 100.0);
        info!("  üîÑ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å: {}", stats.max_concurrent_batches_used);

        // –ü–µ—á–∞—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É memory pool
        if self.config.memory_pooling_enabled {
            info!("üíæ –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Memory Pool:");
            GPU_MEMORY_POOL.print_stats();
        }

        Ok(all_embeddings)
    }

    /// –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    async fn calculate_adaptive_batch_size(&self, total_texts: usize) -> usize {
        let stats = self.stats.lock().await;
        let base_batch_size = self.config.optimal_batch_size;
        
        // –ï—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ –∏–ª–∏ –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä
        if stats.total_batches_processed < 3 {
            return base_batch_size;
        }
        
        let current_throughput = stats.throughput_per_second();
        
        // –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        let adaptive_size = if current_throughput > 50.0 {
            // –í—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å - –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å batch size
            (base_batch_size as f32 * 1.2) as usize
        } else if current_throughput < 10.0 {
            // –ù–∏–∑–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å - —É–º–µ–Ω—å—à–∞–µ–º batch size
            (base_batch_size as f32 * 0.8) as usize
        } else {
            base_batch_size
        };
        
        // –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑—É–º–Ω—ã–º–∏ –ø—Ä–µ–¥–µ–ª–∞–º–∏
        adaptive_size
            .max(self.config.min_batch_size)
            .min(total_texts)
            .min(256) // –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑—É–º–Ω—ã–π batch size
    }

    /// –ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–π–ø–ª–∞–π–Ω–∞
    pub async fn get_stats(&self) -> PipelineStats {
        let stats = self.stats.lock().await;
        
        // –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É memory pool
        let pool_stats = GPU_MEMORY_POOL.get_stats();
        let mut result = stats.clone();
        result.memory_pool_hits = pool_stats.hits;
        result.memory_pool_misses = pool_stats.misses;
        
        result
    }

    /// –ü–µ—á–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    pub async fn print_detailed_stats(&self) {
        let stats = self.get_stats().await;
        
        info!("üìä –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ OptimizedGpuPipeline:");
        info!("  üè≠ –í—Å–µ–≥–æ –±–∞—Ç—á–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {}", stats.total_batches_processed);
        info!("  üìù –í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {}", stats.total_texts_processed);
        info!("  ‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {}ms", stats.total_processing_time_ms);
        info!("  üöÄ –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {:.1} —Ç–µ–∫—Å—Ç–æ–≤/—Å–µ–∫", stats.throughput_per_second());
        info!("  üìà –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {:.1}", stats.avg_batch_size);
        info!("  üîÑ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å: {}", stats.max_concurrent_batches_used);
        info!("  üíæ Memory pool hits: {}", stats.memory_pool_hits);
        info!("  üíæ Memory pool misses: {}", stats.memory_pool_misses);
        info!("  üíæ Memory pool efficiency: {:.1}%", stats.memory_pool_efficiency() * 100.0);
    }

    /// –û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ –∏ memory pool
    pub async fn cleanup(&self) {
        info!("üßπ –û—á–∏—Å—Ç–∫–∞ OptimizedGpuPipelineManager...");
        
        if self.config.memory_pooling_enabled {
            GPU_MEMORY_POOL.clear_unused();
            info!("üíæ Memory pool –æ—á–∏—â–µ–Ω");
        }
        
        info!("‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞");
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::GpuConfig;

    #[tokio::test]
    async fn test_optimized_pipeline() {
        let gpu_config = GpuConfig::auto_optimized();
        let embedding_config = EmbeddingConfig {
            model_name: "qwen3emb".to_string(),
            max_length: 256,
            embedding_dim: Some(1024),
            use_gpu: true,
            gpu_config: Some(gpu_config),
            ..Default::default()
        };

        let pipeline_config = PipelineConfig {
            max_concurrent_batches: 2,
            optimal_batch_size: 16,
            memory_pooling_enabled: true,
            adaptive_batching: true,
            ..Default::default()
        };

        // –°–æ–∑–¥–∞–µ–º pipeline - –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è –∏–∑-–∑–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
        let pipeline = OptimizedGpuPipelineManager::new(embedding_config, pipeline_config).await;
        
        // –í —Ç–µ—Å—Ç–æ–≤–æ–π —Å—Ä–µ–¥–µ –º–æ–∂–µ—Ç –Ω–µ –±—ã—Ç—å GPU, –ø–æ—ç—Ç–æ–º—É –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä–∏–º —á—Ç–æ —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–µ –ø–∞–¥–∞–µ—Ç
        match pipeline {
            Ok(pipeline) => {
                let stats = pipeline.get_stats().await;
                assert_eq!(stats.total_batches_processed, 0);
                println!("‚úÖ OptimizedGpuPipeline —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ");
            }
            Err(e) => {
                println!("‚ö†Ô∏è GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤ —Ç–µ—Å—Ç–æ–≤–æ–π —Å—Ä–µ–¥–µ: {}", e);
                // –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è CI/CD –æ–∫—Ä—É–∂–µ–Ω–∏—è –±–µ–∑ GPU
            }
        }
    }
}