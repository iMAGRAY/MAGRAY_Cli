#[cfg(feature = "gpu")]
use crate::gpu_detector::{GpuDetector, GpuDevice};
use crate::EmbeddingConfig;
use anyhow::Result;
#[cfg(not(feature = "gpu"))]
use dummy_gpu::GpuDetector;
use std::time::Instant;
#[cfg(feature = "gpu")]
use tracing::warn;
use tracing::{debug, info};

#[cfg(not(feature = "gpu"))]
mod dummy_gpu {
    use serde::{Deserialize, Serialize};

    #[derive(Debug, Clone, Serialize, Deserialize)]
    pub struct GpuDetector {
        pub available: bool,
        pub devices: Vec<GpuDevice>,
        pub cuda_version: String,
        pub driver_version: String,
    }

    #[derive(Debug, Clone, Serialize, Deserialize)]
    pub struct GpuDevice {
        pub index: u32,
        pub name: String,
        pub compute_capability: String,
        pub total_memory_mb: u64,
        pub free_memory_mb: u64,
        pub temperature_c: Option<u32>,
        pub utilization_percent: Option<u32>,
        pub power_draw_w: Option<f32>,
    }

    impl GpuDetector {
        pub fn has_sufficient_memory(&self, _required_mb: u64) -> bool {
            false
        }
    }
}

#[cfg(not(feature = "gpu"))]
fn create_dummy_detector() -> GpuDetector {
    GpuDetector {
        available: false,
        devices: Vec::new(),
        cuda_version: "N/A".to_string(),
        driver_version: "N/A".to_string(),
    }
}

#[derive(Debug, Clone)]
pub struct AutoDeviceSelector {
    /// –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–∞ –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∞
    benchmark_size: usize,
    /// –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ GPU –¥–ª—è –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, 2.0 = 2x –±—ã—Å—Ç—Ä–µ–µ)
    min_gpu_speedup: f32,
    /// –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–±–æ—Ä–∞
    cached_decision: Option<DeviceDecision>,
}

#[derive(Debug, Clone)]
pub struct DeviceDecision {
    pub use_gpu: bool,
    pub reason: String,
    pub cpu_score: f32,
    pub gpu_score: Option<f32>,
    pub recommended_batch_size: usize,
}

impl Default for AutoDeviceSelector {
    fn default() -> Self {
        Self {
            benchmark_size: 100,
            min_gpu_speedup: 1.5, // GPU –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –º–∏–Ω–∏–º—É–º –≤ 1.5 —Ä–∞–∑–∞ –±—ã—Å—Ç—Ä–µ–µ
            cached_decision: None,
        }
    }
}

impl AutoDeviceSelector {
    pub fn new() -> Self {
        Self::default()
    }

    /// –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±—Ä–∞—Ç—å –ª—É—á—à–µ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    pub async fn select_device(&mut self, config: &EmbeddingConfig) -> Result<DeviceDecision> {
        // –ï—Å–ª–∏ –µ—Å—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if let Some(ref decision) = self.cached_decision {
            debug!(
                "üìã –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ: GPU={}",
                decision.use_gpu
            );
            return Ok(decision.clone());
        }

        info!("üîç –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ (CPU vs GPU)...");

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU
        #[cfg(feature = "gpu")]
        let detector = GpuDetector::detect();
        #[cfg(not(feature = "gpu"))]
        let detector = create_dummy_detector();
        if !detector.available {
            let decision = DeviceDecision {
                use_gpu: false,
                reason: "GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω".to_string(),
                cpu_score: 0.0,
                gpu_score: None,
                recommended_batch_size: num_cpus::get().min(32),
            };
            self.cached_decision = Some(decision.clone());
            return Ok(decision);
        }

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ –ø–∞–º—è—Ç–∏ GPU
        let required_memory = 1000; // ~1GB –¥–ª—è –º–æ–¥–µ–ª–∏
        if !detector.has_sufficient_memory(required_memory) {
            let decision = DeviceDecision {
                use_gpu: false,
                reason: format!("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ GPU –ø–∞–º—è—Ç–∏ (–Ω—É–∂–Ω–æ {required_memory} MB)"),
                cpu_score: 0.0,
                gpu_score: None,
                recommended_batch_size: num_cpus::get().min(32),
            };
            self.cached_decision = Some(decision.clone());
            return Ok(decision);
        }

        // –ó–∞–ø—É—Å–∫–∞–µ–º –±–µ–Ω—á–º–∞—Ä–∫
        info!("‚ö° –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...");

        // –¢–µ—Å—Ç CPU
        let cpu_score = self.benchmark_cpu(config).await?;
        info!("üíª CPU score: {:.2} items/sec", cpu_score);

        // –¢–µ—Å—Ç GPU
        let gpu_score = self.benchmark_gpu(config).await?;
        info!("üéÆ GPU score: {:.2} items/sec", gpu_score);

        // –ü—Ä–∏–Ω–∏–º–∞–µ–º —Ä–µ—à–µ–Ω–∏–µ
        let speedup = gpu_score / cpu_score;
        let use_gpu = speedup >= self.min_gpu_speedup;

        let decision = DeviceDecision {
            use_gpu,
            reason: if use_gpu {
                format!("GPU –±—ã—Å—Ç—Ä–µ–µ –≤ {speedup:.1}x —Ä–∞–∑")
            } else {
                format!("GPU –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±—ã—Å—Ç—Ä–µ–µ (—Ç–æ–ª—å–∫–æ {speedup:.1}x)")
            },
            cpu_score,
            gpu_score: Some(gpu_score),
            recommended_batch_size: if use_gpu {
                // –î–ª—è GPU –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª—å—à–∏–π batch size
                match detector.devices.first().map(|d| d.total_memory_mb) {
                    Some(mem) if mem >= 16000 => 256,
                    Some(mem) if mem >= 8000 => 128,
                    Some(mem) if mem >= 4000 => 64,
                    _ => 32,
                }
            } else {
                num_cpus::get().min(32)
            },
        };

        info!(
            "‚úÖ –†–µ—à–µ–Ω–∏–µ: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å {} ({})",
            if decision.use_gpu { "GPU" } else { "CPU" },
            decision.reason
        );
        info!(
            "üìä –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π batch size: {}",
            decision.recommended_batch_size
        );

        self.cached_decision = Some(decision.clone());
        Ok(decision)
    }

    /// –ë–µ–Ω—á–º–∞—Ä–∫ CPU
    async fn benchmark_cpu(&self, config: &EmbeddingConfig) -> Result<f32> {
        use crate::embeddings_cpu::CpuEmbeddingService;

        // –°–æ–∑–¥–∞—ë–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è CPU
        let mut cpu_config = config.clone();
        cpu_config.use_gpu = false;
        cpu_config.batch_size = num_cpus::get().min(32);

        // –°–æ–∑–¥–∞—ë–º CPU —Å–µ—Ä–≤–∏—Å
        let service = CpuEmbeddingService::new(cpu_config)?;

        // –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        let test_texts: Vec<String> = (0..self.benchmark_size)
            .map(|i| {
                format!(
                    "This is test text number {i} for benchmarking embedding performance on CPU"
                )
            })
            .collect();

        // –ü—Ä–æ–≥—Ä–µ–≤
        let warmup_texts: Vec<String> = test_texts.iter().take(10).cloned().collect();
        let _ = service.embed_batch(&warmup_texts)?;

        // –ó–∞–ø—É—Å–∫–∞–µ–º –±–µ–Ω—á–º–∞—Ä–∫
        let start = Instant::now();
        let _ = service.embed_batch(&test_texts)?;
        let elapsed = start.elapsed().as_secs_f32();

        let score = self.benchmark_size as f32 / elapsed;
        Ok(score)
    }

    /// –ë–µ–Ω—á–º–∞—Ä–∫ GPU
    async fn benchmark_gpu(&self, _config: &EmbeddingConfig) -> Result<f32> {
        #[cfg(not(feature = "gpu"))]
        {
            // –ï—Å–ª–∏ GPU –Ω–µ –≤–∫–ª—é—á–µ–Ω –ø—Ä–∏ –∫–æ–º–ø–∏–ª—è—Ü–∏–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º 0
            Ok(0.0)
        }

        #[cfg(feature = "gpu")]
        {
            use crate::embeddings_gpu::GpuEmbeddingService;

            // –°–æ–∑–¥–∞—ë–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è GPU
            let mut gpu_config = _config.clone();
            gpu_config.use_gpu = true;
            gpu_config.gpu_config = Some(crate::GpuConfig::auto_optimized());

            // –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π batch size –¥–ª—è GPU
            #[cfg(feature = "gpu")]
            let detector = GpuDetector::detect();
            #[cfg(not(feature = "gpu"))]
            let detector = crate::types::GpuDetectionResult::default();
            if let Some(gpu) = detector.devices.first() {
                gpu_config.batch_size = match gpu.total_memory_mb {
                    mem if mem >= 16000 => 256,
                    mem if mem >= 8000 => 128,
                    mem if mem >= 4000 => 64,
                    _ => 32,
                };
            }

            // –°–æ–∑–¥–∞—ë–º GPU —Å–µ—Ä–≤–∏—Å
            let service = match GpuEmbeddingService::new(gpu_config).await {
                Ok(s) => s,
                Err(e) => {
                    warn!("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å GPU —Å–µ—Ä–≤–∏—Å: {}", e);
                    return Ok(0.0);
                }
            };

            // –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            let test_texts: Vec<String> = (0..self.benchmark_size)
                .map(|i| {
                    format!(
                        "This is test text number {} for benchmarking embedding performance on GPU",
                        i
                    )
                })
                .collect();

            // –ü—Ä–æ–≥—Ä–µ–≤ GPU
            let warmup_texts = test_texts.iter().take(10).cloned().collect();
            let _ = service.embed_batch(warmup_texts).await?;

            // –ó–∞–ø—É—Å–∫–∞–µ–º –±–µ–Ω—á–º–∞—Ä–∫
            let start = Instant::now();
            let _ = service.embed_batch(test_texts).await?;
            let elapsed = start.elapsed().as_secs_f32();

            let score = self.benchmark_size as f32 / elapsed;
            Ok(score)
        }
    }

    /// –°–±—Ä–æ—Å–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ
    pub fn reset_cache(&mut self) {
        self.cached_decision = None;
    }
}

/// –£–º–Ω–∞—è —Ñ–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è embedding —Å–µ—Ä–≤–∏—Å–∞ —Å fallback
pub struct SmartEmbeddingFactory;

impl SmartEmbeddingFactory {
    /// –°–æ–∑–¥–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π embedding —Å–µ—Ä–≤–∏—Å —Å GPU/CPU fallback
    pub async fn create_optimized(
        base_config: EmbeddingConfig,
    ) -> Result<(Box<dyn EmbeddingServiceTrait>, DeviceDecision)> {
        let mut selector = AutoDeviceSelector::new();
        let decision = selector.select_device(&base_config).await?;

        #[cfg(feature = "gpu")]
        {
            // –ü—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å –Ω–∞–¥—ë–∂–Ω—ã–π —Å–µ—Ä–≤–∏—Å —Å fallback
            let fallback_service =
                crate::gpu_fallback::GpuFallbackManager::new(base_config.clone()).await?;

            info!(
                "‚úÖ –°–æ–∑–¥–∞–Ω SmartEmbeddingService —Å {} fallback",
                if decision.use_gpu {
                    "GPU-primary"
                } else {
                    "CPU-only"
                }
            );

            Ok((Box::new(fallback_service), decision))
        }

        #[cfg(not(feature = "gpu"))]
        {
            // CPU-only –≤–µ—Ä—Å–∏—è
            use crate::embeddings_cpu::CpuEmbeddingService;
            let cpu_service = CpuEmbeddingService::new(base_config)?;
            info!("‚úÖ –°–æ–∑–¥–∞–Ω CPU-only EmbeddingService");
            Ok((Box::new(cpu_service), decision))
        }
    }

    #[cfg(feature = "gpu")]
    /// –°–æ–∑–¥–∞—Ç—å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è high-throughput —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤
    pub async fn create_high_throughput_pipeline(
        base_config: EmbeddingConfig,
        max_concurrent_batches: Option<usize>,
    ) -> Result<crate::gpu_pipeline::GpuPipelineManager> {
        use crate::gpu_pipeline::{GpuPipelineManager, PipelineConfig};

        #[cfg(feature = "gpu")]
        // –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –±–∞—Ç—á–µ–π
        let detector = crate::gpu_detector::GpuDetector::detect();
        #[cfg(not(feature = "gpu"))]
        let detector = crate::types::GpuDetectionResult::default();
        let optimal_concurrency = if detector.available {
            max_concurrent_batches.unwrap_or(4.min(detector.devices.len() * 2))
        } else {
            1 // –û–¥–∏–Ω CPU stream
        };

        let pipeline_config = PipelineConfig {
            max_concurrent_batches: optimal_concurrency,
            optimal_batch_size: if detector.available { 64 } else { 16 },
            adaptive_batching: true,
            memory_pooling_enabled: true,
            prefetch_enabled: true,
            ..Default::default()
        };

        info!(
            "üöÄ –°–æ–∑–¥–∞–Ω high-throughput pipeline: {} concurrent batches",
            optimal_concurrency
        );

        GpuPipelineManager::new(base_config, pipeline_config).await
    }

    /// –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∏—Å—Ç–µ–º—ã
    pub fn optimize_config_for_system(mut config: EmbeddingConfig) -> EmbeddingConfig {
        #[cfg(feature = "gpu")]
        let detector = crate::gpu_detector::GpuDetector::detect();
        #[cfg(not(feature = "gpu"))]
        let detector = create_dummy_detector();

        if detector.available {
            // –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –¥–ª—è GPU
            let best_device = detector.devices.iter().max_by_key(|d| d.free_memory_mb);

            if let Some(device) = best_device {
                config.use_gpu = true;
                config.batch_size = match device.total_memory_mb {
                    mem if mem >= 16000 => 128, // 16GB+ GPU
                    mem if mem >= 8000 => 64,   // 8GB+ GPU
                    mem if mem >= 4000 => 32,   // 4GB+ GPU
                    _ => 16,                    // –ú–µ–Ω—å—à–µ 4GB
                };

                #[cfg(feature = "gpu")]
                {
                    let mut gpu_config = crate::GpuConfig::auto_optimized();
                    gpu_config.preferred_provider = if cfg!(windows) {
                        crate::gpu_config::GpuProviderType::Auto // –ü—Ä–æ–±—É–µ–º CUDA -> DirectML -> OpenVINO
                    } else {
                        crate::gpu_config::GpuProviderType::CUDA // Linux/macOS: CUDA -> OpenVINO
                    };
                    config.gpu_config = Some(gpu_config);
                }
                #[cfg(not(feature = "gpu"))]
                {
                    // CPU-only —Å–±–æ—Ä–∫–∞
                    config.gpu_config = None;
                }

                info!(
                    "‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è GPU: batch_size={}, device={}",
                    config.batch_size, device.name
                );
            }
        } else {
            // –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –¥–ª—è CPU
            config.use_gpu = false;
            config.batch_size = num_cpus::get().min(16); // –ù–µ –±–æ–ª—å—à–µ 16 –¥–ª—è CPU
            config.gpu_config = None;

            info!(
                "‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è CPU: batch_size={}",
                config.batch_size
            );
        }

        config
    }
}

use async_trait::async_trait;

/// Trait –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏ embedding —Å–µ—Ä–≤–∏—Å–æ–≤
#[async_trait]
pub trait EmbeddingServiceTrait: Send + Sync {
    async fn embed_batch(&self, texts: Vec<String>) -> Result<Vec<Vec<f32>>>;
}

#[cfg(feature = "gpu")]
#[async_trait]
impl EmbeddingServiceTrait for crate::embeddings_gpu::GpuEmbeddingService {
    async fn embed_batch(&self, texts: Vec<String>) -> Result<Vec<Vec<f32>>> {
        self.embed_batch(texts).await
    }
}

#[cfg(feature = "embeddings")]
#[async_trait]
impl EmbeddingServiceTrait for crate::embeddings_cpu::CpuEmbeddingService {
    async fn embed_batch(&self, texts: Vec<String>) -> Result<Vec<Vec<f32>>> {
        let results = self.embed_batch(&texts)?;
        Ok(results.into_iter().map(|r| r.embedding).collect())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_device_selection() {
        let mut selector = AutoDeviceSelector::new();
        let config = EmbeddingConfig::default();

        // Try to select device, but expect it might fail due to missing models
        match selector.select_device(&config).await {
            Ok(decision) => {
                println!("Device decision: {:?}", decision);

                // –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
                let cached = selector.select_device(&config).await.unwrap();
                assert_eq!(decision.use_gpu, cached.use_gpu);
            }
            Err(e) => {
                println!("Expected error without models: {}", e);
                // This is fine - models are not available in test environment
            }
        }
    }
}
