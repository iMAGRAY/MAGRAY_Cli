use crate::embeddings_gpu::GpuEmbeddingService;
use crate::{gpu_memory_pool::GPU_MEMORY_POOL, EmbeddingConfig};
use anyhow::Result;
use std::sync::Arc;
use std::time::Instant;
use tokio::sync::{Mutex, Semaphore};
use tracing::{debug, info};

pub struct GpuPipelineManager {
    services: Vec<Arc<GpuEmbeddingService>>,
    semaphore: Arc<Semaphore>,
    config: PipelineConfig,
    stats: Arc<Mutex<PipelineStats>>,
}

#[derive(Debug, Clone)]
pub struct PipelineConfig {
    pub max_concurrent_batches: usize,
    pub optimal_batch_size: usize,
    pub min_batch_size: usize,
    pub prefetch_enabled: bool,
    pub memory_pooling_enabled: bool,
    pub adaptive_batching: bool,
}

impl Default for PipelineConfig {
    fn default() -> Self {
        Self {
            max_concurrent_batches: 4,
            optimal_batch_size: 64,
            min_batch_size: 8,
            prefetch_enabled: true,
            memory_pooling_enabled: true,
            adaptive_batching: true,
        }
    }
}

#[derive(Debug, Default, Clone)]
pub struct PipelineStats {
    pub total_batches_processed: u64,
    pub total_texts_processed: u64,
    pub total_processing_time_ms: u64,
    pub avg_batch_size: f32,
    pub max_concurrent_batches_used: usize,
    pub memory_pool_hits: u64,
    pub memory_pool_misses: u64,
    pub cache_efficiency: f32,
}

impl PipelineStats {
    pub fn throughput_per_second(&self) -> f32 {
        if self.total_processing_time_ms == 0 {
            0.0
        } else {
            (self.total_texts_processed as f32 / self.total_processing_time_ms as f32) * 1000.0
        }
    }

    pub fn memory_pool_efficiency(&self) -> f32 {
        let total = self.memory_pool_hits + self.memory_pool_misses;
        if total == 0 {
            0.0
        } else {
            self.memory_pool_hits as f32 / total as f32
        }
    }
}

/// –°—Ç—Ä–∞—Ç–µ–≥–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–µ–π –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è —Ü–∏–∫–ª–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
struct BatchProcessingStrategy;

impl BatchProcessingStrategy {
    /// –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞
    fn determine_batch_size(
        config: &PipelineConfig,
        _total_texts: usize,
        stats: &PipelineStats,
    ) -> usize {
        if !config.adaptive_batching {
            return config.optimal_batch_size;
        }

        // Early return –¥–ª—è –ø–µ—Ä–≤—ã—Ö –∑–∞–ø—É—Å–∫–æ–≤
        if stats.total_batches_processed < 3 {
            return config.optimal_batch_size;
        }

        Self::calculate_adaptive_size(config, stats)
    }

    fn calculate_adaptive_size(config: &PipelineConfig, stats: &PipelineStats) -> usize {
        let current_throughput = stats.throughput_per_second();
        let base_size = config.optimal_batch_size;

        let adaptive_size = if current_throughput > 50.0 {
            (base_size as f32 * 1.2) as usize
        } else if current_throughput < 10.0 {
            (base_size as f32 * 0.8) as usize
        } else {
            base_size
        };

        adaptive_size.max(config.min_batch_size).min(256) // –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑—É–º–Ω—ã–π batch size
    }

    /// –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ memory pool –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω
    fn prepare_memory_pool(config: &PipelineConfig, total_texts: usize) {
        if !config.memory_pooling_enabled {
            return;
        }

        let estimated_memory = total_texts * 1024 * std::mem::size_of::<f32>();
        debug!(
            "üíæ –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –≤—ã–¥–µ–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏: {} MB",
            estimated_memory / 1024 / 1024
        );
    }

    /// –°–æ–∑–¥–∞–Ω–∏–µ –±–∞—Ç—á–µ–π –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤
    fn create_batches(texts: Vec<String>, batch_size: usize) -> Vec<Vec<String>> {
        texts
            .chunks(batch_size)
            .map(|chunk| chunk.to_vec())
            .collect()
    }

    /// –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–∞—Ç—á–µ–π
    fn process_batch_results(mut batch_results: Vec<(usize, Vec<Vec<f32>>)>) -> Vec<Vec<f32>> {
        // –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ batch_id –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞
        batch_results.sort_by_key(|(batch_id, _)| *batch_id);

        let mut all_embeddings = Vec::new();
        for (_, embeddings) in batch_results {
            all_embeddings.extend(embeddings);
        }

        all_embeddings
    }

    /// –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    fn log_final_statistics(
        total_texts: usize,
        total_elapsed: std::time::Duration,
        stats: &PipelineStats,
        config: &PipelineConfig,
    ) {
        info!("üéØ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:");
        info!("  üìä –í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤: {}", total_texts);
        info!("  ‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {:?}", total_elapsed);
        info!(
            "  üöÄ –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {:.1} —Ç–µ–∫—Å—Ç–æ–≤/—Å–µ–∫",
            total_texts as f32 / total_elapsed.as_secs_f32()
        );
        info!("  üìà –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {:.1}", stats.avg_batch_size);
        info!(
            "  üíæ Memory pool efficiency: {:.1}%",
            stats.memory_pool_efficiency() * 100.0
        );
        info!(
            "  üîÑ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å: {}",
            stats.max_concurrent_batches_used
        );

        if config.memory_pooling_enabled {
            info!("üíæ –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Memory Pool:");
            let _ = GPU_MEMORY_POOL.print_stats();
        }
    }
}

impl GpuPipelineManager {
    pub async fn new(embedding_config: EmbeddingConfig, config: PipelineConfig) -> Result<Self> {
        info!("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GpuPipelineManager");
        info!(
            "‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: max_concurrent={}, optimal_batch={}, memory_pooling={}",
            config.max_concurrent_batches, config.optimal_batch_size, config.memory_pooling_enabled
        );

        // –°–æ–∑–¥–∞–µ–º –ø—É–ª GPU —Å–µ—Ä–≤–∏—Å–æ–≤ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        let mut services = Vec::new();
        for i in 0..config.max_concurrent_batches {
            debug!("üîß –°–æ–∑–¥–∞–Ω–∏–µ GPU service #{}", i + 1);
            let service = Arc::new(GpuEmbeddingService::new(embedding_config.clone()).await?);
            services.push(service);
        }

        // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º memory pool –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω
        if config.memory_pooling_enabled {
            info!("üíæ Memory pooling –≤–∫–ª—é—á–µ–Ω");
            let _ = GPU_MEMORY_POOL.print_stats();
        }

        Ok(Self {
            services,
            semaphore: Arc::new(Semaphore::new(config.max_concurrent_batches)),
            config,
            stats: Arc::new(Mutex::new(PipelineStats::default())),
        })
    }

    /// –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø–∞–π–ø–ª–∞–π–Ω–æ–º (—É–ø—Ä–æ—â—ë–Ω–Ω–∞—è)
    pub async fn process_texts_optimized(&self, texts: Vec<String>) -> Result<Vec<Vec<f32>>> {
        let total_texts = texts.len();
        let start_time = Instant::now();

        info!(
            "üè≠ –ù–∞—á–∏–Ω–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É {} —Ç–µ–∫—Å—Ç–æ–≤",
            total_texts
        );

        // –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ —á–µ—Ä–µ–∑ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
        let stats = self.get_stats().await;
        let effective_batch_size =
            BatchProcessingStrategy::determine_batch_size(&self.config, total_texts, &stats);
        info!("üìä –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {}", effective_batch_size);

        // –°–æ–∑–¥–∞—ë–º –±–∞—Ç—á–∏ –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞–º—è—Ç—å
        let batches = BatchProcessingStrategy::create_batches(texts, effective_batch_size);
        info!("üì¶ –°–æ–∑–¥–∞–Ω–æ {} –±–∞—Ç—á–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏", batches.len());

        BatchProcessingStrategy::prepare_memory_pool(&self.config, total_texts);

        // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –±–∞—Ç—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        let batch_results = self.process_batches_parallel(batches).await?;

        // –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        let all_embeddings = BatchProcessingStrategy::process_batch_results(batch_results);
        let total_elapsed = start_time.elapsed();

        // –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        let final_stats = self.get_stats().await;
        BatchProcessingStrategy::log_final_statistics(
            total_texts,
            total_elapsed,
            &final_stats,
            &self.config,
        );

        Ok(all_embeddings)
    }

    /// –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –±–∞—Ç—á–µ–π
    async fn process_batches_parallel(
        &self,
        batches: Vec<Vec<String>>,
    ) -> Result<Vec<(usize, Vec<Vec<f32>>)>> {
        let mut handles = Vec::new();

        for (batch_id, batch) in batches.into_iter().enumerate() {
            let permit = self.semaphore.clone().acquire_owned().await?;
            let service = self.services[batch_id % self.services.len()].clone();
            let stats = self.stats.clone();
            let batch_size = batch.len();

            let handle = tokio::spawn(async move {
                let _permit = permit;
                let batch_start = Instant::now();

                debug!("üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ #{} —Ä–∞–∑–º–µ—Ä–æ–º {}", batch_id, batch_size);

                let result = service.embed_batch(batch).await;
                let batch_elapsed = batch_start.elapsed();

                // –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∞—Ç–æ–º–∞—Ä–Ω–æ
                Self::update_batch_stats(stats, batch_size, batch_elapsed, batch_id).await;

                debug!("‚úÖ –ë–∞—Ç—á #{} –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {:?}", batch_id, batch_elapsed);
                (batch_id, result)
            });

            handles.push(handle);
        }

        // –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        let mut batch_results = Vec::new();
        for handle in handles {
            let (batch_id, result) = handle.await?;
            batch_results.push((batch_id, result?));
        }

        Ok(batch_results)
    }

    /// –ê—Ç–æ–º–∞—Ä–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–∞—Ç—á–∞
    async fn update_batch_stats(
        stats: Arc<tokio::sync::Mutex<PipelineStats>>,
        batch_size: usize,
        batch_elapsed: std::time::Duration,
        batch_id: usize,
    ) {
        let mut stats_guard = stats.lock().await;
        stats_guard.total_batches_processed += 1;
        stats_guard.total_texts_processed += batch_size as u64;
        stats_guard.total_processing_time_ms += batch_elapsed.as_millis() as u64;
        stats_guard.avg_batch_size = (stats_guard.avg_batch_size
            * (stats_guard.total_batches_processed - 1) as f32
            + batch_size as f32)
            / stats_guard.total_batches_processed as f32;

        if batch_id < stats_guard.max_concurrent_batches_used {
            stats_guard.max_concurrent_batches_used = batch_id + 1;
        }
    }

    /// –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    #[allow(dead_code)]
    async fn calculate_adaptive_batch_size(&self, total_texts: usize) -> usize {
        let stats = self.stats.lock().await;
        let base_batch_size = self.config.optimal_batch_size;

        // –ï—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ –∏–ª–∏ –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä
        if stats.total_batches_processed < 3 {
            return base_batch_size;
        }

        let current_throughput = stats.throughput_per_second();

        // –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        let adaptive_size = if current_throughput > 50.0 {
            // –í—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å - –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å batch size
            (base_batch_size as f32 * 1.2) as usize
        } else if current_throughput < 10.0 {
            // –ù–∏–∑–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å - —É–º–µ–Ω—å—à–∞–µ–º batch size
            (base_batch_size as f32 * 0.8) as usize
        } else {
            base_batch_size
        };

        // –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑—É–º–Ω—ã–º–∏ –ø—Ä–µ–¥–µ–ª–∞–º–∏
        adaptive_size
            .max(self.config.min_batch_size)
            .min(total_texts)
            .min(256) // –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑—É–º–Ω—ã–π batch size
    }

    /// –ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–π–ø–ª–∞–π–Ω–∞
    pub async fn get_stats(&self) -> PipelineStats {
        let stats = self.stats.lock().await;

        // –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É memory pool
        let pool_stats = GPU_MEMORY_POOL.get_stats().unwrap_or_default();
        let mut result = stats.clone();
        result.memory_pool_hits = pool_stats.hits;
        result.memory_pool_misses = pool_stats.misses;

        result
    }

    /// –ü–µ—á–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    pub async fn print_detailed_stats(&self) {
        let stats = self.get_stats().await;

        info!("üìä –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ OptimizedGpuPipeline:");
        info!(
            "  üè≠ –í—Å–µ–≥–æ –±–∞—Ç—á–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {}",
            stats.total_batches_processed
        );
        info!(
            "  üìù –í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {}",
            stats.total_texts_processed
        );
        info!(
            "  ‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {}ms",
            stats.total_processing_time_ms
        );
        info!(
            "  üöÄ –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {:.1} —Ç–µ–∫—Å—Ç–æ–≤/—Å–µ–∫",
            stats.throughput_per_second()
        );
        info!("  üìà –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {:.1}", stats.avg_batch_size);
        info!(
            "  üîÑ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å: {}",
            stats.max_concurrent_batches_used
        );
        info!("  üíæ Memory pool hits: {}", stats.memory_pool_hits);
        info!("  üíæ Memory pool misses: {}", stats.memory_pool_misses);
        info!(
            "  üíæ Memory pool efficiency: {:.1}%",
            stats.memory_pool_efficiency() * 100.0
        );
    }

    /// –û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ –∏ memory pool
    pub async fn cleanup(&self) {
        info!("üßπ –û—á–∏—Å—Ç–∫–∞ GpuPipelineManager...");

        if self.config.memory_pooling_enabled {
            let _ = GPU_MEMORY_POOL.clear_unused();
            info!("üíæ Memory pool –æ—á–∏—â–µ–Ω");
        }

        info!("‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞");
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::GpuConfig;

    #[tokio::test]
    async fn test_optimized_pipeline() {
        let gpu_config = GpuConfig::auto_optimized();
        let embedding_config = EmbeddingConfig {
            model_name: "qwen3emb".to_string(),
            max_length: 256,
            embedding_dim: Some(1024),
            use_gpu: true,
            gpu_config: Some(gpu_config),
            ..Default::default()
        };

        let pipeline_config = PipelineConfig {
            max_concurrent_batches: 2,
            optimal_batch_size: 16,
            memory_pooling_enabled: true,
            adaptive_batching: true,
            ..Default::default()
        };

        // –°–æ–∑–¥–∞–µ–º pipeline - –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è –∏–∑-–∑–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
        let pipeline = GpuPipelineManager::new(embedding_config, pipeline_config).await;

        // –í —Ç–µ—Å—Ç–æ–≤–æ–π —Å—Ä–µ–¥–µ –º–æ–∂–µ—Ç –Ω–µ –±—ã—Ç—å GPU, –ø–æ—ç—Ç–æ–º—É –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä–∏–º —á—Ç–æ —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–µ –ø–∞–¥–∞–µ—Ç
        match pipeline {
            Ok(pipeline) => {
                let stats = pipeline.get_stats().await;
                assert_eq!(stats.total_batches_processed, 0);
                println!("‚úÖ OptimizedGpuPipeline —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ");
            }
            Err(e) => {
                println!("‚ö†Ô∏è GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤ —Ç–µ—Å—Ç–æ–≤–æ–π —Å—Ä–µ–¥–µ: {}", e);
                // –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è CI/CD –æ–∫—Ä—É–∂–µ–Ω–∏—è –±–µ–∑ GPU
            }
        }
    }
}
