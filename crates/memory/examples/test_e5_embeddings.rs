use anyhow::Result;
use ort::{session::Session, value::Tensor, inputs};
use std::path::PathBuf;

fn main() -> Result<()> {
    println!("=== MULTILINGUAL E5-SMALL: ENCODER-ONLY –ú–û–î–ï–õ–¨ ===\n");
    
    // –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—É—Ç–∏ –∫ DLL
    let dll_path = PathBuf::from(env!("CARGO_MANIFEST_DIR"))
        .parent().unwrap()
        .parent().unwrap()
        .join("scripts")
        .join("onnxruntime")
        .join("lib")
        .join("onnxruntime.dll");
    std::env::set_var("ORT_DYLIB_PATH", dll_path.to_str().unwrap());
    
    // –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ E5
    let model_path = PathBuf::from(env!("CARGO_MANIFEST_DIR"))
        .join("models")
        .join("multilingual-e5-small")
        .join("model.onnx");
    
    let tokenizer_path = PathBuf::from(env!("CARGO_MANIFEST_DIR"))
        .join("models")
        .join("multilingual-e5-small")
        .join("tokenizer.json");
    
    println!("üéØ –ú–û–î–ï–õ–¨: multilingual-e5-small (BertModel - encoder-only!)");
    println!("üìÅ –ú–æ–¥–µ–ª—å: {}", model_path.display());
    println!("üìù –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: {}", tokenizer_path.display());
    println!("‚úÖ –ú–æ–¥–µ–ª—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {}", model_path.exists());
    println!("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {}", tokenizer_path.exists());
    
    if !model_path.exists() || !tokenizer_path.exists() {
        return Err(anyhow::anyhow!("–§–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"));
    }
    
    // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ORT
    println!("\n1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ONNX Runtime...");
    ort::init()
        .with_name("e5_embeddings")
        .commit()?;
    println!("‚úÖ ORT –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω");
    
    // –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏
    println!("\n2. –°–æ–∑–¥–∞–Ω–∏–µ ONNX —Å–µ—Å—Å–∏–∏...");
    let mut session = Session::builder()?
        .with_optimization_level(ort::session::builder::GraphOptimizationLevel::Level3)?
        .with_intra_threads(4)?
        .commit_from_file(&model_path)?;
    
    println!("‚úÖ –°–µ—Å—Å–∏—è —Å–æ–∑–¥–∞–Ω–∞");
    println!("   –í—Ö–æ–¥–æ–≤: {}", session.inputs.len());
    println!("   –í—ã—Ö–æ–¥–æ–≤: {}", session.outputs.len());
    
    // –ê–Ω–∞–ª–∏–∑ –≤—Ö–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏
    println!("\n3. –ê–Ω–∞–ª–∏–∑ –≤—Ö–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏:");
    for (i, input) in session.inputs.iter().enumerate() {
        println!("   {}: {} - {:?}", i, input.name, input.input_type);
    }
    
    // –ê–Ω–∞–ª–∏–∑ –≤—ã—Ö–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏
    println!("\n4. –ê–Ω–∞–ª–∏–∑ –≤—ã—Ö–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏:");
    for (i, output) in session.outputs.iter().enumerate() {
        println!("   {}: {} - {:?}", i, output.name, output.output_type);
    }
    
    // –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤—Ö–æ–¥–æ–≤ –¥–ª—è BERT –º–æ–¥–µ–ª–∏
    println!("\n5. –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤—Ö–æ–¥–æ–≤...");
    
    let seq_len = 8;
    // –ü—Ä–æ—Å—Ç—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è —Ç–µ—Å—Ç–∞ (–±—É–¥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å –ª—é–±—ã–º BERT-like —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º)
    let input_ids = vec![
        101i64,    // [CLS]
        7592,      // "hello"
        2088,      // "world"
        1037,      // "a"  
        2154,      // "test"
        1997,      // "of"
        12645,     // "embeddings"
        102        // [SEP]
    ];
    let attention_mask = vec![1i64; seq_len]; // –í—Å–µ —Ç–æ–∫–µ–Ω—ã –∞–∫—Ç–∏–≤–Ω—ã
    let token_type_ids = vec![0i64; seq_len]; // –í—Å–µ —Ç–æ–∫–µ–Ω—ã –æ–¥–Ω–æ–≥–æ —Ç–∏–ø–∞ (–¥–ª—è BERT)
    
    let input_ids_tensor = Tensor::from_array(([1, seq_len], input_ids))?;
    let attention_mask_tensor = Tensor::from_array(([1, seq_len], attention_mask))?;
    let token_type_ids_tensor = Tensor::from_array(([1, seq_len], token_type_ids))?;
    
    println!("‚úÖ –í—Ö–æ–¥–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã —Å–æ–∑–¥–∞–Ω—ã:");
    println!("   input_ids: [1, {}]", seq_len);
    println!("   attention_mask: [1, {}]", seq_len);
    println!("   token_type_ids: [1, {}]", seq_len);
    
    // –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –ú–û–ú–ï–ù–¢: –ó–∞–ø—É—Å–∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ —Å encoder-only –º–æ–¥–µ–ª—å—é
    println!("\n6. üöÄ –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –¢–ï–°–¢: –ó–∞–ø—É—Å–∫ BERT –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞...");
    
    let outputs = match session.run(inputs![
        "input_ids" => input_ids_tensor,
        "attention_mask" => attention_mask_tensor,
        "token_type_ids" => token_type_ids_tensor
    ]) {
        Ok(outputs) => {
            println!("üéâüéâüéâ –ù–ï–í–ï–†–û–Ø–¢–ù–´–ô –£–°–ü–ï–•!");
            println!("   –ü–æ–ª—É—á–µ–Ω–æ {} –≤—ã—Ö–æ–¥–æ–≤", outputs.len());
            outputs
        },
        Err(e) => {
            println!("‚ùå –û—à–∏–±–∫–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: {}", e);
            return Err(e.into());
        }
    };
    
    // –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    println!("\n7. üéØ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...");
    
    let mut found_embeddings = false;
    
    for (name, output) in outputs.iter() {
        if let Ok((shape, data)) = output.try_extract_tensor::<f32>() {
            let shape_vec: Vec<i64> = (0..shape.len()).map(|i| shape[i]).collect();
            println!("   –í—ã—Ö–æ–¥ '{}': —Ñ–æ—Ä–º–∞ {:?}, –¥–∞–Ω–Ω—ã—Ö {}", name, shape_vec, data.len());
            
            // –î–ª—è BERT –º–æ–¥–µ–ª–∏ –∏—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π [batch, seq, hidden]
            if shape_vec.len() == 3 && shape_vec[0] == 1 && shape_vec[1] == seq_len as i64 {
                let hidden_size = shape_vec[2] as usize;
                
                println!("   üéØ –ù–ê–ô–î–ï–ù–´ –≠–ú–ë–ï–î–î–ò–ù–ì–ò!");
                println!("   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: [1, {}, {}]", seq_len, hidden_size);
                
                // –ü—Ä–∏–º–µ–Ω—è–µ–º mean pooling (—Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è encoder-only –º–æ–¥–µ–ª–µ–π)
                let mut pooled_embedding = vec![0.0f32; hidden_size];
                
                for seq_idx in 0..seq_len {
                    for hidden_idx in 0..hidden_size {
                        let data_idx = seq_idx * hidden_size + hidden_idx;
                        if data_idx < data.len() {
                            pooled_embedding[hidden_idx] += data[data_idx];
                        }
                    }
                }
                
                // –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ
                for val in &mut pooled_embedding {
                    *val /= seq_len as f32;
                }
                
                println!("   ‚úÖ MEAN POOLING –ü–†–ò–ú–ï–ù–ï–ù!");
                println!("   –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {}", pooled_embedding.len());
                
                // –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
                let min = pooled_embedding.iter().fold(f32::INFINITY, |a, &b| a.min(b));
                let max = pooled_embedding.iter().fold(f32::NEG_INFINITY, |a, &b| a.max(b));
                let mean = pooled_embedding.iter().sum::<f32>() / pooled_embedding.len() as f32;
                let norm = pooled_embedding.iter().map(|x| x * x).sum::<f32>().sqrt();
                
                println!("   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: min={:.4}, max={:.4}, mean={:.4}, norm={:.4}", min, max, mean, norm);
                println!("   –û–±—Ä–∞–∑–µ—Ü (–ø–µ—Ä–≤—ã–µ 5): {:?}", &pooled_embedding[..5.min(pooled_embedding.len())]);
                
                // –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
                if norm > 0.0 {
                    let normalized: Vec<f32> = pooled_embedding.iter().map(|x| x / norm).collect();
                    let new_norm = normalized.iter().map(|x| x * x).sum::<f32>().sqrt();
                    
                    println!("   ‚úÖ –≠–ú–ë–ï–î–î–ò–ù–ì –ù–û–†–ú–ê–õ–ò–ó–û–í–ê–ù!");
                    println!("   –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞: {:.6}", new_norm);
                    println!("   üéØ –ì–û–¢–û–í–´–ô –≠–ú–ë–ï–î–î–ò–ù–ì: {} —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å", normalized.len());
                    
                    found_embeddings = true;
                    break;
                }
            }
        }
    }
    
    // –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    println!("\n8. üèÜ –§–ò–ù–ê–õ–¨–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢:");
    
    if found_embeddings {
        println!("üéäüéäüéä –ü–û–õ–ù–ê–Ø –ü–û–ë–ï–î–ê!");
        println!("‚úÖ Multilingual E5-Small –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç!");
        println!("‚úÖ ONNX Runtime 2.0 –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–µ–Ω!");
        println!("‚úÖ Encoder-only –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏–¥–µ–∞–ª—å–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç!");
        println!("‚úÖ –†–µ–∞–ª—å–Ω—ã–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–æ–ª—É—á–µ–Ω—ã!");
        println!("‚úÖ –ù–∏–∫–∞–∫–∏—Ö KV –∫–µ—à–µ–π –Ω–µ –Ω—É–∂–Ω–æ!");
        println!("‚úÖ –ü—Ä–æ—Å—Ç—ã–µ –≤—Ö–æ–¥—ã: input_ids + attention_mask!");
        
        println!("\nüöÄ –ì–û–¢–û–í–û –î–õ–Ø PRODUCTION:");
        println!("- –ë—ã—Å—Ç—Ä—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å (encoder-only)");
        println!("- –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (multilingual-e5)");
        println!("- –ü—Ä–æ—Å—Ç–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è (2 –≤—Ö–æ–¥–∞)");
        println!("- –°—Ç–∞–±–∏–ª—å–Ω—ã–π ORT 2.0 API");
        
    } else {
        println!("‚ö†Ô∏è –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –ø—Ä–æ—à–µ–ª, –Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –æ–∂–∏–¥–∞–µ–º–æ–º —Ñ–æ—Ä–º–∞—Ç–µ");
    }
    
    println!("\nüéØ ENCODER-ONLY –ú–û–î–ï–õ–¨ = –ò–î–ï–ê–õ–¨–ù–û–ï –†–ï–®–ï–ù–ò–ï!");
    
    Ok(())
}