use anyhow::{anyhow, Result};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use sha2::{Sha256, Digest};
use std::fs::{self, File};
use std::io::{BufReader, BufWriter, Write, Read};
use std::path::{Path, PathBuf};
use std::sync::Arc;
use tar::Builder as TarBuilder;
use tracing::{debug, info, warn, error};
use flate2::write::GzEncoder;
use flate2::read::GzDecoder;
use flate2::Compression;

use crate::{
    storage::VectorStore,
    types::{Layer, Record},
    vector_index_hnswlib::HnswRsConfig,
};

/// –í–µ—Ä—Å–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ backup –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
const BACKUP_FORMAT_VERSION: u32 = 1;

/// –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ backup
#[derive(Debug, Serialize, Deserialize)]
pub struct BackupMetadata {
    pub version: u32,
    pub created_at: DateTime<Utc>,
    pub magray_version: String,
    pub layers: Vec<LayerInfo>,
    pub total_records: usize,
    pub index_config: HnswRsConfig,
    /// SHA256 –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è —Å—É–º–º–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö backup'–∞
    pub checksum: Option<String>,
    /// –î–µ—Ç–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Å—É–º–º—ã –ø–æ —Å–ª–æ—è–º
    pub layer_checksums: Option<std::collections::HashMap<String, String>>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct LayerInfo {
    pub layer: Layer,
    pub record_count: usize,
    pub size_bytes: usize,
}

/// –ú–µ–Ω–µ–¥–∂–µ—Ä —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è
pub struct BackupManager {
    base_path: PathBuf,
}

impl BackupManager {
    pub fn new(base_path: impl AsRef<Path>) -> Result<Self> {
        let base_path = base_path.as_ref().to_path_buf();
        
        // –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è backup –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if !base_path.exists() {
            fs::create_dir_all(&base_path)?;
        }
        
        Ok(Self { base_path })
    }

    /// –°–æ–∑–¥–∞—Ç—å –ø–æ–ª–Ω—ã–π backup —Å–∏—Å—Ç–µ–º—ã –ø–∞–º—è—Ç–∏
    pub async fn create_backup(
        &self,
        store: Arc<VectorStore>,
        backup_name: Option<String>,
    ) -> Result<PathBuf> {
        let timestamp = Utc::now().format("%Y%m%d_%H%M%S");
<<<<<<< HEAD
        let backup_name = backup_name.unwrap_or_else(|| format!("backup_{timestamp}"));
        let backup_path = self.base_path.join(format!("{backup_name}.tar.gz"));
=======
        let backup_name = backup_name.unwrap_or_else(|| format!("backup_{}", timestamp));
        let backup_path = self.base_path.join(format!("{}.tar.gz", backup_name));
>>>>>>> cdac5c55f689e319aa18d538b93d7c8f8759a52c
        
        info!("Creating backup: {:?}", backup_path);
        
        // –°–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–±–æ—Ä–∞ —Ñ–∞–π–ª–æ–≤
        let temp_dir = tempfile::TempDir::new()?;
        let temp_path = temp_dir.path();
        
        // –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å–ª–æ—è–º
        let mut layers_info = Vec::new();
        let mut total_records = 0;
        
        // –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è —Å –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ–º –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Å—É–º–º
        let mut layer_checksums = std::collections::HashMap::new();
        let mut master_hasher = Sha256::new();
        
        for layer in [Layer::Interact, Layer::Insights, Layer::Assets] {
            let layer_file = temp_path.join(format!("{}_records.json", layer.as_str()));
            let (count, size, layer_checksum) = self.export_layer_with_checksum(&store, layer, &layer_file).await?;
            
            layers_info.push(LayerInfo {
                layer,
                record_count: count,
                size_bytes: size as usize,
            });
            
            // –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—É—é —Å—É–º–º—É —Å–ª–æ—è
            let layer_name = layer.as_str().to_string();
            layer_checksums.insert(layer_name.clone(), layer_checksum.clone());
            
            // –î–æ–±–∞–≤–ª—è–µ–º –≤ master checksum –¥–ª—è –æ–±—â–µ–π –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ–π —Å—É–º–º—ã
            master_hasher.update(layer_checksum.as_bytes());
            master_hasher.update(count.to_le_bytes());
            
            total_records += count;
            info!("‚úÖ Exported {} records from layer {:?} (checksum: {}...)", 
                  count, layer, &layer_checksum[..16]);
        }
        
        // –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â—É—é –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—É—é —Å—É–º–º—É
        let master_checksum = format!("{:x}", master_hasher.finalize());
        
        // –°–æ–∑–¥–∞—ë–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–º–∏ —Å—É–º–º–∞–º–∏
        let metadata = BackupMetadata {
            version: BACKUP_FORMAT_VERSION,
            created_at: Utc::now(),
            magray_version: env!("CARGO_PKG_VERSION").to_string(),
            layers: layers_info,
            total_records,
            index_config: HnswRsConfig::default(), // –ü–æ–ª—É—á–∞–µ–º –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è
            checksum: Some(master_checksum.clone()),
            layer_checksums: Some(layer_checksums),
        };
        
        info!("üîí Backup integrity: master checksum {}", &master_checksum[..16]);
        
        // –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        let metadata_path = temp_path.join("metadata.json");
        let metadata_file = File::create(&metadata_path)?;
        serde_json::to_writer_pretty(metadata_file, &metadata)?;
        
        // –°–æ–∑–¥–∞—ë–º tar.gz –∞—Ä—Ö–∏–≤
        let tar_gz = File::create(&backup_path)?;
        let encoder = GzEncoder::new(tar_gz, Compression::default());
        let mut tar = TarBuilder::new(encoder);
        
        // –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –≤ –∞—Ä—Ö–∏–≤
        tar.append_dir_all(".", temp_path)?;
        
        // –§–∏–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∞—Ä—Ö–∏–≤
        tar.finish()?;
        
        info!("Backup created successfully: {:?} ({} records)", backup_path, total_records);
        Ok(backup_path)
    }

    /// –í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏–∑ backup
    pub async fn restore_backup(
        &self,
        store: Arc<VectorStore>,
        backup_path: impl AsRef<Path>,
    ) -> Result<BackupMetadata> {
        let backup_path = backup_path.as_ref();
        
        if !backup_path.exists() {
            return Err(anyhow!("Backup file not found: {:?}", backup_path));
        }
        
        info!("Restoring from backup: {:?}", backup_path);
        
        // –°–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏
        let temp_dir = tempfile::TempDir::new()?;
        let temp_path = temp_dir.path();
        
        // –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –∞—Ä—Ö–∏–≤
        let tar_gz = File::open(backup_path)?;
        let decoder = GzDecoder::new(tar_gz);
        let mut tar = tar::Archive::new(decoder);
        tar.unpack(temp_path)?;
        
        // –ß–∏—Ç–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        let metadata_path = temp_path.join("metadata.json");
        let metadata_file = File::open(&metadata_path)?;
        let metadata: BackupMetadata = serde_json::from_reader(BufReader::new(metadata_file))?;
        
        info!("üì¶ Restoring backup: version {}, {} total records", 
              metadata.version, metadata.total_records);
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Å—É–º–º—ã –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
        if let Some(ref expected_checksum) = metadata.checksum {
            info!("üîí Verifying backup integrity (checksum: {}...)", &expected_checksum[..16]);
            let verified = self.verify_backup_integrity(temp_path, &metadata).await?;
            if !verified {
                return Err(anyhow!("‚ùå Backup integrity check FAILED - corrupted data detected"));
            }
            info!("‚úÖ Backup integrity verified successfully");
        } else {
            warn!("‚ö†Ô∏è No checksum found - skipping integrity verification");
        }
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ—Ä—Å–∏—é —Ñ–æ—Ä–º–∞—Ç–∞
        if metadata.version != BACKUP_FORMAT_VERSION {
            return Err(anyhow!(
                "Incompatible backup format version: {} (expected {})",
                metadata.version,
                BACKUP_FORMAT_VERSION
            ));
        }
        
        info!("Backup metadata: {} records from {}", 
              metadata.total_records, 
              metadata.created_at);
        
        // –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Å–ª–æ–π
        let mut restored_count = 0;
        
        for layer_info in &metadata.layers {
            let layer_file = temp_path.join(format!("{}_records.json", layer_info.layer.as_str()));
            
            if layer_file.exists() {
                let count = self.import_layer(&store, layer_info.layer, &layer_file).await?;
                restored_count += count;
                info!("Restored {} records to layer {:?}", count, layer_info.layer);
            } else {
                warn!("Layer file not found: {:?}", layer_file);
            }
        }
        
        info!("Restore completed: {} records restored", restored_count);
        
        Ok(metadata)
    }

    /// –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–π –≤ —Ñ–∞–π–ª
    /// –≠–∫—Å–ø–æ—Ä—Ç —Å–ª–æ—è —Å –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ–º –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ–π —Å—É–º–º—ã
    async fn export_layer_with_checksum(
        &self,
        store: &Arc<VectorStore>,
        layer: Layer,
        output_path: &Path,
    ) -> Result<(usize, u64, String)> {
        let mut count = 0;
        let mut records = Vec::new();
        let mut hasher = Sha256::new();
        
        // –ò—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ –≤—Å–µ–º –∑–∞–ø–∏—Å—è–º —Å–ª–æ—è
        let iter = store.iter_layer(layer).await?;
        
        for item in iter {
            match item {
                Ok((key, value)) => {
                    // –î–µ—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –∑–∞–ø–∏—Å—å
                    if let Ok(stored) = bincode::deserialize::<crate::storage::StoredRecord>(&value) {
                        // –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å –≤ hash –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ–π —Å—É–º–º—ã
                        let record_json = serde_json::to_string(&stored.record)?;
                        hasher.update(record_json.as_bytes());
                        
                        records.push(stored.record);
                        count += 1;
                        
                        // –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                        if records.len() >= 1000 {
<<<<<<< HEAD
                            self.append_records_to_file(output_path, &records)?;
=======
                            self.append_records_to_file(&output_path, &records)?;
>>>>>>> cdac5c55f689e319aa18d538b93d7c8f8759a52c
                            records.clear();
                        }
                    } else {
                        debug!("Failed to deserialize record with key: {:?}", key);
                    }
                }
                Err(e) => {
                    error!("Error iterating layer {}: {}", layer.as_str(), e);
                }
            }
        }
        
        // –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –∑–∞–ø–∏—Å–∏
        if !records.is_empty() {
<<<<<<< HEAD
            self.append_records_to_file(output_path, &records)?;
=======
            self.append_records_to_file(&output_path, &records)?;
>>>>>>> cdac5c55f689e319aa18d538b93d7c8f8759a52c
        }
        
        let file_size = if output_path.exists() {
            output_path.metadata()?.len()
        } else {
            0
        };
        
        let checksum = format!("{:x}", hasher.finalize());
        
        Ok((count, file_size, checksum))
    }
    
    /// Legacy –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏  
    #[allow(dead_code)]
    async fn export_layer(
        &self,
        store: &Arc<VectorStore>,
        layer: Layer,
        output_path: &Path,
    ) -> Result<(usize, usize)> {
        let (count, file_size, _checksum) = self.export_layer_with_checksum(store, layer, output_path).await?;
        Ok((count, file_size as usize))
    }
    
    /// –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å backup'–∞ –ø–æ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–º —Å—É–º–º–∞–º
    async fn verify_backup_integrity(&self, backup_dir: &Path, metadata: &BackupMetadata) -> Result<bool> {
        info!("üîç Starting backup integrity verification...");
        
        let mut master_hasher = Sha256::new();
        let mut verified_layers = 0;
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Å—É–º–º—ã –ø–æ —Å–ª–æ—è–º –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
        if let Some(ref layer_checksums) = metadata.layer_checksums {
            for layer_info in &metadata.layers {
                let layer_name = layer_info.layer.as_str();
<<<<<<< HEAD
                let layer_file = backup_dir.join(format!("{layer_name}_records.json"));
=======
                let layer_file = backup_dir.join(format!("{}_records.json", layer_name));
>>>>>>> cdac5c55f689e319aa18d538b93d7c8f8759a52c
                
                if !layer_file.exists() {
                    warn!("‚ö†Ô∏è Layer file missing: {:?}", layer_file);
                    continue;
                }
                
                // –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—É—é —Å—É–º–º—É —Å–ª–æ—è
                let actual_checksum = self.calculate_file_checksum(&layer_file).await?;
                
                if let Some(expected_checksum) = layer_checksums.get(layer_name) {
                    if &actual_checksum == expected_checksum {
                        debug!("‚úÖ Layer {} checksum verified", layer_name);
                        verified_layers += 1;
                        
                        // –î–æ–±–∞–≤–ª—è–µ–º –≤ master checksum
                        master_hasher.update(actual_checksum.as_bytes());
                        master_hasher.update(layer_info.record_count.to_le_bytes());
                    } else {
                        error!("‚ùå Layer {} checksum mismatch: expected {}, got {}", 
                               layer_name, expected_checksum, actual_checksum);
                        return Ok(false);
                    }
                } else {
                    warn!("‚ö†Ô∏è No checksum found for layer {}", layer_name);
                }
            }
        } else {
            warn!("‚ö†Ô∏è No layer checksums available for verification");
        }
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â—É—é –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—É—é —Å—É–º–º—É
        if let Some(ref expected_master) = metadata.checksum {
            let actual_master = format!("{:x}", master_hasher.finalize());
            
            if &actual_master == expected_master {
                info!("‚úÖ Master checksum verified: {}", &actual_master[..16]);
                return Ok(true);
            } else {
                error!("‚ùå Master checksum mismatch: expected {}, got {}", 
                       expected_master, actual_master);
                return Ok(false);
            }
        }
        
        // –ï—Å–ª–∏ –Ω–µ—Ç master checksum, –Ω–æ –µ—Å—Ç—å layer checksums
        if verified_layers > 0 {
            info!("‚úÖ {} layer checksums verified (no master checksum)", verified_layers);
            return Ok(true);
        }
        
        // –ù–∏–∫–∞–∫–∏—Ö checksums –Ω–µ—Ç
        warn!("‚ö†Ô∏è No checksums available for verification");
        Ok(true) // –°—á–∏—Ç–∞–µ–º –≤–∞–ª–∏–¥–Ω—ã–º –µ—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    }
    
    /// –í—ã—á–∏—Å–ª—è–µ—Ç SHA256 –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—É—é —Å—É–º–º—É JSON —Ñ–∞–π–ª–∞
    async fn calculate_file_checksum(&self, file_path: &Path) -> Result<String> {
        let mut file = File::open(file_path)?;
        let mut hasher = Sha256::new();
        
        // –ß–∏—Ç–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª –∫–∞–∫ JSON –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        let mut contents = String::new();
        file.read_to_string(&mut contents)?;
        
        // –ü–∞—Ä—Å–∏–º JSON —á—Ç–æ–±—ã —É–±—Ä–∞—Ç—å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ differences
        let records: Vec<crate::types::Record> = serde_json::from_str(&contents)?;
        
        // –ü–µ—Ä–µ—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –≤ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        for record in records {
            let normalized_json = serde_json::to_string(&record)?;
            hasher.update(normalized_json.as_bytes());
        }
        
        Ok(format!("{:x}", hasher.finalize()))
    }

    /// –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–π –∏–∑ —Ñ–∞–π–ª–∞
    async fn import_layer(
        &self,
        store: &Arc<VectorStore>,
        layer: Layer,
        input_path: &Path,
    ) -> Result<usize> {
        let file = File::open(input_path)?;
        let reader = BufReader::new(file);
        
        let mut count = 0;
        let mut batch = Vec::new();
        
        // –ß–∏—Ç–∞–µ–º –∑–∞–ø–∏—Å–∏ –ø–æ—Å—Ç—Ä–æ—á–Ω–æ (–∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ - JSON –æ–±—ä–µ–∫—Ç)
        for line in std::io::BufRead::lines(reader) {
            let line = line?;
            
            if let Ok(mut record) = serde_json::from_str::<Record>(&line) {
                // –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –∑–∞–ø–∏—Å—å –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Å–ª–æ–µ
                record.layer = layer;
                batch.push(record);
                
                // Batch insert –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                if batch.len() >= 100 {
                    let refs: Vec<&Record> = batch.iter().collect();
                    store.insert_batch_atomic(&refs).await?;
                    count += batch.len();
                    batch.clear();
                }
            } else {
                debug!("Failed to parse record: {}", line);
            }
        }
        
        // –í—Å—Ç–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –∑–∞–ø–∏—Å–∏
        if !batch.is_empty() {
            let refs: Vec<&Record> = batch.iter().collect();
            store.insert_batch_atomic(&refs).await?;
            count += batch.len();
        }
        
        Ok(count)
    }

    /// –î–æ–±–∞–≤–∏—Ç—å –∑–∞–ø–∏—Å–∏ –≤ —Ñ–∞–π–ª (–¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–π –∑–∞–ø–∏—Å–∏)
    fn append_records_to_file(&self, path: &Path, records: &[Record]) -> Result<()> {
        let file = fs::OpenOptions::new()
            .create(true)
            .append(true)
            .open(path)?;
        
        let mut writer = BufWriter::new(file);
        
        for record in records {
            serde_json::to_writer(&mut writer, record)?;
            writer.write_all(b"\n")?;
        }
        
        writer.flush()?;
        Ok(())
    }

    /// –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö backup —Ñ–∞–π–ª–æ–≤
    pub fn list_backups(&self) -> Result<Vec<BackupInfo>> {
        let mut backups = Vec::new();
        
        for entry in fs::read_dir(&self.base_path)? {
            let entry = entry?;
            let path = entry.path();
            
            if path.extension().and_then(|s| s.to_str()) == Some("gz") {
                if let Ok(metadata) = self.read_backup_metadata(&path) {
                    let file_size = entry.metadata()?.len();
                    
                    backups.push(BackupInfo {
                        path,
                        metadata,
                        size_bytes: file_size,
                    });
                }
            }
        }
        
        // –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ —Å–æ–∑–¥–∞–Ω–∏—è (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–µ)
        backups.sort_by(|a, b| b.metadata.created_at.cmp(&a.metadata.created_at));
        
        Ok(backups)
    }

    /// –ü—Ä–æ—á–∏—Ç–∞—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ backup –±–µ–∑ –ø–æ–ª–Ω–æ–π —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏
<<<<<<< HEAD
    pub fn read_backup_metadata(&self, backup_path: &Path) -> Result<BackupMetadata> {
=======
    fn read_backup_metadata(&self, backup_path: &Path) -> Result<BackupMetadata> {
>>>>>>> cdac5c55f689e319aa18d538b93d7c8f8759a52c
        let tar_gz = File::open(backup_path)?;
        let decoder = GzDecoder::new(tar_gz);
        let mut tar = tar::Archive::new(decoder);
        
        // –ò—â–µ–º —Ñ–∞–π–ª metadata.json –≤ –∞—Ä—Ö–∏–≤–µ
        for entry in tar.entries()? {
            let entry = entry?;
            let path = entry.path()?;
            
            if path.file_name() == Some(std::ffi::OsStr::new("metadata.json")) {
                let metadata: BackupMetadata = serde_json::from_reader(entry)?;
                return Ok(metadata);
            }
        }
        
        Err(anyhow!("Metadata not found in backup"))
    }

    /// –£–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–µ backup —Ñ–∞–π–ª—ã
    pub fn cleanup_old_backups(&self, keep_count: usize) -> Result<usize> {
        let backups = self.list_backups()?;
        let mut deleted = 0;
        
        // –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ keep_count backup —Ñ–∞–π–ª–æ–≤
        for (i, backup_info) in backups.iter().enumerate() {
            if i >= keep_count {
                fs::remove_file(&backup_info.path)?;
                deleted += 1;
                info!("Deleted old backup: {:?}", backup_info.path);
            }
        }
        
        Ok(deleted)
    }
<<<<<<< HEAD

    /// –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å backup —Ñ–∞–π–ª–∞
    pub async fn verify_backup(&self, backup_path: impl AsRef<Path>) -> Result<bool> {
        let backup_path = backup_path.as_ref();
        
        if !backup_path.exists() {
            return Ok(false);
        }
        
        // –ß–∏—Ç–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Å—É–º–º
        let metadata = self.read_backup_metadata(backup_path)?;
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Å—É–º–º—ã
        if metadata.checksum.is_none() && metadata.layer_checksums.is_none() {
            warn!("Backup –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Å—É–º–º –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏: {:?}", backup_path);
            return Ok(true); // –°—á–∏—Ç–∞–µ–º –≤–∞–ª–∏–¥–Ω—ã–º –µ—Å–ª–∏ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Å—É–º–º –Ω–µ—Ç
        }
        
        // –°–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        let temp_dir = tempfile::TempDir::new()?;
        let temp_path = temp_dir.path();
        
        // –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –∞—Ä—Ö–∏–≤
        let tar_gz = File::open(backup_path)?;
        let decoder = GzDecoder::new(tar_gz);
        let mut tar = tar::Archive::new(decoder);
        tar.unpack(temp_path)?;
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Å—É–º–º—ã —Å–ª–æ—ë–≤ –µ—Å–ª–∏ –µ—Å—Ç—å
        if let Some(layer_checksums) = &metadata.layer_checksums {
            for (layer_name, expected_checksum) in layer_checksums {
                let layer_file = temp_path.join(format!("{}_records.json", layer_name));
                if layer_file.exists() {
                    let actual_checksum = self.calculate_file_checksum(&layer_file).await?;
                    if actual_checksum != *expected_checksum {
                        warn!("–ö–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è —Å—É–º–º–∞ —Å–ª–æ—è {} –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç: –æ–∂–∏–¥–∞–µ—Ç—Å—è {}, –ø–æ–ª—É—á–µ–Ω–æ {}", 
                              layer_name, expected_checksum, actual_checksum);
                        return Ok(false);
                    }
                }
            }
        }
        
        info!("Backup –ø—Ä–æ—à—ë–ª –ø—Ä–æ–≤–µ—Ä–∫—É —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏: {:?}", backup_path);
        Ok(true)
    }

=======
>>>>>>> cdac5c55f689e319aa18d538b93d7c8f8759a52c
}

/// –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ backup —Ñ–∞–π–ª–µ
#[derive(Debug)]
pub struct BackupInfo {
    pub path: PathBuf,
    pub metadata: BackupMetadata,
    pub size_bytes: u64,
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;

    #[tokio::test]
    async fn test_backup_manager_creation() {
        let temp_dir = TempDir::new().unwrap();
        let manager = BackupManager::new(temp_dir.path()).unwrap();
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å–æ–∑–¥–∞–Ω–∞
        assert!(temp_dir.path().exists());
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–∏—Å–æ–∫ backup (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø—É—Å—Ç–æ–π)
        let backups = manager.list_backups().unwrap();
        assert_eq!(backups.len(), 0);
    }

    #[test]
    fn test_metadata_serialization() {
        let metadata = BackupMetadata {
            version: BACKUP_FORMAT_VERSION,
            created_at: Utc::now(),
            magray_version: "0.1.0".to_string(),
            layers: vec![
                LayerInfo {
                    layer: Layer::Interact,
                    record_count: 100,
                    size_bytes: 1024,
                }
            ],
            total_records: 100,
            index_config: HnswRsConfig::default(),
            checksum: None,
            layer_checksums: None,
        };
        
        let json = serde_json::to_string_pretty(&metadata).unwrap();
        let parsed: BackupMetadata = serde_json::from_str(&json).unwrap();
        
        assert_eq!(parsed.version, metadata.version);
        assert_eq!(parsed.total_records, metadata.total_records);
    }
}