use anyhow::Result;
use std::sync::Arc;
use tokio::sync::{Mutex, Semaphore};
use tracing::{info, warn, debug};

use ai::{GpuFallbackManager, EmbeddingConfig, EmbeddingServiceTrait};
use ai::gpu_fallback::FallbackStats;
use crate::cache_interface::EmbeddingCacheInterface;

/// –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è GPU –æ–±—Ä–∞–±–æ—Ç–∫–∏
const MAX_BATCH_SIZE: usize = 128;
/// –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö GPU –æ–ø–µ—Ä–∞—Ü–∏–π
const MAX_CONCURRENT_GPU_OPS: usize = 4;

// @component: {"k":"C","id":"gpu_batch_processor","t":"GPU batch embedding processor","m":{"cur":95,"tgt":100,"u":"%"},"f":["gpu","batch","embeddings","fallback"]}
pub struct GpuBatchProcessor {
    embedding_service: Arc<GpuFallbackManager>,
    cache: Arc<dyn EmbeddingCacheInterface>,
    batch_semaphore: Arc<Semaphore>,
    processing_queue: Arc<Mutex<Vec<PendingEmbedding>>>,
    config: BatchProcessorConfig,
}

#[derive(Clone)]
pub struct BatchProcessorConfig {
    pub max_batch_size: usize,
    pub batch_timeout_ms: u64,
    pub use_gpu_if_available: bool,
    pub cache_embeddings: bool,
}

impl Default for BatchProcessorConfig {
    fn default() -> Self {
        Self {
            max_batch_size: MAX_BATCH_SIZE,
            batch_timeout_ms: 50,
            use_gpu_if_available: true,
            cache_embeddings: true,
        }
    }
}

struct PendingEmbedding {
    text: String,
    callback: tokio::sync::oneshot::Sender<Result<Vec<f32>>>,
}

impl GpuBatchProcessor {
    /// –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å –Ω–∞–¥—ë–∂–Ω—ã–º GPU fallback –º–µ—Ö–∞–Ω–∏–∑–º–æ–º
    pub async fn new(
        config: BatchProcessorConfig,
        embedding_config: EmbeddingConfig,
        cache: Arc<dyn EmbeddingCacheInterface>,
    ) -> Result<Self> {
        info!("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GpuBatchProcessor —Å –Ω–∞–¥—ë–∂–Ω—ã–º fallback");
        
        // –°–æ–∑–¥–∞—ë–º embedding —Å–µ—Ä–≤–∏—Å —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º GPU/CPU fallback
        let embedding_service = Arc::new(
            GpuFallbackManager::new(embedding_config).await
                .map_err(|e| anyhow::anyhow!("Failed to create embedding service: {}", e))?
        );

        info!("‚úÖ GPU batch processor initialized with robust fallback mechanism");

        Ok(Self {
            embedding_service,
            cache,
            batch_semaphore: Arc::new(Semaphore::new(MAX_CONCURRENT_GPU_OPS)),
            processing_queue: Arc::new(Mutex::new(Vec::new())),
            config,
        })
    }

    /// –ü–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (—Å –±–∞—Ç—á–µ–≤–∞–Ω–∏–µ–º)
    pub async fn embed(&self, text: &str) -> Result<Vec<f32>> {
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if self.config.cache_embeddings {
            if let Some(embedding) = self.cache.get(text, "bge-m3") {
                debug!("Cache hit for embedding");
                return Ok(embedding);
            }
        }

        // –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π fallback —Å–µ—Ä–≤–∏—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è embedding
        let embeddings = self.embedding_service.embed_batch(vec![text.to_string()]).await?;
        let embedding = embeddings.into_iter().next()
            .ok_or_else(|| anyhow::anyhow!("No embedding returned"))?;

        // –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if self.config.cache_embeddings {
            if let Err(e) = self.cache.insert(text, "bge-m3", embedding.clone()) {
                warn!("Failed to cache embedding: {}", e);
            }
        }

        Ok(embedding)
    }

    /// –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –±–∞—Ç—á —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞–ø—Ä—è–º—É—é
    pub async fn embed_batch(&self, texts: Vec<String>) -> Result<Vec<Vec<f32>>> {
        if texts.is_empty() {
            return Ok(vec![]);
        }

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –∏ —Ä–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ cached/uncached
        let mut results = vec![None; texts.len()];
        let mut uncached_indices = Vec::new();
        let mut uncached_texts = Vec::new();

        if self.config.cache_embeddings {
            for (i, text) in texts.iter().enumerate() {
                if let Some(embedding) = self.cache.get(text, "bge-m3") {
                    results[i] = Some(embedding);
                } else {
                    uncached_indices.push(i);
                    uncached_texts.push(text.clone());
                }
            }
        } else {
            uncached_texts = texts.clone();
            uncached_indices = (0..texts.len()).collect();
        }

        // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º uncached —Ç–µ–∫—Å—Ç—ã —á–µ—Ä–µ–∑ fallback —Å–µ—Ä–≤–∏—Å
        if !uncached_texts.is_empty() {
            let embeddings = self.embedding_service.embed_batch(uncached_texts.clone()).await?;

            // –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for (idx, (text, embedding)) in uncached_texts.iter()
                .zip(embeddings.iter())
                .enumerate() 
            {
                if self.config.cache_embeddings {
                    self.cache.insert(text, "bge-m3", embedding.clone())?;
                }
                results[uncached_indices[idx]] = Some(embedding.clone());
            }
        }

        // –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        Ok(results.into_iter()
            .map(|r| r.expect("All results should be filled"))
            .collect())
    }

    /// –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π –±–∞—Ç—á
    async fn process_batch(&self) -> Result<()> {
        let pending = {
            let mut queue = self.processing_queue.lock().await;
            std::mem::take(&mut *queue)
        };

        if pending.is_empty() {
            return Ok(());
        }

        let texts: Vec<String> = pending.iter()
            .map(|p| p.text.clone())
            .collect();

        debug!("Processing batch of {} texts", texts.len());

        // –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        let embeddings = self.embed_batch(texts).await?;

        // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for (pending_item, embedding) in pending.into_iter().zip(embeddings) {
            let _ = pending_item.callback.send(Ok(embedding));
        }

        Ok(())
    }

    /// –°–æ–∑–¥–∞—Ç—å –∫–ª–æ–Ω –¥–ª—è —Ñ–æ–Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á
    fn clone_for_task(&self) -> Arc<Self> {
        Arc::new(Self {
            embedding_service: self.embedding_service.clone(),
            cache: self.cache.clone(),
            batch_semaphore: self.batch_semaphore.clone(),
            processing_queue: self.processing_queue.clone(),
            config: self.config.clone(),
        })
    }

    /// –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU —á–µ—Ä–µ–∑ fallback manager
    pub fn has_gpu(&self) -> bool {
        // –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—Ç fallback manager
        let stats = self.embedding_service.get_stats();
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º success rate –≤–º–µ—Å—Ç–æ –ø—Ä—è–º–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ –ø–æ–ª—è–º
        stats.gpu_success_rate() > 0.0 || stats.fallback_rate() < 1.0
    }
    
    /// –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É fallback
    pub fn get_fallback_stats(&self) -> FallbackStats {
        self.embedding_service.get_stats()
    }
    
    /// –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ CPU —Ä–µ–∂–∏–º
    pub fn force_cpu_mode(&self) {
        self.embedding_service.force_cpu_mode();
    }

    /// –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    pub async fn get_stats(&self) -> BatchProcessorStats {
        let queue_size = self.processing_queue.lock().await.len();
        
        BatchProcessorStats {
            has_gpu: self.has_gpu(),
            queue_size,
            cache_stats: self.cache.stats(),
        }
    }
}

#[derive(Debug)]
pub struct BatchProcessorStats {
    pub has_gpu: bool,
    pub queue_size: usize,
    pub cache_stats: (u64, u64, u64), // (hits, misses, size)
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;

    #[tokio::test]
    async fn test_batch_processor_creation() {
        let temp_dir = TempDir::new().unwrap();
        let cache = Arc::new(crate::EmbeddingCache::new(temp_dir.path()).unwrap()) as Arc<dyn EmbeddingCacheInterface>;
        
        let config = BatchProcessorConfig::default();
        let embedding_config = EmbeddingConfig::default();
        
        match GpuBatchProcessor::new(config, embedding_config, cache).await {
            Ok(_) => {
                // –î–æ–ª–∂–µ–Ω —Å–æ–∑–¥–∞—Ç—å—Å—è —Ö–æ—Ç—è –±—ã —Å CPU fallback
                println!("Processor created successfully");
            },
            Err(e) => {
                println!("Expected error without models: {}", e);
                // This is fine in test environment without models
            }
        }
    }

    #[tokio::test]
    async fn test_single_embedding() {
        let temp_dir = TempDir::new().unwrap();
        let cache = Arc::new(crate::EmbeddingCache::new(temp_dir.path()).unwrap()) as Arc<dyn EmbeddingCacheInterface>;
        
        let config = BatchProcessorConfig {
            use_gpu_if_available: false, // –§–æ—Ä—Å–∏—Ä—É–µ–º CPU
            ..Default::default()
        };
        let embedding_config = EmbeddingConfig::default();
        
        match GpuBatchProcessor::new(config, embedding_config, cache).await {
            Ok(processor) => {
                let embedding = processor.embed("test text").await.unwrap();
                assert!(!embedding.is_empty());
            },
            Err(e) => {
                println!("Expected error without models: {}", e);
                // This is fine in test environment without models
            }
        }
    }

    #[tokio::test] 
    async fn test_batch_embedding() {
        let temp_dir = TempDir::new().unwrap();
        let cache = Arc::new(crate::EmbeddingCache::new(temp_dir.path()).unwrap()) as Arc<dyn EmbeddingCacheInterface>;
        
        let config = BatchProcessorConfig {
            use_gpu_if_available: false, // –§–æ—Ä—Å–∏—Ä—É–µ–º CPU
            ..Default::default()
        };
        let embedding_config = EmbeddingConfig::default();
        
        match GpuBatchProcessor::new(config, embedding_config, cache).await {
            Ok(processor) => {
                let texts = vec![
                    "first text".to_string(),
                    "second text".to_string(),
                    "third text".to_string(),
                ];
                
                let embeddings = processor.embed_batch(texts).await.unwrap();
                assert_eq!(embeddings.len(), 3);
                
                for embedding in embeddings {
                    assert!(!embedding.is_empty());
                }
            },
            Err(e) => {
                println!("Expected error without models: {}", e);
                // This is fine in test environment without models
            }
        }
    }
}