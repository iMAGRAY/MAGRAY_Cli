use anyhow::Result;
use async_trait::async_trait;
use std::{
    sync::Arc,
    time::{Duration, Instant},
    collections::HashMap,
};
use tracing::{debug, info, warn, error};
use tokio::sync::RwLock;

use crate::{
    health::{HealthMonitor, SystemHealthStatus, HealthStatus},
    orchestration::traits::{Coordinator, HealthCoordinator},
};

/// Production-ready –º–µ–Ω–µ–¥–∂–µ—Ä –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã —Å comprehensive monitoring
pub struct HealthManager {
    health_monitor: Arc<HealthMonitor>,
    ready: std::sync::atomic::AtomicBool,
    
    // === Production Health Metrics ===
    /// –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
    health_metrics: Arc<RwLock<HealthMetrics>>,
    /// SLA –º–µ—Ç—Ä–∏–∫–∏ (–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å, –∑–∞–¥–µ—Ä–∂–∫–∞, –æ—à–∏–±–∫–∏)
    sla_metrics: Arc<RwLock<SlaMetrics>>,
    /// –ê–∫—Ç–∏–≤–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã
    active_alerts: Arc<RwLock<Vec<Alert>>>,
    /// –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    monitored_components: Arc<RwLock<HashMap<String, ComponentHealth>>>,
    /// –ò—Å—Ç–æ—Ä–∏—è –ø—Ä–æ–≤–µ—Ä–æ–∫ –∑–¥–æ—Ä–æ–≤—å—è
    health_history: Arc<RwLock<Vec<HealthCheckResult>>>,
}

/// –ú–µ—Ç—Ä–∏–∫–∏ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã
#[derive(Debug, Default)]
struct HealthMetrics {
    total_checks: u64,
    successful_checks: u64,
    failed_checks: u64,
    avg_check_duration_ms: f64,
    uptime_seconds: u64,
    last_check_timestamp: Option<Instant>,
    system_load: f64,
    memory_usage_percent: f64,
    disk_usage_percent: f64,
}

/// SLA –º–µ—Ç—Ä–∏–∫–∏
#[derive(Debug, Default)]
struct SlaMetrics {
    availability_percent: f64,
    avg_response_time_ms: f64,
    error_rate_percent: f64,
    mttr_minutes: f64, // Mean Time To Recovery
    mtbf_hours: f64,   // Mean Time Between Failures
    sla_violations: u64,
}

/// –ê–ª–µ—Ä—Ç
#[derive(Debug, Clone)]
#[allow(dead_code)]
struct Alert {
    id: String,
    component: String,
    level: AlertLevel,
    message: String,
    timestamp: Instant,
    resolved: bool,
}

#[derive(Debug, Clone, PartialEq)]
#[allow(dead_code)]
enum AlertLevel {
    Info,
    Warning,
    Critical,
}

/// –ó–¥–æ—Ä–æ–≤—å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
#[derive(Debug, Clone)]
struct ComponentHealth {
    name: String,
    status: HealthStatus,
    last_check: Instant,
    check_interval: Duration,
    consecutive_failures: u32,
    total_checks: u64,
    successful_checks: u64,
}

/// –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–¥–æ—Ä–æ–≤—å—è
#[derive(Debug, Clone)]
#[allow(dead_code)]
struct HealthCheckResult {
    timestamp: Instant,
    overall_healthy: bool,
    component_count: usize,
    healthy_components: usize,
    check_duration_ms: f64,
}

impl HealthManager {
    pub fn new(health_monitor: Arc<HealthMonitor>) -> Self {
        Self {
            health_monitor,
            ready: std::sync::atomic::AtomicBool::new(false),
            health_metrics: Arc::new(RwLock::new(HealthMetrics::default())),
            sla_metrics: Arc::new(RwLock::new(SlaMetrics::default())),
            active_alerts: Arc::new(RwLock::new(Vec::new())),
            monitored_components: Arc::new(RwLock::new(HashMap::new())),
            health_history: Arc::new(RwLock::new(Vec::new())),
        }
    }
    
    /// –ù–∞—Å—Ç—Ä–æ–π–∫–∞ production monitoring
    pub async fn setup_production_monitoring(&self) -> Result<()> {
        info!("üé® –ù–∞—Å—Ç—Ä–æ–π–∫–∞ production health monitoring...");
        
        // –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.register_component("vector_store", Duration::from_secs(30)).await;
        self.register_component("embedding_service", Duration::from_secs(30)).await;
        self.register_component("search_coordinator", Duration::from_secs(60)).await;
        self.register_component("cache_system", Duration::from_secs(60)).await;
        self.register_component("gpu_processor", Duration::from_secs(120)).await;
        
        // –ó–∞–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        self.start_health_monitoring_loop().await;
        self.start_sla_monitoring_loop().await;
        self.start_alert_processor().await;
        
        Ok(())
    }
}

#[async_trait]
impl Coordinator for HealthManager {
    async fn initialize(&self) -> Result<()> {
        info!("üöë –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è production HealthManager...");
        
        // –ó–∞–ø—É—Å–∫–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –∑–¥–æ—Ä–æ–≤—å—è
        self.run_health_check().await?;
        
        self.ready.store(true, std::sync::atomic::Ordering::Relaxed);
        info!("‚úÖ HealthManager –≥–æ—Ç–æ–≤ –∫ production monitoring");
        Ok(())
    }
    
    async fn is_ready(&self) -> bool {
        self.ready.load(std::sync::atomic::Ordering::Relaxed)
    }
    
    async fn shutdown(&self) -> Result<()> {
        self.ready.store(false, std::sync::atomic::Ordering::Relaxed);
        Ok(())
    }
    
    async fn metrics(&self) -> serde_json::Value {
        let health_status = self.system_health().await.unwrap_or_default();
        let health_metrics = self.health_metrics.read().await;
        let sla_metrics = self.sla_metrics.read().await;
        let alerts = self.get_alerts().await;
        let components = self.monitored_components.read().await;
        
        // –°–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º
        let healthy_components = components.values().filter(|c| matches!(c.status, HealthStatus::Healthy)).count();
        let degraded_components = components.values().filter(|c| matches!(c.status, HealthStatus::Degraded)).count();
        let unhealthy_components = components.values().filter(|c| matches!(c.status, HealthStatus::Unhealthy)).count();
        
        // –°—á–∏—Ç–∞–µ–º –∞–ª–µ—Ä—Ç—ã –ø–æ —É—Ä–æ–≤–Ω—è–º
        let critical_alerts = alerts.iter().filter(|a| a.contains("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π")).count();
        let warning_alerts = alerts.iter().filter(|a| a.contains("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ")).count();
        
        serde_json::json!({
            "ready": self.is_ready().await,
            "type": "health_manager",
            "system_health": {
                "overall_healthy": health_status.overall_status,
                "components_count": components.len(),
                "healthy_components": healthy_components,
                "degraded_components": degraded_components,
                "unhealthy_components": unhealthy_components,
                "last_updated": health_status.last_updated.to_rfc3339()
            },
            "performance_metrics": {
                "total_checks": health_metrics.total_checks,
                "successful_checks": health_metrics.successful_checks,
                "failed_checks": health_metrics.failed_checks,
                "success_rate_percent": if health_metrics.total_checks > 0 {
                    (health_metrics.successful_checks as f64 / health_metrics.total_checks as f64) * 100.0
                } else { 0.0 },
                "avg_check_duration_ms": health_metrics.avg_check_duration_ms,
                "uptime_seconds": health_metrics.uptime_seconds,
                "system_load": health_metrics.system_load,
                "memory_usage_percent": health_metrics.memory_usage_percent,
                "disk_usage_percent": health_metrics.disk_usage_percent
            },
            "sla_metrics": {
                "availability_percent": sla_metrics.availability_percent,
                "avg_response_time_ms": sla_metrics.avg_response_time_ms,
                "error_rate_percent": sla_metrics.error_rate_percent,
                "mttr_minutes": sla_metrics.mttr_minutes,
                "mtbf_hours": sla_metrics.mtbf_hours,
                "sla_violations": sla_metrics.sla_violations
            },
            "alerts": {
                "active_count": alerts.len(),
                "critical_count": critical_alerts,
                "warning_count": warning_alerts,
                "info_count": alerts.len() - critical_alerts - warning_alerts,
                "alerts": alerts
            },
            "component_details": components.values().map(|c| {
                serde_json::json!({
                    "name": c.name,
                    "status": format!("{:?}", c.status),
                    "last_check_seconds_ago": c.last_check.elapsed().as_secs(),
                    "consecutive_failures": c.consecutive_failures,
                    "success_rate_percent": if c.total_checks > 0 {
                        (c.successful_checks as f64 / c.total_checks as f64) * 100.0
                    } else { 0.0 }
                })
            }).collect::<Vec<_>>()
        })
    }
}

#[async_trait]
impl HealthCoordinator for HealthManager {
    async fn system_health(&self) -> Result<SystemHealthStatus> {
        self.health_monitor.overall_health().await
    }
    
    async fn component_health(&self, component: &str) -> Result<bool> {
        let components = self.monitored_components.read().await;
        
        if let Some(comp_health) = components.get(component) {
            Ok(matches!(comp_health.status, HealthStatus::Healthy))
        } else {
            warn!("–ö–æ–º–ø–æ–Ω–µ–Ω—Ç '{}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–µ", component);
            Ok(false)
        }
    }
    
    async fn run_health_check(&self) -> Result<()> {
        let start_time = Instant::now();
        let health_result = self.health_monitor.overall_health().await;
        let check_duration = start_time.elapsed();
        
        // –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        let mut metrics = self.health_metrics.write().await;
        metrics.total_checks += 1;
        
        match &health_result {
            Ok(status) => {
                metrics.successful_checks += 1;
                
                // –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å—ã –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
                let mut components = self.monitored_components.write().await;
                for (comp_type, comp_status) in &status.component_statuses {
                    let comp_name = format!("{:?}", comp_type);
                    if let Some(comp_health) = components.get_mut(&comp_name) {
                        comp_health.status = comp_status.clone();
                        comp_health.last_check = Instant::now();
                        comp_health.total_checks += 1;
                        
                        if matches!(comp_status, HealthStatus::Healthy) {
                            comp_health.successful_checks += 1;
                            comp_health.consecutive_failures = 0;
                        } else {
                            comp_health.consecutive_failures += 1;
                            
                            // –°–æ–∑–¥–∞—ë–º –∞–ª–µ—Ä—Ç –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
                            if comp_health.consecutive_failures >= 3 {
                                self.create_alert(
                                    &comp_name,
                                    AlertLevel::Critical,
                                    format!("–ö–æ–º–ø–æ–Ω–µ–Ω—Ç {} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ({} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –æ—à–∏–±–æ–∫)", comp_name, comp_health.consecutive_failures)
                                ).await;
                            } else if comp_health.consecutive_failures == 1 {
                                self.create_alert(
                                    &comp_name,
                                    AlertLevel::Warning,
                                    format!("–ö–æ–º–ø–æ–Ω–µ–Ω—Ç {} –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã", comp_name)
                                ).await;
                            }
                        }
                    }
                }
                
                // –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
                let mut history = self.health_history.write().await;
                history.push(HealthCheckResult {
                    timestamp: Instant::now(),
                    overall_healthy: matches!(status.overall_status, HealthStatus::Healthy),
                    component_count: status.component_statuses.len(),
                    healthy_components: status.component_statuses.values().filter(|s| matches!(s, HealthStatus::Healthy)).count(),
                    check_duration_ms: check_duration.as_millis() as f64,
                });
                
                // –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏ (100 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫)
                if history.len() > 100 {
                    history.remove(0);
                }
            }
            Err(_) => {
                metrics.failed_checks += 1;
                self.create_alert(
                    "health_monitor",
                    AlertLevel::Critical,
                    "–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–¥–æ—Ä–æ–≤—å—è".to_string()
                ).await;
            }
        }
        
        // –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        let alpha = 0.1;
        if metrics.avg_check_duration_ms == 0.0 {
            metrics.avg_check_duration_ms = check_duration.as_millis() as f64;
        } else {
            metrics.avg_check_duration_ms = alpha * check_duration.as_millis() as f64 + (1.0 - alpha) * metrics.avg_check_duration_ms;
        }
        
        metrics.last_check_timestamp = Some(Instant::now());
        
        health_result.map(|_| ())
    }
    
    async fn get_alerts(&self) -> Vec<String> {
        let alerts = self.active_alerts.read().await;
        alerts.iter()
            .filter(|alert| !alert.resolved)
            .map(|alert| {
                format!("{:?}: {} - {} ({})", 
                    alert.level, 
                    alert.component, 
                    alert.message,
                    format_duration(alert.timestamp.elapsed())
                )
            })
            .collect()
    }
    
    async fn clear_alerts(&self) -> Result<()> {
        let mut alerts = self.active_alerts.write().await;
        let cleared_count = alerts.len();
        
        // –ü–æ–º–µ—á–∞–µ–º –≤—Å–µ –∞–ª–µ—Ä—Ç—ã –∫–∞–∫ —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ
        for alert in alerts.iter_mut() {
            alert.resolved = true;
        }
        
        info!("‚úÖ –û—á–∏—â–µ–Ω–æ {} –∞–ª–µ—Ä—Ç–æ–≤", cleared_count);
        Ok(())
    }
}

impl HealthManager {
    /// –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è production monitoring
    
    /// –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    async fn register_component(&self, name: &str, check_interval: Duration) {
        let mut components = self.monitored_components.write().await;
        components.insert(name.to_string(), ComponentHealth {
            name: name.to_string(),
            status: HealthStatus::Healthy,
            last_check: Instant::now(),
            check_interval,
            consecutive_failures: 0,
            total_checks: 0,
            successful_checks: 0,
        });
        
        debug!("üìà –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω –∫–æ–º–ø–æ–Ω–µ–Ω—Ç '{}' —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º {:?}", name, check_interval);
    }
    
    /// –°–æ–∑–¥–∞–Ω–∏–µ –∞–ª–µ—Ä—Ç–∞
    async fn create_alert(&self, component: &str, level: AlertLevel, message: String) {
        let alert = Alert {
            id: format!("alert_{}", std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_millis()),
            component: component.to_string(),
            level: level.clone(),
            message,
            timestamp: Instant::now(),
            resolved: false,
        };
        
        let mut alerts = self.active_alerts.write().await;
        alerts.push(alert.clone());
        
        // –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–ª–µ—Ä—Ç–æ–≤ (100 –º–∞–∫—Å)
        if alerts.len() > 100 {
            alerts.remove(0);
        }
        
        let level_emoji = match level {
            AlertLevel::Info => "üìù",
            AlertLevel::Warning => "‚ö†Ô∏è",
            AlertLevel::Critical => "üö®",
        };
        
        warn!("{} –ù–æ–≤—ã–π –∞–ª–µ—Ä—Ç: {} - {}", level_emoji, alert.component, alert.message);
    }
    
    /// –ó–∞–ø—É—Å–∫ health monitoring loop
    async fn start_health_monitoring_loop(&self) {
        let health_monitor = self.health_monitor.clone();
        let _components = self.monitored_components.clone();
        let metrics = self.health_metrics.clone();
        
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_secs(30));
            
            loop {
                interval.tick().await;
                
                // –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–µ–µ –∑–¥–æ—Ä–æ–≤—å–µ —Å–∏—Å—Ç–µ–º—ã
                if let Err(e) = health_monitor.check_health().await {
                    error!("–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–¥–æ—Ä–æ–≤—å—è: {}", e);
                }
                
                // –û–±–Ω–æ–≤–ª—è–µ–º uptime
                {
                    let mut metrics_guard = metrics.write().await;
                    metrics_guard.uptime_seconds += 30;
                }
                
                debug!("üìà Health monitoring —Ü–∏–∫–ª –∑–∞–≤–µ—Ä—à—ë–Ω");
            }
        });
        
        debug!("üìà Health monitoring loop –∑–∞–ø—É—â–µ–Ω");
    }
    
    /// –ó–∞–ø—É—Å–∫ SLA monitoring loop
    async fn start_sla_monitoring_loop(&self) {
        let sla_metrics = self.sla_metrics.clone();
        let health_history = self.health_history.clone();
        
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_secs(300)); // –ö–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç
            
            loop {
                interval.tick().await;
                
                let history = health_history.read().await;
                if history.len() > 10 {
                    let mut sla_guard = sla_metrics.write().await;
                    
                    // –°—á–∏—Ç–∞–µ–º availability
                    let healthy_checks = history.iter().filter(|h| h.overall_healthy).count();
                    sla_guard.availability_percent = (healthy_checks as f64 / history.len() as f64) * 100.0;
                    
                    // –°—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞
                    let total_duration: f64 = history.iter().map(|h| h.check_duration_ms).sum();
                    sla_guard.avg_response_time_ms = total_duration / history.len() as f64;
                    
                    // –û–±–Ω–æ–≤–ª—è–µ–º error rate
                    let error_checks = history.iter().filter(|h| !h.overall_healthy).count();
                    sla_guard.error_rate_percent = (error_checks as f64 / history.len() as f64) * 100.0;
                    
                    // –ü—Ä–æ–≤–µ—Ä—è–µ–º SLA –Ω–∞—Ä—É—à–µ–Ω–∏—è
                    if sla_guard.availability_percent < 99.9 {
                        sla_guard.sla_violations += 1;
                        warn!("üö® SLA –Ω–∞—Ä—É—à–µ–Ω–∏–µ: availability {:.2}%", sla_guard.availability_percent);
                    }
                    
                    if sla_guard.avg_response_time_ms > 1000.0 {
                        sla_guard.sla_violations += 1;
                        warn!("üö® SLA –Ω–∞—Ä—É—à–µ–Ω–∏–µ: response time {:.2}ms", sla_guard.avg_response_time_ms);
                    }
                }
                
                debug!("üìà SLA monitoring —Ü–∏–∫–ª –∑–∞–≤–µ—Ä—à—ë–Ω");
            }
        });
        
        debug!("üìà SLA monitoring loop –∑–∞–ø—É—â–µ–Ω");
    }
    
    /// –ó–∞–ø—É—Å–∫ alert processor
    async fn start_alert_processor(&self) {
        let active_alerts = self.active_alerts.clone();
        
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_secs(60)); // –ö–∞–∂–¥—É—é –º–∏–Ω—É—Ç—É
            
            loop {
                interval.tick().await;
                
                let mut alerts = active_alerts.write().await;
                
                // –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ resolved –∞–ª–µ—Ä—Ç—ã (>—á–∞—Å–∞)
                alerts.retain(|alert| {
                    if alert.resolved && alert.timestamp.elapsed() > Duration::from_secs(3600) {
                        false
                    } else {
                        true
                    }
                });
                
                // –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∞–ª–µ—Ä—Ç—ã
                let critical_count = alerts.iter().filter(|a| !a.resolved && matches!(a.level, AlertLevel::Critical)).count();
                if critical_count > 0 {
                    error!("üö® {} –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∞–ª–µ—Ä—Ç–æ–≤!", critical_count);
                }
                
                debug!("üîî Alert processor: {} –∞–∫—Ç–∏–≤–Ω—ã—Ö –∞–ª–µ—Ä—Ç–æ–≤", alerts.iter().filter(|a| !a.resolved).count());
            }
        });
        
        debug!("üîî Alert processor –∑–∞–ø—É—â–µ–Ω");
    }
    
    /// –ü–æ–ª—É—á–∏—Ç—å –ø–æ–¥—Ä–æ–±–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—É
    pub async fn get_component_detailed_stats(&self, component: &str) -> Option<serde_json::Value> {
        let components = self.monitored_components.read().await;
        
        if let Some(comp_health) = components.get(component) {
            Some(serde_json::json!({
                "name": comp_health.name,
                "status": format!("{:?}", comp_health.status),
                "last_check_duration": format_duration(comp_health.last_check.elapsed()),
                "check_interval_seconds": comp_health.check_interval.as_secs(),
                "consecutive_failures": comp_health.consecutive_failures,
                "total_checks": comp_health.total_checks,
                "successful_checks": comp_health.successful_checks,
                "failure_rate_percent": if comp_health.total_checks > 0 {
                    ((comp_health.total_checks - comp_health.successful_checks) as f64 / comp_health.total_checks as f64) * 100.0
                } else { 0.0 },
                "is_overdue": comp_health.last_check.elapsed() > comp_health.check_interval * 2
            }))
        } else {
            None
        }
    }
}

/// –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ duration –¥–ª—è –≤—ã–≤–æ–¥–∞
fn format_duration(duration: Duration) -> String {
    let secs = duration.as_secs();
    if secs < 60 {
        format!("{}s", secs)
    } else if secs < 3600 {
        format!("{}m {}s", secs / 60, secs % 60)
    } else {
        format!("{}h {}m", secs / 3600, (secs % 3600) / 60)
    }
}