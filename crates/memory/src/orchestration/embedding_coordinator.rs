use anyhow::Result;
use async_trait::async_trait;
use std::{
    sync::Arc,
    time::{Duration, Instant},
    collections::VecDeque,
};
use tokio::{
    sync::{RwLock, Semaphore, Notify},
};
use tracing::{debug, info, warn};

use crate::{
    cache_interface::EmbeddingCacheInterface,
    gpu_accelerated::GpuBatchProcessor,
    orchestration::{
        traits::{Coordinator, EmbeddingCoordinator as EmbeddingCoordinatorTrait},
        retry_handler::{RetryHandler, RetryPolicy},
    },
};

/// Production-ready –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å embeddings
pub struct EmbeddingCoordinator {
    /// GPU batch processor –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è embeddings
    gpu_processor: Arc<GpuBatchProcessor>,
    /// –ö—ç—à embeddings
    cache: Arc<dyn EmbeddingCacheInterface>,
    /// Retry handler –¥–ª—è –æ–ø–µ—Ä–∞—Ü–∏–π
    retry_handler: RetryHandler,
    /// –§–ª–∞–≥ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
    ready: std::sync::atomic::AtomicBool,
    
    // === AI/ML Optimization Infrastructure ===
    /// Circuit breaker –¥–ª—è GPU –æ–ø–µ—Ä–∞—Ü–∏–π
    circuit_breaker: Arc<RwLock<CircuitBreaker>>,
    /// Semaphore –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
    concurrency_limiter: Arc<Semaphore>,
    /// Adaptive batch —Ä–∞–∑–º–µ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ load
    adaptive_batch_size: Arc<RwLock<AdaptiveBatchConfig>>,
    /// Model warming —Å—Ç–∞—Ç—É—Å
    model_warmed: Arc<std::sync::atomic::AtomicBool>,
    /// Performance –º–µ—Ç—Ä–∏–∫–∏
    performance_metrics: Arc<RwLock<PerformanceMetrics>>,
    /// Queue –¥–ª—è batch processing —Å backpressure
    batch_queue: Arc<RwLock<VecDeque<BatchRequest>>>,
    /// Notification –¥–ª—è batch processing
    batch_notify: Arc<Notify>,
}

/// Circuit breaker state –¥–ª—è GPU –æ–ø–µ—Ä–∞—Ü–∏–π
#[derive(Debug, Clone)]
#[allow(dead_code)]
struct CircuitBreaker {
    state: CircuitState,
    failure_count: u32,
    success_count: u32,
    last_failure: Option<Instant>,
    failure_threshold: u32,
    timeout: Duration,
    success_threshold: u32,
}

#[derive(Debug, Clone, PartialEq)]
#[allow(dead_code)]
enum CircuitState {
    Closed,     // –ù–æ—Ä–º–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    Open,       // –û—Ç–∫–∞–∑, –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
    HalfOpen,   // –¢–µ—Å—Ç–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
}

/// –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è batch —Ä–∞–∑–º–µ—Ä–∞
#[derive(Debug, Clone)]
#[allow(dead_code)]
struct AdaptiveBatchConfig {
    current_size: usize,
    min_size: usize,
    max_size: usize,
    target_latency_ms: u64,
    recent_latencies: VecDeque<u64>,
    last_adjustment: Instant,
}

/// Performance –º–µ—Ç—Ä–∏–∫–∏
#[derive(Debug, Default, Clone)]
pub struct PerformanceMetrics {
    #[allow(dead_code)] // –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    total_requests: u64,
    #[allow(dead_code)]
    successful_requests: u64,
    #[allow(dead_code)]
    failed_requests: u64,
    #[allow(dead_code)]
    cache_hits: u64,
    #[allow(dead_code)]
    cache_misses: u64,
    #[allow(dead_code)]
    avg_latency_ms: f64,
    #[allow(dead_code)]
    gpu_utilization: f64,
    #[allow(dead_code)]
    batch_efficiency: f64,
    #[allow(dead_code)]
    circuit_breaker_trips: u64,
}

/// Batch request –¥–ª—è queue
#[derive(Debug)]
#[allow(dead_code)]
struct BatchRequest {
    texts: Vec<String>,
    response_sender: tokio::sync::oneshot::Sender<Result<Vec<Vec<f32>>>>,
    created_at: Instant,
}

impl EmbeddingCoordinator {
    pub fn new(
        gpu_processor: Arc<GpuBatchProcessor>,
        cache: Arc<dyn EmbeddingCacheInterface>,
    ) -> Self {
        let circuit_breaker = CircuitBreaker {
            state: CircuitState::Closed,
            failure_count: 0,
            success_count: 0,
            last_failure: None,
            failure_threshold: 5,
            timeout: Duration::from_secs(30),
            success_threshold: 3,
        };
        
        let adaptive_batch_config = AdaptiveBatchConfig {
            current_size: 32,
            min_size: 8,
            max_size: 128,
            target_latency_ms: 100,
            recent_latencies: VecDeque::with_capacity(10),
            last_adjustment: Instant::now(),
        };
        
        Self {
            gpu_processor,
            cache,
            retry_handler: RetryHandler::new(RetryPolicy::fast()),
            ready: std::sync::atomic::AtomicBool::new(false),
            circuit_breaker: Arc::new(RwLock::new(circuit_breaker)),
            concurrency_limiter: Arc::new(Semaphore::new(16)), // Max 16 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
            adaptive_batch_size: Arc::new(RwLock::new(adaptive_batch_config)),
            model_warmed: Arc::new(std::sync::atomic::AtomicBool::new(false)),
            performance_metrics: Arc::new(RwLock::new(PerformanceMetrics::default())),
            batch_queue: Arc::new(RwLock::new(VecDeque::new())),
            batch_notify: Arc::new(Notify::new()),
        }
    }
    
    /// –°–æ–∑–¥–∞—Ç—å —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π retry –ø–æ–ª–∏—Ç–∏–∫–æ–π
    pub fn with_retry_policy(
        gpu_processor: Arc<GpuBatchProcessor>,
        cache: Arc<dyn EmbeddingCacheInterface>,
        retry_policy: RetryPolicy,
    ) -> Self {
        let circuit_breaker = CircuitBreaker {
            state: CircuitState::Closed,
            failure_count: 0,
            success_count: 0,
            last_failure: None,
            failure_threshold: 5,
            timeout: Duration::from_secs(30),
            success_threshold: 3,
        };
        
        let adaptive_batch_config = AdaptiveBatchConfig {
            current_size: 32,
            min_size: 8,
            max_size: 128,
            target_latency_ms: 100,
            recent_latencies: VecDeque::with_capacity(10),
            last_adjustment: Instant::now(),
        };
        
        Self {
            gpu_processor,
            cache,
            retry_handler: RetryHandler::new(retry_policy),
            ready: std::sync::atomic::AtomicBool::new(false),
            circuit_breaker: Arc::new(RwLock::new(circuit_breaker)),
            concurrency_limiter: Arc::new(Semaphore::new(16)),
            adaptive_batch_size: Arc::new(RwLock::new(adaptive_batch_config)),
            model_warmed: Arc::new(std::sync::atomic::AtomicBool::new(false)),
            performance_metrics: Arc::new(RwLock::new(PerformanceMetrics::default())),
            batch_queue: Arc::new(RwLock::new(VecDeque::new())),
            batch_notify: Arc::new(Notify::new()),
        }
    }
}

#[async_trait]
impl Coordinator for EmbeddingCoordinator {
    async fn initialize(&self) -> Result<()> {
        info!("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è EmbeddingCoordinator");
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ GPU processor –≥–æ—Ç–æ–≤
        let gpu_ready = self.retry_handler
            .execute(|| async {
                // –¢–µ—Å—Ç–æ–≤—ã–π embedding –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                self.gpu_processor.embed("test").await?;
                Ok(())
            })
            .await
            .into_result()
            .is_ok();
            
        if !gpu_ready {
            warn!("GPU processor –Ω–µ –≥–æ—Ç–æ–≤, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è fallback");
        }
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        let cache_stats = self.cache.stats();
        info!("–ö—ç—à embeddings: hits={}, misses={}, size={}MB", 
              cache_stats.0, cache_stats.1, cache_stats.2 / 1024 / 1024);
        
        self.ready.store(true, std::sync::atomic::Ordering::Relaxed);
        info!("‚úÖ EmbeddingCoordinator –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω");
        Ok(())
    }
    
    async fn is_ready(&self) -> bool {
        self.ready.load(std::sync::atomic::Ordering::Relaxed)
    }
    
    async fn shutdown(&self) -> Result<()> {
        info!("–û—Å—Ç–∞–Ω–æ–≤–∫–∞ EmbeddingCoordinator");
        self.ready.store(false, std::sync::atomic::Ordering::Relaxed);
        
        // –£ –Ω–∞—Å –Ω–µ—Ç –º–µ—Ç–æ–¥–∞ flush –≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ, –ø—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ stats
        let (hits, misses, size) = self.cache.stats();
        debug!("–§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞: hits={}, misses={}, size={}MB", 
               hits, misses, size / 1024 / 1024);
        
        Ok(())
    }
    
    async fn metrics(&self) -> serde_json::Value {
        let (hits, misses, size) = self.cache.stats();
        let hit_rate = if hits + misses > 0 {
            (hits as f64 / (hits + misses) as f64) * 100.0
        } else {
            0.0
        };
        
        serde_json::json!({
            "cache": {
                "hits": hits,
                "misses": misses,
                "size_bytes": size,
                "size_mb": size / 1024 / 1024,
                "hit_rate": format!("{:.2}%", hit_rate)
            },
            "ready": self.is_ready().await,
            "gpu_available": true // TODO: –ø–æ–ª—É—á–∏—Ç—å –∏–∑ gpu_processor
        })
    }
}

#[async_trait]
impl EmbeddingCoordinatorTrait for EmbeddingCoordinator {
    async fn get_embedding(&self, text: &str) -> Result<Vec<f32>> {
        // –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if let Some(embedding) = self.check_cache(text).await {
            debug!("Embedding –Ω–∞–π–¥–µ–Ω –≤ –∫—ç—à–µ –¥–ª—è: '{}'", text);
            return Ok(embedding);
        }
        
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º concurrency_limiter –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–≥—Ä—É–∑–∫–∏
        let _permit = self.concurrency_limiter.acquire().await
            .map_err(|e| anyhow::anyhow!("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å permit –¥–ª—è embedding: {}", e))?;
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ model warming
        if !self.model_warmed.load(std::sync::atomic::Ordering::Relaxed) {
            // –ó–∞–ø—É—Å–∫–∞–µ–º model warming
            self.warm_model().await?;
        }
        
        // –ü–æ–ª—É—á–∞–µ–º —á–µ—Ä–µ–∑ GPU processor —Å retry
        let result = self.retry_handler
            .execute_with_fallback(
                || async { self.gpu_processor.embed(text).await },
                || async { 
                    // Fallback –Ω–∞ –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä –≤ –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ
                    warn!("–ò—Å–ø–æ–ª—å–∑—É–µ–º fallback embedding –¥–ª—è: '{}'", text);
                    Ok(vec![0.0; 1024]) // Qwen3 dimension
                }
            )
            .await?;
        
        // –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        if let Err(e) = self.cache.insert(text, "bge-m3", result.clone()) {
            warn!("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å embedding –≤ –∫—ç—à: {}", e);
        }
        
        Ok(result)
    }
    
    async fn get_embeddings(&self, texts: &[String]) -> Result<Vec<Vec<f32>>> {
        debug!("–ü–æ–ª—É—á–µ–Ω–∏–µ batch embeddings –¥–ª—è {} —Ç–µ–∫—Å—Ç–æ–≤", texts.len());
        
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º concurrency_limiter –¥–ª—è batch –æ–ø–µ—Ä–∞—Ü–∏–π
        let _permit = self.concurrency_limiter.acquire().await
            .map_err(|e| anyhow::anyhow!("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å permit –¥–ª—è batch embeddings: {}", e))?;
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ model warming
        if !self.model_warmed.load(std::sync::atomic::Ordering::Relaxed) {
            // –ó–∞–ø—É—Å–∫–∞–µ–º model warming
            self.warm_model().await?;
        }
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        let mut cached_indices = Vec::new();
        let mut uncached_texts = Vec::new();
        let mut results = vec![None; texts.len()];
        
        for (idx, text) in texts.iter().enumerate() {
            if let Some(embedding) = self.check_cache(text).await {
                results[idx] = Some(embedding);
                cached_indices.push(idx);
            } else {
                uncached_texts.push((idx, text.clone()));
            }
        }
        
        debug!("–ù–∞–π–¥–µ–Ω–æ –≤ –∫—ç—à–µ: {}/{}", cached_indices.len(), texts.len());
        
        // –ü–æ–ª—É—á–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —á–µ—Ä–µ–∑ batch processing
        if !uncached_texts.is_empty() {
            let uncached_strings: Vec<String> = uncached_texts.iter()
                .map(|(_, text)| text.clone())
                .collect();
                
            let embeddings = self.retry_handler
                .execute_with_fallback(
                    || async { 
                        self.gpu_processor.embed_batch(uncached_strings.clone()).await 
                    },
                    || async {
                        // Fallback –Ω–∞ –Ω—É–ª–µ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
                        warn!("Batch embedding fallback –¥–ª—è {} —Ç–µ–∫—Å—Ç–æ–≤", uncached_strings.len());
                        Ok(vec![vec![0.0; 768]; uncached_strings.len()])
                    }
                )
                .await?;
            
            // –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for ((idx, text), embedding) in uncached_texts.iter().zip(embeddings.iter()) {
                results[*idx] = Some(embedding.clone());
                if let Err(e) = self.cache.insert(text, "bge-m3", embedding.clone()) {
                    warn!("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å batch embedding –≤ –∫—ç—à: {}", e);
                }
            }
        }
        
        // –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        let final_results: Vec<Vec<f32>> = results
            .into_iter()
            .map(|opt| opt.expect("–í—Å–µ embeddings –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∑–∞–ø–æ–ª–Ω–µ–Ω—ã"))
            .collect();
            
        Ok(final_results)
    }
    
    async fn check_cache(&self, text: &str) -> Option<Vec<f32>> {
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º default model name –¥–ª—è BGE-M3
        self.cache.get(text, "bge-m3")
    }
    
    async fn cache_stats(&self) -> (u64, u64, u64) {
        self.cache.stats()
    }
    
    async fn clear_cache(&self) -> Result<()> {
        info!("üóÑÔ∏è –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ embeddings...");
        self.cache.clear()?;
        
        // –û–±–Ω—É–ª—è–µ–º cache –º–µ—Ç—Ä–∏–∫–∏
        let mut metrics = self.performance_metrics.write().await;
        metrics.cache_hits = 0;
        metrics.cache_misses = 0;
        
        Ok(())
    }
}

impl EmbeddingCoordinator {
    /// –ó–∞–ø—É—Å–∫ batch processing worker'–∞
    #[allow(dead_code)]
    async fn start_batch_processor(&self) {
        let queue = self.batch_queue.clone();
        let notify = self.batch_notify.clone();
        let _gpu_processor = self.gpu_processor.clone();
        let adaptive_config = self.adaptive_batch_size.clone();
        let _circuit_breaker = self.circuit_breaker.clone();
        let _performance_metrics = self.performance_metrics.clone();
        
        tokio::spawn(async move {
            loop {
                // –û–∂–∏–¥–∞–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö
                notify.notified().await;
                
                let current_batch_size = adaptive_config.read().await.current_size;
                let mut batch_requests = Vec::new();
                
                // –°–æ–±–∏—Ä–∞–µ–º batch
                {
                    let mut queue_guard = queue.write().await;
                    for _ in 0..current_batch_size {
                        if let Some(request) = queue_guard.pop_front() {
                            batch_requests.push(request);
                        } else {
                            break;
                        }
                    }
                }
                
                if !batch_requests.is_empty() {
                    debug!("üì¶ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º batch –∏–∑ {} –∑–∞–ø—Ä–æ—Å–æ–≤", batch_requests.len());
                    
                    // –û–±—Ä–∞–±–æ—Ç–∫–∞ batch'–∞ (—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ª–æ–≥–∏–∫–∏)
                    // TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å batch processing
                }
                
                // –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ —á—Ç–æ–±—ã –Ω–µ –Ω–∞–≥—Ä—É–∂–∞—Ç—å CPU
                tokio::time::sleep(Duration::from_millis(10)).await;
            }
        });
        
        debug!("üì¶ Batch processing worker –∑–∞–ø—É—â–µ–Ω");
    }
    
    /// Model warming –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    async fn warm_model(&self) -> Result<()> {
        if self.model_warmed.load(std::sync::atomic::Ordering::Relaxed) {
            return Ok(()); // –£–∂–µ –ø—Ä–æ–≥—Ä–µ—Ç
        }
        
        debug!("üî• –ü—Ä–æ–≥—Ä–µ–≤–∞–µ–º –º–æ–¥–µ–ª—å embedding...");
        let start = std::time::Instant::now();
        
        // –î–µ–ª–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö embedding –¥–ª—è –ø—Ä–æ–≥—Ä–µ–≤–∞
        let warmup_texts = vec![
            "Hello world".to_string(),
            "Test embedding".to_string(),
            "Model warmup".to_string(),
        ];
        
        for text in &warmup_texts {
            if let Err(e) = self.gpu_processor.embed(text).await {
                warn!("–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≥—Ä–µ–≤–µ –º–æ–¥–µ–ª–∏: {}", e);
                return Err(e);
            }
        }
        
        let warmup_duration = start.elapsed();
        info!("‚úÖ –ú–æ–¥–µ–ª—å –ø—Ä–æ–≥—Ä–µ—Ç–∞ –∑–∞ {:?}", warmup_duration);
        
        // –ü–æ–º–µ—á–∞–µ–º –º–æ–¥–µ–ª—å –∫–∞–∫ –ø—Ä–æ–≥—Ä–µ—Ç—É—é
        self.model_warmed.store(true, std::sync::atomic::Ordering::Relaxed);
        
        Ok(())
    }
    
    /// –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    pub async fn get_performance_metrics(&self) -> PerformanceMetrics {
        let metrics = self.performance_metrics.read().await;
        (*metrics).clone()
    }
    
    /// –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ batch —Ä–∞–∑–º–µ—Ä–∞
    #[allow(dead_code)]
    async fn adjust_batch_size(&self, latency_ms: u64) {
        let mut config = self.adaptive_batch_size.write().await;
        
        config.recent_latencies.push_back(latency_ms);
        if config.recent_latencies.len() > 10 {
            config.recent_latencies.pop_front();
        }
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω—É–∂–Ω–æ –ª–∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å
        if config.last_adjustment.elapsed() < std::time::Duration::from_secs(5) {
            return; // –°–ª–∏—à–∫–æ–º —Ä–∞–Ω–æ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        }
        
        let avg_latency: f64 = config.recent_latencies.iter().sum::<u64>() as f64 / config.recent_latencies.len() as f64;
        
        if avg_latency > config.target_latency_ms as f64 * 1.2 {
            // –°–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω–æ - —É–º–µ–Ω—å—à–∞–µ–º batch size
            if config.current_size > config.min_size {
                config.current_size = ((config.current_size as f64) * 0.8) as usize;
                config.current_size = config.current_size.max(config.min_size);
                debug!("‚ö° –£–º–µ–Ω—å—à–∏–ª–∏ batch size –¥–æ {} (avg latency: {:.1}ms)", config.current_size, avg_latency);
            }
        } else if avg_latency < config.target_latency_ms as f64 * 0.8 {
            // –ë—ã—Å—Ç—Ä–æ - —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º batch size
            if config.current_size < config.max_size {
                config.current_size = ((config.current_size as f64) * 1.2) as usize;
                config.current_size = config.current_size.min(config.max_size);
                debug!("üöÄ –£–≤–µ–ª–∏—á–∏–ª–∏ batch size –¥–æ {} (avg latency: {:.1}ms)", config.current_size, avg_latency);
            }
        }
        
        config.last_adjustment = std::time::Instant::now();
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::{
        cache_lru::{EmbeddingCacheLRU, CacheConfig},
        gpu_accelerated::BatchProcessorConfig,
    };
    use tempfile::TempDir;
    
    async fn create_test_coordinator() -> Result<EmbeddingCoordinator> {
        let temp_dir = TempDir::new()?;
        let cache_path = temp_dir.path().join("cache");
        
        let cache = Arc::new(EmbeddingCacheLRU::new(
            cache_path,
            CacheConfig::default()
        )?);
        
        let gpu_processor = Arc::new(GpuBatchProcessor::new(
            BatchProcessorConfig::default(),
            ai::EmbeddingConfig::default(),
            cache.clone(),
        ).await?);
        
        Ok(EmbeddingCoordinator::new(gpu_processor, cache))
    }
    
    #[tokio::test]
    async fn test_coordinator_initialization() -> Result<()> {
        let coordinator = create_test_coordinator().await?;
        
        assert!(!coordinator.is_ready().await);
        coordinator.initialize().await?;
        assert!(coordinator.is_ready().await);
        
        Ok(())
    }
    
    #[tokio::test]
    async fn test_embedding_with_cache() -> Result<()> {
        let coordinator = create_test_coordinator().await?;
        coordinator.initialize().await?;
        
        let text = "test embedding";
        
        // –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å - cache miss
        let embedding1 = coordinator.get_embedding(text).await?;
        let (hits1, misses1, _) = coordinator.cache_stats().await;
        
        // –í—Ç–æ—Ä–æ–π –∑–∞–ø—Ä–æ—Å - cache hit
        let embedding2 = coordinator.get_embedding(text).await?;
        let (hits2, misses2, _) = coordinator.cache_stats().await;
        
        assert_eq!(embedding1, embedding2);
        assert_eq!(hits2, hits1 + 1);
        assert_eq!(misses2, misses1);
        
        Ok(())
    }
}