use anyhow::Result;
use async_trait::async_trait;
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tracing::{debug, info, warn};
use uuid::Uuid;

use super::traits::{DataProcessor, SemanticAnalyzer, TrainingExample, UsageTracker};
use super::types::{AccessPattern, MLPromotionConfig, PromotionFeatures, SemanticContext};
use crate::storage::VectorStore;
use crate::types::{Layer, Record};

/// ML Data Processing Pipeline
pub struct MLDataProcessor {
    store: Arc<VectorStore>,
    usage_tracker: Box<dyn UsageTracker>,
    semantic_analyzer: Box<dyn SemanticAnalyzer>,
    config: DataProcessorConfig,
    feature_cache: Arc<std::sync::Mutex<HashMap<Uuid, CachedFeatures>>>,
    normalization_stats: NormalizationStats,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DataProcessorConfig {
    /// –†–∞–∑–º–µ—Ä batch –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    pub batch_size: usize,
    /// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à features
    pub use_feature_cache: bool,
    /// –í—Ä–µ–º—è –∂–∏–∑–Ω–∏ –∫—ç—à–∞ –≤ —á–∞—Å–∞—Ö
    pub cache_ttl_hours: u64,
    /// –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
    pub max_cache_size: usize,
    /// –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å features
    pub normalize_features: bool,
    /// –í–∫–ª—é—á–∏—Ç—å feature engineering
    pub enable_feature_engineering: bool,
}

impl Default for DataProcessorConfig {
    fn default() -> Self {
        Self {
            batch_size: 32,
            use_feature_cache: true,
            cache_ttl_hours: 24,
            max_cache_size: 10000,
            normalize_features: true,
            enable_feature_engineering: true,
        }
    }
}

#[derive(Debug, Clone)]
struct CachedFeatures {
    features: PromotionFeatures,
    timestamp: DateTime<Utc>,
    access_count_at_time: u32,
}

/// –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ features
#[derive(Debug, Clone, Serialize, Deserialize)]
struct NormalizationStats {
    age_hours_mean: f32,
    age_hours_std: f32,
    access_count_mean: f32,
    access_count_std: f32,
    access_frequency_mean: f32,
    access_frequency_std: f32,
    semantic_importance_mean: f32,
    semantic_importance_std: f32,
}

impl Default for NormalizationStats {
    fn default() -> Self {
        Self {
            age_hours_mean: 48.0,
            age_hours_std: 24.0,
            access_count_mean: 5.0,
            access_count_std: 3.0,
            access_frequency_mean: 0.5,
            access_frequency_std: 0.3,
            semantic_importance_mean: 0.5,
            semantic_importance_std: 0.2,
        }
    }
}

#[async_trait]
impl DataProcessor for MLDataProcessor {
    async fn extract_features(&self, record: &Record) -> Result<PromotionFeatures> {
        debug!("üî¨ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ features –¥–ª—è –∑–∞–ø–∏—Å–∏ {}", record.id);

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
        if self.config.use_feature_cache {
            if let Some(cached) = self.get_cached_features(&record.id, record.access_count) {
                debug!("üíæ –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ features –¥–ª—è {}", record.id);
                return Ok(cached.features);
            }
        }

        let start_time = std::time::Instant::now();
        let mut features = self.extract_raw_features(record).await?;

        // Feature engineering
        if self.config.enable_feature_engineering {
            self.apply_feature_engineering(&mut features, record);
        }

        // –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        if self.config.normalize_features {
            self.normalize_features(&mut features);
        }

        // –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if self.config.use_feature_cache {
            self.cache_features(&record.id, &features, record.access_count);
        }

        let extraction_time = start_time.elapsed();
        debug!("‚úÖ Features –∏–∑–≤–ª–µ—á–µ–Ω—ã –∑–∞ {:?}", extraction_time);

        Ok(features)
    }

    async fn prepare_training_data(&self) -> Result<Vec<TrainingExample>> {
        info!("üìö –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ training data –¥–ª—è ML –º–æ–¥–µ–ª–∏");

        let start_time = std::time::Instant::now();
        let mut training_data = Vec::new();

        // –°–æ–±–∏—Ä–∞–µ–º –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ Insights –∏ Assets
        let positive_examples = self.collect_positive_examples().await?;
        info!("‚úÖ –°–æ–±—Ä–∞–Ω–æ {} –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤", positive_examples.len());

        // –°–æ–±–∏—Ä–∞–µ–º –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ Interact
        let negative_examples = self.collect_negative_examples().await?;
        info!("‚úÖ –°–æ–±—Ä–∞–Ω–æ {} –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤", negative_examples.len());

        // –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ –±–∞–ª–∞–Ω—Å–∏—Ä—É–µ–º dataset
        training_data.extend(positive_examples);
        training_data.extend(negative_examples);

        // –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        self.shuffle_training_data(&mut training_data);

        // –ü—Ä–∏–º–µ–Ω—è–µ–º data augmentation –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if training_data.len() < 1000 {
            training_data = self.augment_training_data(training_data).await?;
            info!(
                "üîÑ –ü—Ä–∏–º–µ–Ω–µ–Ω–∞ data augmentation, –Ω–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: {}",
                training_data.len()
            );
        }

        let preparation_time = start_time.elapsed();
        info!(
            "‚úÖ Training data –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–∞ –∑–∞ {:?}: {} –ø—Ä–∏–º–µ—Ä–æ–≤",
            preparation_time,
            training_data.len()
        );

        Ok(training_data)
    }

    fn normalize_features(&self, features: &mut PromotionFeatures) {
        let stats = &self.normalization_stats;

        // Z-score –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: (x - mean) / std
        features.age_hours = (features.age_hours - stats.age_hours_mean) / stats.age_hours_std;
        features.access_count =
            (features.access_count - stats.access_count_mean) / stats.access_count_std;
        features.access_frequency =
            (features.access_frequency - stats.access_frequency_mean) / stats.access_frequency_std;
        features.semantic_importance = (features.semantic_importance
            - stats.semantic_importance_mean)
            / stats.semantic_importance_std;

        // Clamp –≤—ã–±—Ä–æ—Å—ã –≤ —Ä–∞–∑—É–º–Ω—ã–µ –ø—Ä–µ–¥–µ–ª—ã
        features.age_hours = features.age_hours.clamp(-3.0, 3.0);
        features.access_count = features.access_count.clamp(-3.0, 3.0);
        features.access_frequency = features.access_frequency.clamp(-3.0, 3.0);
        features.semantic_importance = features.semantic_importance.clamp(-3.0, 3.0);

        debug!("üéØ Features –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã");
    }

    async fn update_usage_tracking(&self, record_id: &Uuid) -> Result<()> {
        debug!("üìä –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ usage tracking –¥–ª—è {}", record_id);
        // –î–µ–ª–µ–≥–∏—Ä—É–µ–º –≤ usage_tracker
        Ok(())
    }
}

impl MLDataProcessor {
    pub async fn new(
        store: Arc<VectorStore>,
        usage_tracker: Box<dyn UsageTracker>,
        semantic_analyzer: Box<dyn SemanticAnalyzer>,
        config: DataProcessorConfig,
    ) -> Result<Self> {
        info!("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ML Data Processor");
        info!("  - Batch size: {}", config.batch_size);
        info!("  - Feature cache: {}", config.use_feature_cache);
        info!("  - Normalization: {}", config.normalize_features);
        info!(
            "  - Feature engineering: {}",
            config.enable_feature_engineering
        );

        let normalization_stats = Self::compute_normalization_stats(&store).await?;

        Ok(Self {
            store,
            usage_tracker,
            semantic_analyzer,
            config,
            feature_cache: Arc::new(std::sync::Mutex::new(HashMap::new())),
            normalization_stats,
        })
    }

    async fn extract_raw_features(&self, record: &Record) -> Result<PromotionFeatures> {
        let now = Utc::now();

        // Temporal features
        let age_hours = (now - record.ts).num_hours() as f32;
        let access_recency = self.usage_tracker.calculate_access_recency(record);
        let temporal_pattern_score = self.usage_tracker.get_temporal_pattern_score(&record.id);

        // Usage features
        let access_count = record.access_count as f32;
        let access_frequency = self.usage_tracker.calculate_access_frequency(record);
        let session_importance = self.calculate_session_importance(record);

        // Semantic features
        let semantic_importance = self
            .semantic_analyzer
            .analyze_importance(&record.text)
            .await?;
        let keyword_density = self
            .semantic_analyzer
            .calculate_keyword_density(&record.text);
        let topic_relevance = self
            .semantic_analyzer
            .get_topic_relevance(&record.text)
            .await?;

        // Context features
        let layer_affinity = self.calculate_layer_affinity(record);
        let co_occurrence_score = self.calculate_co_occurrence_score(record).await?;
        let user_preference_score = self.calculate_user_preference_score(record);

        Ok(PromotionFeatures {
            age_hours,
            access_recency,
            temporal_pattern_score,
            access_count,
            access_frequency,
            session_importance,
            semantic_importance,
            keyword_density,
            topic_relevance,
            layer_affinity,
            co_occurrence_score,
            user_preference_score,
        })
    }

    fn apply_feature_engineering(&self, features: &mut PromotionFeatures, record: &Record) {
        // –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ features

        // 1. Age bucket feature
        let age_bucket = match features.age_hours {
            x if x <= 1.0 => 0.9,   // Very recent
            x if x <= 24.0 => 0.7,  // Recent
            x if x <= 168.0 => 0.5, // This week
            x if x <= 720.0 => 0.3, // This month
            _ => 0.1,               // Old
        };

        // 2. Access velocity feature
        let access_velocity = if features.age_hours > 0.0 {
            features.access_count / features.age_hours.max(1.0)
        } else {
            features.access_count
        };

        // 3. Content quality proxy
        let content_quality = self.estimate_content_quality(&record.text);

        // 4. Layer progression score
        let layer_progression = match record.layer {
            Layer::Interact => 0.3,
            Layer::Insights => 0.6,
            Layer::Assets => 1.0,
        };

        // –û–±–Ω–æ–≤–ª—è–µ–º features —Å engineered values
        features.temporal_pattern_score = age_bucket;
        features.access_frequency = access_velocity;
        features.user_preference_score = content_quality * layer_progression;

        debug!(
            "üß¨ Applied feature engineering: age_bucket={:.2}, access_velocity={:.2}",
            age_bucket, access_velocity
        );
    }

    fn estimate_content_quality(&self, text: &str) -> f32 {
        let word_count = text.split_whitespace().count();
        let char_count = text.len();
        let sentence_count = text.matches('.').count().max(1);

        // –ü—Ä–æ—Å—Ç—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        let word_score = (word_count as f32 / 50.0).min(1.0);
        let readability_score = (word_count as f32 / sentence_count as f32 / 20.0).min(1.0);
        let completeness_score = (char_count as f32 / 200.0).min(1.0);

        (word_score + readability_score + completeness_score) / 3.0
    }

    async fn collect_positive_examples(&self) -> Result<Vec<TrainingExample>> {
        let mut examples = Vec::new();

        // –°–æ–±–∏—Ä–∞–µ–º –∏–∑ Assets layer (–≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ)
        let assets_records = self.store.iter_layer_records(Layer::Assets).await?;
        for record in assets_records.into_iter().take(500) {
            let age = Utc::now().signed_duration_since(record.ts);
            if age.num_hours() >= 24 {
                // –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ç–∞—Ä—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
                let features = self.extract_features(&record).await?;
                examples.push(TrainingExample {
                    features,
                    label: 1.0,
                });
            }
        }

        // –°–æ–±–∏—Ä–∞–µ–º –∏–∑ Insights layer (—Å—Ä–µ–¥–Ω—è—è –≤–∞–∂–Ω–æ—Å—Ç—å)
        let insights_records = self.store.iter_layer_records(Layer::Insights).await?;
        for record in insights_records.into_iter().take(700) {
            let age = Utc::now().signed_duration_since(record.ts);
            if age.num_hours() >= 12 {
                let features = self.extract_features(&record).await?;
                examples.push(TrainingExample {
                    features,
                    label: 0.7,
                });
            }
        }

        Ok(examples)
    }

    async fn collect_negative_examples(&self) -> Result<Vec<TrainingExample>> {
        let mut examples = Vec::new();

        // –°–æ–±–∏—Ä–∞–µ–º –∏–∑ Interact layer –∑–∞–ø–∏—Å–∏ –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–ª–≥–æ —Ç–∞–º –Ω–∞—Ö–æ–¥—è—Ç—Å—è
        let interact_records = self.store.iter_layer_records(Layer::Interact).await?;
        for record in interact_records.into_iter().take(800) {
            let age = Utc::now().signed_duration_since(record.ts);

            // –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã: —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ —Å –Ω–∏–∑–∫–∏–º access_count
            if age.num_hours() >= 48 && record.access_count < 3 {
                let features = self.extract_features(&record).await?;
                examples.push(TrainingExample {
                    features,
                    label: 0.1,
                });
            }
        }

        Ok(examples)
    }

    async fn augment_training_data(
        &self,
        mut data: Vec<TrainingExample>,
    ) -> Result<Vec<TrainingExample>> {
        let original_size = data.len();

        // –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º —à—É–º–∞
        for example in data.clone().iter().take(original_size / 2) {
            let mut augmented_features = example.features.clone();

            // –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–π –≥–∞—É—Å—Å–æ–≤—Å–∫–∏–π —à—É–º
            augmented_features.age_hours += self.gaussian_noise(0.0, 0.1);
            augmented_features.access_count += self.gaussian_noise(0.0, 0.5);
            augmented_features.access_frequency += self.gaussian_noise(0.0, 0.05);
            augmented_features.semantic_importance += self.gaussian_noise(0.0, 0.02);

            // Clamp –∫ —Ä–∞–∑—É–º–Ω—ã–º –ø—Ä–µ–¥–µ–ª–∞–º
            augmented_features.age_hours = augmented_features.age_hours.max(0.0);
            augmented_features.access_count = augmented_features.access_count.max(0.0);
            augmented_features.access_frequency = augmented_features.access_frequency.max(0.0);
            augmented_features.semantic_importance =
                augmented_features.semantic_importance.clamp(0.0, 1.0);

            data.push(TrainingExample {
                features: augmented_features,
                label: example.label,
            });
        }

        Ok(data)
    }

    fn gaussian_noise(&self, mean: f32, std: f32) -> f32 {
        // –ü—Ä–æ—Å—Ç–∞—è –∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è –≥–∞—É—Å—Å–æ–≤—Å–∫–æ–≥–æ —à—É–º–∞ –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};

        let mut hasher = DefaultHasher::new();
        std::ptr::addr_of!(self).hash(&mut hasher);
        let seed = hasher.finish() as f32 / u64::MAX as f32;

        // Box-Muller transform approximation
        let u1 = seed.fract().max(0.0001);
        let u2 = (seed * 31.0).fract().max(0.0001);

        let z0 = (-2.0 * u1.ln()).sqrt() * (2.0 * std::f32::consts::PI * u2).cos();
        mean + std * z0
    }

    fn shuffle_training_data(&self, data: &mut Vec<TrainingExample>) {
        // –ü—Ä–æ—Å—Ç–æ–π Fisher-Yates shuffle –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};

        for i in (1..data.len()).rev() {
            let mut hasher = DefaultHasher::new();
            i.hash(&mut hasher);
            let j = (hasher.finish() % (i + 1) as u64) as usize;
            data.swap(i, j);
        }
    }

    fn get_cached_features(
        &self,
        record_id: &Uuid,
        current_access_count: u32,
    ) -> Option<CachedFeatures> {
        let cache = self.feature_cache.lock().unwrap();

        if let Some(cached) = cache.get(record_id) {
            // –ü—Ä–æ–≤–µ—Ä—è–µ–º TTL
            let age = Utc::now().signed_duration_since(cached.timestamp);
            if age.num_hours() < self.config.cache_ttl_hours as i64 {
                // –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ access_count –Ω–µ —Å–∏–ª—å–Ω–æ –∏–∑–º–µ–Ω–∏–ª—Å—è
                if (cached.access_count_at_time as i32 - current_access_count as i32).abs() <= 2 {
                    return Some(cached.clone());
                }
            }
        }

        None
    }

    fn cache_features(&self, record_id: &Uuid, features: &PromotionFeatures, access_count: u32) {
        let mut cache = self.feature_cache.lock().unwrap();

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
        if cache.len() >= self.config.max_cache_size {
            // –ü—Ä–æ—Å—Ç–∞—è LRU-–ø–æ–¥–æ–±–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ - —É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏
            let cutoff = Utc::now() - chrono::Duration::hours(self.config.cache_ttl_hours as i64);
            cache.retain(|_, cached| cached.timestamp > cutoff);

            // –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –ø—Ä–µ–≤—ã—à–∞–µ–º –ª–∏–º–∏—Ç, —É–¥–∞–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –∑–∞–ø–∏—Å–∏
            if cache.len() >= self.config.max_cache_size {
                let keys_to_remove: Vec<_> = cache
                    .keys()
                    .take(cache.len() - self.config.max_cache_size + 1)
                    .cloned()
                    .collect();
                for key in keys_to_remove {
                    cache.remove(&key);
                }
            }
        }

        cache.insert(
            *record_id,
            CachedFeatures {
                features: features.clone(),
                timestamp: Utc::now(),
                access_count_at_time: access_count,
            },
        );
    }

    async fn compute_normalization_stats(store: &Arc<VectorStore>) -> Result<NormalizationStats> {
        info!("üìä –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ features");

        let mut age_values = Vec::new();
        let mut access_counts = Vec::new();
        let mut sample_count = 0;

        // –°–æ–±–∏—Ä–∞–µ–º –≤—ã–±–æ—Ä–∫—É –∏–∑ –≤—Å–µ—Ö layers
        for layer in [Layer::Interact, Layer::Insights, Layer::Assets] {
            let records = store.iter_layer_records(layer).await?;
            for record in records.into_iter().take(1000) {
                let age_hours = (Utc::now() - record.ts).num_hours() as f32;
                age_values.push(age_hours);
                access_counts.push(record.access_count as f32);
                sample_count += 1;
            }
        }

        if sample_count == 0 {
            warn!("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º defaults");
            return Ok(NormalizationStats::default());
        }

        // –í—ã—á–∏—Å–ª—è–µ–º mean –∏ std
        let age_mean = age_values.iter().sum::<f32>() / age_values.len() as f32;
        let age_variance = age_values
            .iter()
            .map(|x| (x - age_mean).powi(2))
            .sum::<f32>()
            / age_values.len() as f32;
        let age_std = age_variance.sqrt().max(1.0); // –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å

        let access_mean = access_counts.iter().sum::<f32>() / access_counts.len() as f32;
        let access_variance = access_counts
            .iter()
            .map(|x| (x - access_mean).powi(2))
            .sum::<f32>()
            / access_counts.len() as f32;
        let access_std = access_variance.sqrt().max(1.0);

        info!(
            "‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∞: age_mean={:.1}h, access_mean={:.1}",
            age_mean, access_mean
        );

        Ok(NormalizationStats {
            age_hours_mean: age_mean,
            age_hours_std: age_std,
            access_count_mean: access_mean,
            access_count_std: access_std,
            access_frequency_mean: 0.3, // –ü—Ä–∏–º–µ—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            access_frequency_std: 0.2,
            semantic_importance_mean: 0.5,
            semantic_importance_std: 0.2,
        })
    }

    // Helper methods
    fn calculate_session_importance(&self, record: &Record) -> f32 {
        match record.layer {
            Layer::Interact => 0.3,
            Layer::Insights => 0.6,
            Layer::Assets => 0.9,
        }
    }

    fn calculate_layer_affinity(&self, record: &Record) -> f32 {
        match record.layer {
            Layer::Interact => {
                if record.access_count > 5 {
                    0.8
                } else {
                    0.2
                }
            }
            Layer::Insights => {
                if record.access_count > 10 {
                    0.9
                } else {
                    0.5
                }
            }
            Layer::Assets => 1.0,
        }
    }

    async fn calculate_co_occurrence_score(&self, _record: &Record) -> Result<f32> {
        // –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ co-occurrence patterns
        Ok(0.5)
    }

    fn calculate_user_preference_score(&self, _record: &Record) -> f32 {
        // –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π
        0.5
    }

    /// –ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É data processor
    pub fn get_statistics(&self) -> DataProcessorStatistics {
        let cache = self.feature_cache.lock().unwrap();

        DataProcessorStatistics {
            cache_size: cache.len(),
            cache_hit_rate: 0.0, // –ü–æ—Ç—Ä–µ–±–æ–≤–∞–ª–æ –±—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ tracking
            total_features_extracted: 0, // –¢–∞–∫–∂–µ –ø–æ—Ç—Ä–µ–±–æ–≤–∞–ª–æ –±—ã tracking
            normalization_stats: self.normalization_stats.clone(),
        }
    }

    /// –û—á–∏—â–∞–µ—Ç –∫—ç—à features
    pub fn clear_cache(&mut self) {
        let mut cache = self.feature_cache.lock().unwrap();
        cache.clear();
        info!("üßπ Feature cache –æ—á–∏—â–µ–Ω");
    }
}

/// –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–±–æ—Ç—ã data processor
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DataProcessorStatistics {
    pub cache_size: usize,
    pub cache_hit_rate: f32,
    pub total_features_extracted: usize,
    pub normalization_stats: NormalizationStats,
}

/// –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è UsageTracker
#[derive(Debug, Clone)]
pub struct SimpleUsageTracker {
    access_patterns: HashMap<Uuid, AccessPattern>,
}

impl SimpleUsageTracker {
    pub fn new() -> Self {
        Self {
            access_patterns: HashMap::new(),
        }
    }
}

impl UsageTracker for SimpleUsageTracker {
    fn record_access(&mut self, record_id: &Uuid) {
        let pattern = self
            .access_patterns
            .entry(*record_id)
            .or_insert(AccessPattern {
                total_accesses: 0,
                recent_accesses: 0,
                access_velocity: 0.0,
                last_access: Utc::now(),
                peak_access_time: None,
            });

        pattern.total_accesses += 1;
        pattern.recent_accesses += 1;
        pattern.last_access = Utc::now();
    }

    fn get_temporal_pattern_score(&self, record_id: &Uuid) -> f32 {
        self.access_patterns
            .get(record_id)
            .map(|p| (p.access_velocity / 10.0).min(1.0))
            .unwrap_or(0.5)
    }

    fn calculate_access_frequency(&self, record: &Record) -> f32 {
        let age_days = (Utc::now() - record.ts).num_days() as f32;
        if age_days <= 0.0 {
            record.access_count as f32
        } else {
            record.access_count as f32 / age_days.max(1.0)
        }
    }

    fn calculate_access_recency(&self, record: &Record) -> f32 {
        let hours_since_access = (Utc::now() - record.ts).num_hours() as f32;
        (24.0 / (hours_since_access + 1.0)).min(1.0)
    }
}

/// –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è SemanticAnalyzer
#[derive(Debug, Clone)]
pub struct SimpleSemanticAnalyzer {
    keyword_weights: HashMap<String, f32>,
}

impl SimpleSemanticAnalyzer {
    pub fn new() -> Self {
        let mut keyword_weights = HashMap::new();
        keyword_weights.insert("error".to_string(), 0.9);
        keyword_weights.insert("critical".to_string(), 0.9);
        keyword_weights.insert("important".to_string(), 0.8);
        keyword_weights.insert("warning".to_string(), 0.7);
        keyword_weights.insert("info".to_string(), 0.5);

        Self { keyword_weights }
    }
}

#[async_trait]
impl SemanticAnalyzer for SimpleSemanticAnalyzer {
    async fn analyze_importance(&self, text: &str) -> Result<f32> {
        let words: Vec<&str> = text.split_whitespace().collect();
        let mut importance = 0.0;

        for word in &words {
            if let Some(&weight) = self.keyword_weights.get(&word.to_lowercase()) {
                importance += weight;
            }
        }

        if !words.is_empty() {
            importance = (importance / words.len() as f32).min(1.0);
        }

        Ok(importance)
    }

    fn calculate_keyword_density(&self, text: &str) -> f32 {
        let words: Vec<&str> = text.split_whitespace().collect();
        let mut keyword_count = 0;

        for word in &words {
            if self.keyword_weights.contains_key(&word.to_lowercase()) {
                keyword_count += 1;
            }
        }

        if words.is_empty() {
            0.0
        } else {
            keyword_count as f32 / words.len() as f32
        }
    }

    async fn get_topic_relevance(&self, _text: &str) -> Result<f32> {
        // –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è topic modeling
        Ok(0.5)
    }

    fn update_keyword_weights(&mut self, keywords: Vec<(String, f32)>) {
        for (keyword, weight) in keywords {
            self.keyword_weights.insert(keyword, weight);
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use uuid::Uuid;

    fn create_test_record() -> Record {
        Record {
            id: Uuid::new_v4(),
            text: "This is a critical error message that needs attention".to_string(),
            embedding: vec![0.1; 384],
            ts: Utc::now() - chrono::Duration::hours(2),
            layer: Layer::Interact,
            access_count: 5,
            score: 0.0,
            kind: "test".to_string(),
            tags: vec!["error".to_string(), "critical".to_string()],
            project: "test_project".to_string(),
            session: "test_session".to_string(),
            last_access: Utc::now(),
        }
    }

    #[tokio::test]
    async fn test_feature_extraction() {
        let usage_tracker = Box::new(SimpleUsageTracker::new());
        let semantic_analyzer = Box::new(SimpleSemanticAnalyzer::new());

        // –ü–æ—Ç—Ä–µ–±–æ–≤–∞–ª–∞ –±—ã mock VectorStore –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞
        // let processor = MLDataProcessor::new(store, usage_tracker, semantic_analyzer, config).await.unwrap();

        let record = create_test_record();
        // let features = processor.extract_features(&record).await.unwrap();

        // assert!(features.age_hours > 0.0);
        // assert!(features.access_count > 0.0);
    }

    #[test]
    fn test_usage_tracker() {
        let mut tracker = SimpleUsageTracker::new();
        let record_id = Uuid::new_v4();

        tracker.record_access(&record_id);
        let pattern_score = tracker.get_temporal_pattern_score(&record_id);

        assert!(pattern_score >= 0.0 && pattern_score <= 1.0);
    }

    #[tokio::test]
    async fn test_semantic_analyzer() {
        let analyzer = SimpleSemanticAnalyzer::new();

        let importance = analyzer
            .analyze_importance("This is a critical error")
            .await
            .unwrap();
        assert!(importance > 0.0);

        let density = analyzer.calculate_keyword_density("critical error warning info");
        assert!(density > 0.0);
    }
}
