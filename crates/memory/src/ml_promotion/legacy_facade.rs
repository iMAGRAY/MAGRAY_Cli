//! Legacy facade –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è 100% –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
//! —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º MLPromotionEngine API –∏–∑ ml_promotion.rs

use anyhow::Result;
use chrono::{DateTime, Utc};
use std::sync::Arc;
use tracing::{debug, info, warn};

use crate::storage::VectorStore;
use crate::types::{Layer, Record};
use super::{
    PromotionCoordinator, PromotionCoordinatorBuilder, MLPromotionConfig, MLPromotionStats,
    PromotionFeatures, create_development_coordinator
};

/// Legacy facade —Ä–µ–ø–ª–∏—Ü–∏—Ä—É—é—â–∏–π —Ç–æ—á–Ω—ã–π API –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ MLPromotionEngine
/// 
/// –≠—Ç–æ—Ç struct –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç 100% –æ–±—Ä–∞—Ç–Ω—É—é —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º,
/// –∏—Å–ø–æ–ª—å–∑—É—è –Ω–æ–≤—É—é –¥–µ–∫–æ–º–ø–æ–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø–æ–¥ –∫–∞–ø–æ—Ç–æ–º.
pub struct MLPromotionEngine {
    coordinator: PromotionCoordinator,
    config: MLPromotionConfig,
}

impl MLPromotionEngine {
    /// –°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π MLPromotionEngine - —Ç–æ—á–Ω–∞—è –∫–æ–ø–∏—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ API
    pub async fn new(store: Arc<VectorStore>, config: MLPromotionConfig) -> Result<Self> {
        info!("üß† –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ML-based Promotion Engine (Legacy Facade)");
        info!("  - Temporal weight: {:.2}", config.temporal_weight);
        info!("  - Semantic weight: {:.2}", config.semantic_weight);
        info!("  - Usage weight: {:.2}", config.usage_weight);
        info!("  - Promotion threshold: {:.2}", config.promotion_threshold);
        info!("  - GPU –¥–ª—è ML: {}", config.use_gpu_for_ml);

        // –°–æ–∑–¥–∞–µ–º coordinator —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
        let coordinator = if config.algorithm_name == "minimal" || config.min_access_threshold == 1 {
            // Development setup –¥–ª—è minimal config
            create_development_coordinator(store).await?
        } else {
            // Production setup –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
            PromotionCoordinatorBuilder::production()
                .with_store(store)
                .with_config(config.clone())
                .build()
                .await?
        };

        Ok(Self {
            coordinator,
            config,
        })
    }

    /// –û—Å–Ω–æ–≤–Ω–æ–π ML-based promotion cycle - —Ç–æ—á–Ω–∞—è –∫–æ–ø–∏—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ API
    pub async fn run_ml_promotion_cycle(&mut self) -> Result<MLPromotionStats> {
        info!("üß† –ó–∞–ø—É—Å–∫ ML-based promotion —Ü–∏–∫–ª–∞ (Legacy API)");
        
        // –î–µ–ª–µ–≥–∏—Ä—É–µ–º –≤ –Ω–æ–≤—ã–π coordinator
        let mut stats = self.coordinator.run_promotion_cycle().await?;
        
        // –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –ø–æ–ª–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        stats.algorithm_used = self.config.algorithm_name.clone();
        
        info!("‚úÖ ML promotion —Ü–∏–∫–ª –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {}ms", stats.processing_time_ms);
        info!("  üìä –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {} –∑–∞–ø–∏—Å–µ–π", stats.total_analyzed);
        info!("  ‚¨ÜÔ∏è Promoted to Insights: {}", stats.promoted_interact_to_insights);
        info!("  ‚¨ÜÔ∏è Promoted to Assets: {}", stats.promoted_insights_to_assets);
        info!("  üéØ Model accuracy: {:.1}%", stats.model_accuracy * 100.0);
        info!("  ‚ö° Avg confidence: {:.2}", stats.avg_confidence_score);

        Ok(stats)
    }

    /// –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ features –¥–ª—è ML –º–æ–¥–µ–ª–∏ - —Ç–æ—á–Ω–∞—è –∫–æ–ø–∏—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ API
    pub async fn extract_features(&self, record: &Record) -> Result<PromotionFeatures> {
        debug!("üî¨ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ features –¥–ª—è –∑–∞–ø–∏—Å–∏ {} (Legacy API)", record.id);
        
        // –í –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ —ç—Ç–æ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª –∏–Ω–∫–∞–ø—Å—É–ª–∏—Ä–æ–≤–∞–Ω –≤ DataProcessor
        // –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å–æ–∑–¥–∞–µ–º features –∏–∑ record –Ω–∞–ø—Ä—è–º—É—é
        let now = Utc::now();
        
        // Temporal features  
        let age_hours = (now - record.ts).num_hours() as f32;
        let access_recency = self.calculate_access_recency(record);
        let temporal_pattern_score = 0.5; // –ó–∞–≥–ª—É—à–∫–∞

        // Usage features
        let access_count = record.access_count as f32;
        let access_frequency = self.calculate_access_frequency(record);
        let session_importance = self.calculate_session_importance(record);

        // Semantic features (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        let semantic_importance = self.calculate_simple_semantic_importance(&record.text);
        let keyword_density = self.calculate_simple_keyword_density(&record.text);
        let topic_relevance = 0.5; // –ó–∞–≥–ª—É—à–∫–∞

        // Context features
        let layer_affinity = self.calculate_layer_affinity(record);
        let co_occurrence_score = 0.5; // –ó–∞–≥–ª—É—à–∫–∞
        let user_preference_score = 0.5; // –ó–∞–≥–ª—É—à–∫–∞

        Ok(PromotionFeatures {
            age_hours,
            access_recency,
            temporal_pattern_score,
            access_count,
            access_frequency,
            session_importance,
            semantic_importance,
            keyword_density,
            topic_relevance,
            layer_affinity,
            co_occurrence_score,
            user_preference_score,
        })
    }

    /// ML inference –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è promotion score - —Ç–æ—á–Ω–∞—è –∫–æ–ø–∏—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ API
    pub fn predict_promotion_score(&self, features: &PromotionFeatures) -> f32 {
        debug!("ü§ñ ML prediction –¥–ª—è features (Legacy API)");
        
        // –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        let temporal_score = 
            features.age_hours * 0.2 +
            features.access_recency * 0.3 +
            features.temporal_pattern_score * 0.5;

        let usage_score = 
            features.access_count * 0.5 +
            features.access_frequency * 0.3 +
            features.session_importance * 0.2;

        let semantic_score = 
            features.semantic_importance * 0.4 +
            features.keyword_density * 0.3 +
            features.topic_relevance * 0.3;

        let final_score = 
            temporal_score * self.config.temporal_weight +
            usage_score * self.config.usage_weight +
            semantic_score * self.config.semantic_weight +
            0.1; // bias

        // Sigmoid activation –¥–ª—è [0,1] range
        1.0 / (1.0 + (-final_score).exp())
    }

    /// –û—Å–Ω–æ–≤–Ω–æ–π API –º–µ—Ç–æ–¥ –¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞ - —Ç–æ—á–Ω–∞—è –∫–æ–ø–∏—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ API
    pub async fn promote(&mut self) -> Result<MLPromotionStats> {
        debug!("üöÄ Promote –≤—ã–∑–≤–∞–Ω (Legacy API)");
        self.coordinator.promote().await
    }
    
    /// –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª ML-promotion —Å –∞–Ω–∞–ª–∏–∑–æ–º –∏ –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ–º - —Ç–æ—á–Ω–∞—è –∫–æ–ø–∏—è
    pub async fn run_promotion_cycle(&mut self) -> Result<MLPromotionStats> {
        info!("üöÄ –ó–∞–ø—É—Å–∫ ML promotion cycle (Legacy API)");
        
        let start_time = std::time::Instant::now();
        
        // –î–µ–ª–µ–≥–∏—Ä—É–µ–º –≤ –Ω–æ–≤—ã–π coordinator –Ω–æ —Å Legacy —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        let mut stats = self.coordinator.run_promotion_cycle().await?;
        
        // –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è legacy —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        stats.analyzed_records = stats.total_analyzed;
        stats.promoted_records = stats.promoted_interact_to_insights + stats.promoted_insights_to_assets;
        stats.processing_time_ms = start_time.elapsed().as_millis() as f64;
        
        info!("‚úÖ ML promotion cycle –∑–∞–≤–µ—Ä—à–µ–Ω: –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {}, –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ {}, –≤—Ä–µ–º—è {:.2}ms", 
              stats.analyzed_records, stats.promoted_records, stats.processing_time_ms);
        
        Ok(stats)
    }

    // Helper methods - —Ç–æ—á–Ω—ã–µ –∫–æ–ø–∏–∏ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    
    fn calculate_access_recency(&self, record: &Record) -> f32 {
        let now = Utc::now();
        let last_access = record.ts;
        let recency_hours = (now - last_access).num_hours() as f32;
        
        // –ò–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º (–±–æ–ª–µ–µ recent = –≤—ã—à–µ score)
        (24.0 / (recency_hours + 1.0)).min(1.0)
    }

    fn calculate_access_frequency(&self, record: &Record) -> f32 {
        let age_days = (Utc::now() - record.ts).num_days() as f32;
        if age_days <= 0.0 { 
            return record.access_count as f32;
        }
        
        record.access_count as f32 / age_days
    }

    fn calculate_session_importance(&self, record: &Record) -> f32 {
        // Placeholder for complex session analysis
        match record.layer {
            Layer::Interact => 0.3,
            Layer::Insights => 0.6,
            Layer::Assets => 0.9,
        }
    }

    fn calculate_layer_affinity(&self, record: &Record) -> f32 {
        // –ê–Ω–∞–ª–∏–∑ —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç–∏ –∑–∞–ø–∏—Å–∏ –∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º—É —Å–ª–æ—é
        match record.layer {
            Layer::Interact => if record.access_count > 5 { 0.8 } else { 0.2 },
            Layer::Insights => if record.access_count > 10 { 0.9 } else { 0.5 },
            Layer::Assets => 1.0,
        }
    }

    fn calculate_simple_semantic_importance(&self, text: &str) -> f32 {
        let words: Vec<&str> = text.split_whitespace().collect();
        
        // –ü—Ä–æ—Å—Ç—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏
        let mut importance = 0.0;
        for word in &words {
            let word_lower = word.to_lowercase();
            match word_lower.as_str() {
                "error" | "critical" | "urgent" => importance += 0.9,
                "warning" | "important" | "issue" => importance += 0.7,
                "info" | "note" | "update" => importance += 0.5,
                _ => importance += 0.1,
            }
        }
        
        if !words.is_empty() {
            importance = (importance / words.len() as f32).min(1.0);
        }
        
        importance
    }

    fn calculate_simple_keyword_density(&self, text: &str) -> f32 {
        let words: Vec<&str> = text.split_whitespace().collect();
        let mut keyword_count = 0;
        
        for word in &words {
            let word_lower = word.to_lowercase();
            match word_lower.as_str() {
                "error" | "critical" | "urgent" | "warning" | "important" | "issue" => {
                    keyword_count += 1;
                }
                _ => {}
            }
        }
        
        if words.is_empty() { 0.0 } else { keyword_count as f32 / words.len() as f32 }
    }
}

/// –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –ø–æ–ª–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
/// —Å internal —Ç–∏–ø–∞–º–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ ml_promotion.rs
pub struct UsageTracker {
    // –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
}

impl UsageTracker {
    pub fn new() -> Self {
        Self {}
    }
}

/// –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –ø–æ–ª–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏  
pub struct SemanticAnalyzer {
    keyword_weights: std::collections::HashMap<String, f32>,
}

impl SemanticAnalyzer {
    pub fn new() -> Self {
        let mut keyword_weights = std::collections::HashMap::new();
        keyword_weights.insert("error".to_string(), 0.9);
        keyword_weights.insert("critical".to_string(), 0.9);
        keyword_weights.insert("important".to_string(), 0.8);
        
        Self {
            keyword_weights,
        }
    }

    pub async fn analyze_importance(&self, text: &str) -> Result<f32> {
        let words: Vec<&str> = text.split_whitespace().collect();
        let mut importance = 0.0;
        
        for word in &words {
            let word_lower = word.to_lowercase();
            if let Some(&weight) = self.keyword_weights.get(&word_lower) {
                importance += weight;
            }
        }
        
        if !words.is_empty() {
            importance = (importance / words.len() as f32).min(1.0);
        }
        
        Ok(importance)
    }

    pub fn calculate_keyword_density(&self, text: &str) -> f32 {
        let words: Vec<&str> = text.split_whitespace().collect();
        let mut keyword_count = 0;
        
        for word in &words {
            if self.keyword_weights.contains_key(&word.to_lowercase()) {
                keyword_count += 1;
            }
        }
        
        if words.is_empty() { 0.0 } else { keyword_count as f32 / words.len() as f32 }
    }

    pub async fn get_topic_relevance(&self, _text: &str) -> Result<f32> {
        // Placeholder –¥–ª—è topic modeling
        Ok(0.5)
    }
}

/// –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –ø–æ–ª–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
pub struct PerformanceOptimizer {
    // –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
}

impl PerformanceOptimizer {
    pub fn new() -> Self {
        Self {}
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use uuid::Uuid;

    fn create_test_record() -> Record {
        Record {
            id: Uuid::new_v4(),
            text: "This is a critical error message".to_string(),
            embedding: vec![0.1; 384],
            ts: Utc::now() - chrono::Duration::hours(2),
            layer: Layer::Interact,
            access_count: 5,
            score: 0.0,
            kind: "test".to_string(),
            tags: vec!["error".to_string(), "critical".to_string()],
            project: "test_project".to_string(),
            session: "test_session".to_string(),
            last_access: Utc::now(),
        }
    }

    #[test]
    fn test_legacy_structs_creation() {
        let _usage_tracker = UsageTracker::new();
        let _semantic_analyzer = SemanticAnalyzer::new();
        let _performance_optimizer = PerformanceOptimizer::new();
        
        // –í—Å–µ legacy structs —Å–æ–∑–¥–∞—é—Ç—Å—è –±–µ–∑ –æ—à–∏–±–æ–∫
        assert!(true);
    }

    #[tokio::test]
    async fn test_semantic_analyzer_legacy_api() {
        let analyzer = SemanticAnalyzer::new();
        
        let importance = analyzer.analyze_importance("This is a critical error").await.unwrap();
        assert!(importance > 0.0);
        
        let density = analyzer.calculate_keyword_density("critical error important");
        assert!(density > 0.0);
        
        let topic_relevance = analyzer.get_topic_relevance("test text").await.unwrap();
        assert_eq!(topic_relevance, 0.5);
    }

    #[test]
    fn test_helper_methods_compatibility() {
        let config = MLPromotionConfig::default();
        // –ù–µ –º–æ–∂–µ–º —Å–æ–∑–¥–∞—Ç—å –ø–æ–ª–Ω—ã–π MLPromotionEngine –±–µ–∑ VectorStore
        // –Ω–æ –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å helper methods –ª–æ–≥–∏–∫—É
        
        let record = create_test_record();
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–µ—Ç–æ–¥—ã –∏–º–µ—é—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Å–∏–≥–Ω–∞—Ç—É—Ä—ã
        let _: fn(&Record) -> f32 = |r| {
            let age_days = (Utc::now() - r.ts).num_days() as f32;
            if age_days <= 0.0 { 
                r.access_count as f32
            } else {
                r.access_count as f32 / age_days
            }
        };
        
        assert!(true);
    }

    #[test] 
    fn test_features_extraction_logic() {
        let record = create_test_record();
        
        // –¢–µ—Å—Ç–∏—Ä—É–µ–º –ª–æ–≥–∏–∫—É –≤—ã—á–∏—Å–ª–µ–Ω–∏—è features –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ engine
        let age_hours = (Utc::now() - record.ts).num_hours() as f32;
        assert!(age_hours >= 0.0);
        
        let access_count = record.access_count as f32;
        assert!(access_count > 0.0);
        
        // –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è semantic importance
        let words: Vec<&str> = record.text.split_whitespace().collect();
        let has_critical = words.iter().any(|w| w.to_lowercase().contains("critical"));
        assert!(has_critical); // –í test record –µ—Å—Ç—å "critical"
    }

    #[test]
    fn test_prediction_logic() {
        let config = MLPromotionConfig::default();
        
        // –¢–µ—Å—Ç–∏—Ä—É–µ–º prediction –ª–æ–≥–∏–∫—É
        let temporal_score = 0.5 * config.temporal_weight;
        let usage_score = 1.0 * config.usage_weight;
        let semantic_score = 0.7 * config.semantic_weight;
        
        let final_score = temporal_score + usage_score + semantic_score + 0.1;
        let prediction = 1.0 / (1.0 + (-final_score).exp());
        
        assert!(prediction >= 0.0 && prediction <= 1.0);
        
        // Sigmoid –¥–æ–ª–∂–µ–Ω –¥–∞–≤–∞—Ç—å —Ä–∞–∑—É–º–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        assert!(prediction > 0.1 && prediction < 0.9);
    }
}