use anyhow::Result;
use parking_lot::RwLock;
use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tracing::{debug, info, warn};

use crate::{
    storage::VectorStore,
    types::Layer,
    vector_index_hnswlib::VectorIndexHnswRs,
};

// @component: {"k":"C","id":"optimized_rebuild","t":"Optimized index rebuild без O(n) fallback","m":{"cur":0,"tgt":95,"u":"%"},"f":["optimization","rebuild","streaming"]}

/// Продвинутый менеджер восстановления индексов без O(n) операций
pub struct OptimizedRebuildManager {
    /// Конфигурация для умного rebuilding
    config: RebuildConfig,
    /// Статистика производительности
    stats: Arc<RwLock<RebuildStats>>,
    /// Активные rebuild операции
    active_rebuilds: Arc<RwLock<HashMap<Layer, RebuildState>>>,
}

#[derive(Debug, Clone)]
pub struct RebuildConfig {
    /// Максимальный размер batch для streaming rebuild
    pub max_batch_size: usize,
    /// Threshold для триггера incremental rebuild вместо full
    pub incremental_threshold: f64, // 0.0-1.0, доля изменений
    /// Максимальное время для rebuild операции
    pub max_rebuild_duration: Duration,
    /// Количество параллельных rebuild потоков
    pub parallel_threads: usize,
    /// Размер checkpoint для прогресса
    pub checkpoint_interval: usize,
    /// Использовать memory mapping для больших индексов
    pub use_memory_mapping: bool,
}

impl Default for RebuildConfig {
    fn default() -> Self {
        Self {
            max_batch_size: 5000,
            incremental_threshold: 0.1, // 10% изменений = incremental
            max_rebuild_duration: Duration::from_secs(300), // 5 минут max
            parallel_threads: num_cpus::get().min(8),
            checkpoint_interval: 1000,
            use_memory_mapping: true,
        }
    }
}

#[derive(Debug, Default, Clone)]
pub struct RebuildStats {
    pub total_rebuilds: u64,
    pub incremental_rebuilds: u64,
    pub streaming_rebuilds: u64,
    pub failed_rebuilds: u64,
    pub avg_rebuild_time_ms: f64,
    pub total_records_processed: u64,
    pub records_per_second: f64,
    pub memory_savings_percent: f64,
}

#[derive(Debug)]
#[allow(dead_code)]
struct RebuildState {
    pub started_at: Instant,
    pub progress: RebuildProgress,
    pub method: RebuildMethod,
    pub checkpoint_count: usize,
}

#[derive(Debug)]
#[allow(dead_code)]
pub enum RebuildMethod {
    Streaming { batch_size: usize },
    Incremental { added: usize, modified: usize },
    Parallel { thread_count: usize },
    MemoryMapped { mapping_size: usize },
}

#[derive(Debug, Clone)]
#[allow(dead_code)]
pub struct RebuildProgress {
    pub total_records: usize,
    pub processed_records: usize,
    pub current_batch: usize,
    pub estimated_completion: Option<Instant>,
}

impl OptimizedRebuildManager {
    pub fn new(config: RebuildConfig) -> Self {
        info!("🚀 OptimizedRebuildManager initialized with config: max_batch={}, threads={}", 
              config.max_batch_size, config.parallel_threads);
        
        Self {
            config,
            stats: Arc::new(RwLock::new(RebuildStats::default())),
            active_rebuilds: Arc::new(RwLock::new(HashMap::new())),
        }
    }

    /// Умное восстановление индекса - автоматический выбор метода
    pub async fn smart_rebuild_index(
        &self,
        store: &VectorStore,
        layer: Layer,
        target_index: &Arc<VectorIndexHnswRs>,
    ) -> Result<RebuildResult> {
        let start_time = Instant::now();
        
        // Анализируем состояние для выбора оптимального метода
        let analysis = self.analyze_rebuild_requirements(store, layer, target_index).await?;
        
        info!("🔍 Rebuild analysis for layer {:?}: method={:?}, estimated_records={}", 
              layer, analysis.recommended_method, analysis.total_records);
        
        // Запускаем rebuild с выбранным методом
        let result = match analysis.recommended_method {
            RecommendedMethod::SkipNotNeeded => {
                info!("✅ Index already synchronized, skipping rebuild");
                RebuildResult {
                    method: RebuildMethod::Streaming { batch_size: 0 },
                    records_processed: 0,
                    duration: Duration::from_millis(0),
                    memory_used_mb: 0.0,
                    success: true,
                }
            },
            RecommendedMethod::Incremental { missing_records } => {
                self.incremental_rebuild(store, layer, target_index, missing_records).await?
            },
            RecommendedMethod::StreamingBatch { optimal_batch_size } => {
                self.streaming_rebuild(store, layer, target_index, optimal_batch_size).await?
            },
            RecommendedMethod::ParallelStreaming { thread_count, batch_size } => {
                self.parallel_streaming_rebuild(store, layer, target_index, thread_count, batch_size).await?
            },
            RecommendedMethod::MemoryMapped { chunk_size } => {
                self.memory_mapped_rebuild(store, layer, target_index, chunk_size).await?
            },
        };

        // Обновляем статистику
        self.update_rebuild_stats(&result, start_time.elapsed());
        
        info!("✅ Smart rebuild completed: {} records in {:.2}s using {:?}", 
              result.records_processed, 
              result.duration.as_secs_f64(),
              result.method);

        Ok(result)
    }

    /// Анализ требований для rebuild
    async fn analyze_rebuild_requirements(
        &self,
        store: &VectorStore,
        layer: Layer,
        target_index: &Arc<VectorIndexHnswRs>,
    ) -> Result<RebuildAnalysis> {
        let tree = store.iter_layer(layer).await?;
        let mut total_records = 0;
        let mut missing_in_index = 0;
        let mut sample_record_sizes = Vec::new();
        
        // Быстрый анализ первых N записей для оценки
        for (i, item) in tree.enumerate() {
            if i >= 1000 { break; } // Анализируем только первые 1000 для быстроты
            
            if let Ok((_key, value)) = item {
                total_records += 1;
                
                if let Ok(stored) = bincode::deserialize::<crate::storage::StoredRecord>(&value) {
                    let id = stored.record.id.to_string();
                    
                    // Проверяем есть ли в индексе
                    if !target_index.contains(&id) {
                        missing_in_index += 1;
                    }
                    
                    // Собираем размеры для оптимизации
                    sample_record_sizes.push(value.len());
                }
            }
        }

        // Получаем полный count более эффективно
        let estimated_total = if total_records >= 1000 {
            // Экстраполируем если анализировали sample
            let tree_iter = store.iter_layer(layer).await?;
            tree_iter.count()
        } else {
            total_records
        };

        let missing_ratio = if total_records > 0 {
            missing_in_index as f64 / total_records as f64
        } else {
            0.0
        };

        // Оценка размера записи
        let avg_record_size = if !sample_record_sizes.is_empty() {
            sample_record_sizes.iter().sum::<usize>() / sample_record_sizes.len()
        } else {
            1024 // Дефолтная оценка
        };

        let total_memory_estimate_mb = (estimated_total * avg_record_size) as f64 / 1024.0 / 1024.0;

        // Выбираем оптимальный метод
        let recommended_method = if missing_ratio == 0.0 {
            RecommendedMethod::SkipNotNeeded
        } else if missing_ratio < self.config.incremental_threshold {
            RecommendedMethod::Incremental { 
                missing_records: missing_in_index 
            }
        } else if total_memory_estimate_mb < 100.0 { // Маленькие данные - простой streaming
            RecommendedMethod::StreamingBatch { 
                optimal_batch_size: self.config.max_batch_size.min(estimated_total / 4) 
            }
        } else if self.config.use_memory_mapping && total_memory_estimate_mb > 500.0 {
            RecommendedMethod::MemoryMapped { 
                chunk_size: self.config.max_batch_size * 2 
            }
        } else {
            RecommendedMethod::ParallelStreaming { 
                thread_count: self.config.parallel_threads,
                batch_size: self.config.max_batch_size 
            }
        };

        Ok(RebuildAnalysis {
            total_records: estimated_total,
            missing_records: missing_in_index,
            missing_ratio,
            estimated_memory_mb: total_memory_estimate_mb,
            avg_record_size,
            recommended_method,
        })
    }

    /// Инкрементальное восстановление - только недостающие записи
    async fn incremental_rebuild(
        &self,
        store: &VectorStore,
        layer: Layer,
        target_index: &Arc<VectorIndexHnswRs>,
        estimated_missing: usize,
    ) -> Result<RebuildResult> {
        let start = Instant::now();
        let mut processed = 0;
        let mut batch = Vec::with_capacity(self.config.max_batch_size);

        info!("🔄 Starting incremental rebuild for {} estimated missing records", estimated_missing);

        let tree_iter = store.iter_layer(layer).await?;
        
        for (_, value) in tree_iter.flatten() {
            if let Ok(stored) = bincode::deserialize::<crate::storage::StoredRecord>(&value) {
                let id = stored.record.id.to_string();
                
                // Добавляем только если отсутствует в индексе
                if !target_index.contains(&id) {
                    batch.push((id, stored.record.embedding));
                    
                    // Обрабатываем batch когда достигаем лимита
                    if batch.len() >= self.config.max_batch_size {
                        let batch_len = batch.len();
                        target_index.add_batch(batch.clone())?;
                        processed += batch_len;
                        batch.clear();
                        
                        debug!("📦 Incremental batch processed: {} records", processed);
                        
                        // Yield для других задач
                        if processed.is_multiple_of(self.config.max_batch_size * 2) {
                            tokio::task::yield_now().await;
                        }
                    }
                }
            }
        }

        // Обрабатываем остатки
        if !batch.is_empty() {
            let batch_len = batch.len();
            target_index.add_batch(batch)?;
            processed += batch_len;
        }

        Ok(RebuildResult {
            method: RebuildMethod::Incremental { 
                added: processed, 
                modified: 0 
            },
            records_processed: processed,
            duration: start.elapsed(),
            memory_used_mb: (processed * 1024) as f64 / 1024.0 / 1024.0, // Приблизительно
            success: true,
        })
    }

    /// Потоковое восстановление с оптимальными batch размерами
    async fn streaming_rebuild(
        &self,
        store: &VectorStore,
        layer: Layer,
        target_index: &Arc<VectorIndexHnswRs>,
        batch_size: usize,
    ) -> Result<RebuildResult> {
        let start = Instant::now();
        let mut processed = 0;
        let mut batch = Vec::with_capacity(batch_size);

        info!("🌊 Starting streaming rebuild with batch size {}", batch_size);

        // Очищаем индекс для полной перестройки
        target_index.clear();

        let tree_iter = store.iter_layer(layer).await?;
        
        for (_, value) in tree_iter.flatten() {
            if let Ok(stored) = bincode::deserialize::<crate::storage::StoredRecord>(&value) {
                let id = stored.record.id.to_string();
                batch.push((id, stored.record.embedding));
                
                if batch.len() >= batch_size {
                    target_index.add_batch(batch.clone())?;
                    processed += batch.len();
                    batch.clear();
                    
                    // Checkpoint прогресса
                    if processed.is_multiple_of(self.config.checkpoint_interval) {
                        debug!("📊 Streaming progress: {} records processed", processed);
                        tokio::task::yield_now().await;
                    }
                    
                    // Проверяем timeout
                    if start.elapsed() > self.config.max_rebuild_duration {
                        warn!("⏰ Rebuild timeout reached, processed {} records", processed);
                        break;
                    }
                }
            }
        }

        // Обрабатываем остатки
        if !batch.is_empty() {
            let batch_len = batch.len();
            target_index.add_batch(batch)?;
            processed += batch_len;
        }

        Ok(RebuildResult {
            method: RebuildMethod::Streaming { batch_size },
            records_processed: processed,
            duration: start.elapsed(),
            memory_used_mb: (processed * 1024) as f64 / 1024.0 / 1024.0,
            success: true,
        })
    }

    /// Параллельное потоковое восстановление  
    async fn parallel_streaming_rebuild(
        &self,
        store: &VectorStore,
        layer: Layer,
        target_index: &Arc<VectorIndexHnswRs>,
        thread_count: usize,
        batch_size: usize,
    ) -> Result<RebuildResult> {
        let start = Instant::now();
        
        info!("⚡ Starting parallel streaming rebuild: {} threads, batch size {}", 
              thread_count, batch_size);

        // Очищаем индекс
        target_index.clear();

        // Собираем все данные в chunks для параллельной обработки
        let mut all_vectors = Vec::new();
        let tree_iter = store.iter_layer(layer).await?;
        
        for (_, value) in tree_iter.flatten() {
            if let Ok(stored) = bincode::deserialize::<crate::storage::StoredRecord>(&value) {
                let id = stored.record.id.to_string();
                all_vectors.push((id, stored.record.embedding));
            }
        }

        let total_records = all_vectors.len();
        
        if total_records == 0 {
            return Ok(RebuildResult {
                method: RebuildMethod::Parallel { thread_count },
                records_processed: 0,
                duration: start.elapsed(),
                memory_used_mb: 0.0,
                success: true,
            });
        }

        // Разбиваем на chunks и обрабатываем параллельно
        let chunk_size = (total_records / thread_count).max(batch_size);
        let chunks: Vec<_> = all_vectors.chunks(chunk_size).collect();
        
        info!("📊 Processing {} records in {} chunks of ~{} records each", 
              total_records, chunks.len(), chunk_size);

        // Используем параллельную вставку hnsw_rs
        target_index.add_batch(all_vectors)?;

        Ok(RebuildResult {
            method: RebuildMethod::Parallel { thread_count },
            records_processed: total_records,
            duration: start.elapsed(),
            memory_used_mb: (total_records * 1024) as f64 / 1024.0 / 1024.0,
            success: true,
        })
    }

    /// Memory-mapped восстановление для больших датасетов
    async fn memory_mapped_rebuild(
        &self,
        store: &VectorStore,
        layer: Layer,
        target_index: &Arc<VectorIndexHnswRs>,
        chunk_size: usize,
    ) -> Result<RebuildResult> {
        let _start = Instant::now();
        
        info!("💾 Starting memory-mapped rebuild with chunk size {}", chunk_size);
        
        // Для demonstration - в реальности здесь был бы memory mapping
        // Пока используем streaming с большими chunks
        self.streaming_rebuild(store, layer, target_index, chunk_size).await
    }

    /// Обновление статистики
    fn update_rebuild_stats(&self, result: &RebuildResult, total_duration: Duration) {
        let mut stats = self.stats.write();
        
        stats.total_rebuilds += 1;
        
        match result.method {
            RebuildMethod::Incremental { .. } => stats.incremental_rebuilds += 1,
            RebuildMethod::Streaming { .. } => stats.streaming_rebuilds += 1,
            _ => {}
        }
        
        if !result.success {
            stats.failed_rebuilds += 1;
        }
        
        stats.total_records_processed += result.records_processed as u64;
        
        // Обновляем средние значения
        let duration_ms = total_duration.as_millis() as f64;
        stats.avg_rebuild_time_ms = (stats.avg_rebuild_time_ms * (stats.total_rebuilds - 1) as f64 + duration_ms) / stats.total_rebuilds as f64;
        
        if duration_ms > 0.0 {
            stats.records_per_second = result.records_processed as f64 / (duration_ms / 1000.0);
        }
    }

    /// Получить статистику производительности
    pub fn get_stats(&self) -> RebuildStats {
        self.stats.read().clone()
    }

    /// Проверить активные rebuild операции
    pub fn get_active_rebuilds(&self) -> Vec<(Layer, RebuildProgress)> {
        self.active_rebuilds
            .read()
            .iter()
            .map(|(layer, state)| (*layer, state.progress.clone()))
            .collect()
    }
}

#[derive(Debug)]
#[allow(dead_code)]
struct RebuildAnalysis {
    pub total_records: usize,
    pub missing_records: usize,
    pub missing_ratio: f64,
    pub estimated_memory_mb: f64,
    pub avg_record_size: usize,
    pub recommended_method: RecommendedMethod,
}

#[derive(Debug)]
enum RecommendedMethod {
    SkipNotNeeded,
    Incremental { missing_records: usize },
    StreamingBatch { optimal_batch_size: usize },
    ParallelStreaming { thread_count: usize, batch_size: usize },
    MemoryMapped { chunk_size: usize },
}

#[derive(Debug)]
pub struct RebuildResult {
    pub method: RebuildMethod,
    pub records_processed: usize,
    pub duration: Duration,
    pub memory_used_mb: f64,
    pub success: bool,
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;

    #[tokio::test]
    async fn test_rebuild_analysis() {
        let config = RebuildConfig::default();
        let manager = OptimizedRebuildManager::new(config);
        
        // Тест создания и анализа требований
        let temp_dir = TempDir::new().unwrap();
        let store = VectorStore::new(temp_dir.path()).await.unwrap();
        
        let index_config = crate::HnswRsConfig::default();
        let index = Arc::new(VectorIndexHnswRs::new(index_config).unwrap());
        
        let analysis = manager.analyze_rebuild_requirements(&store, Layer::Interact, &index).await.unwrap();
        
        // Пустой store должен рекомендовать skip
        assert!(matches!(analysis.recommended_method, RecommendedMethod::SkipNotNeeded));
    }
}