use anyhow::Result;
use parking_lot::RwLock;
use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tracing::{debug, info, warn};

use crate::{
    storage::VectorStore,
    types::Layer,
    vector_index_hnswlib::VectorIndexHnswRs,
};

// @component: {"k":"C","id":"optimized_rebuild","t":"Optimized index rebuild –±–µ–∑ O(n) fallback","m":{"cur":0,"tgt":95,"u":"%"},"f":["optimization","rebuild","streaming"]}

/// –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤ –±–µ–∑ O(n) –æ–ø–µ—Ä–∞—Ü–∏–π
pub struct OptimizedRebuildManager {
    /// –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —É–º–Ω–æ–≥–æ rebuilding
    config: RebuildConfig,
    /// –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    stats: Arc<RwLock<RebuildStats>>,
    /// –ê–∫—Ç–∏–≤–Ω—ã–µ rebuild –æ–ø–µ—Ä–∞—Ü–∏–∏
    active_rebuilds: Arc<RwLock<HashMap<Layer, RebuildState>>>,
}

#[derive(Debug, Clone)]
pub struct RebuildConfig {
    /// –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä batch –¥–ª—è streaming rebuild
    pub max_batch_size: usize,
    /// Threshold –¥–ª—è —Ç—Ä–∏–≥–≥–µ—Ä–∞ incremental rebuild –≤–º–µ—Å—Ç–æ full
    pub incremental_threshold: f64, // 0.0-1.0, –¥–æ–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏–π
    /// –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –¥–ª—è rebuild –æ–ø–µ—Ä–∞—Ü–∏–∏
    pub max_rebuild_duration: Duration,
    /// –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö rebuild –ø–æ—Ç–æ–∫–æ–≤
    pub parallel_threads: usize,
    /// –†–∞–∑–º–µ—Ä checkpoint –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    pub checkpoint_interval: usize,
    /// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å memory mapping –¥–ª—è –±–æ–ª—å—à–∏—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
    pub use_memory_mapping: bool,
}

impl Default for RebuildConfig {
    fn default() -> Self {
        Self {
            max_batch_size: 5000,
            incremental_threshold: 0.1, // 10% –∏–∑–º–µ–Ω–µ–Ω–∏–π = incremental
            max_rebuild_duration: Duration::from_secs(300), // 5 –º–∏–Ω—É—Ç max
            parallel_threads: num_cpus::get().min(8),
            checkpoint_interval: 1000,
            use_memory_mapping: true,
        }
    }
}

#[derive(Debug, Default, Clone)]
pub struct RebuildStats {
    pub total_rebuilds: u64,
    pub incremental_rebuilds: u64,
    pub streaming_rebuilds: u64,
    pub failed_rebuilds: u64,
    pub avg_rebuild_time_ms: f64,
    pub total_records_processed: u64,
    pub records_per_second: f64,
    pub memory_savings_percent: f64,
}

#[derive(Debug)]
#[allow(dead_code)]
struct RebuildState {
    pub started_at: Instant,
    pub progress: RebuildProgress,
    pub method: RebuildMethod,
    pub checkpoint_count: usize,
}

#[derive(Debug)]
#[allow(dead_code)]
pub enum RebuildMethod {
    Streaming { batch_size: usize },
    Incremental { added: usize, modified: usize },
    Parallel { thread_count: usize },
    MemoryMapped { mapping_size: usize },
}

#[derive(Debug, Clone)]
#[allow(dead_code)]
pub struct RebuildProgress {
    pub total_records: usize,
    pub processed_records: usize,
    pub current_batch: usize,
    pub estimated_completion: Option<Instant>,
}

impl OptimizedRebuildManager {
    pub fn new(config: RebuildConfig) -> Self {
        info!("üöÄ OptimizedRebuildManager initialized with config: max_batch={}, threads={}", 
              config.max_batch_size, config.parallel_threads);
        
        Self {
            config,
            stats: Arc::new(RwLock::new(RebuildStats::default())),
            active_rebuilds: Arc::new(RwLock::new(HashMap::new())),
        }
    }

    /// –£–º–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞
    pub async fn smart_rebuild_index(
        &self,
        store: &VectorStore,
        layer: Layer,
        target_index: &Arc<VectorIndexHnswRs>,
    ) -> Result<RebuildResult> {
        let start_time = Instant::now();
        
        // –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –≤—ã–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞
        let analysis = self.analyze_rebuild_requirements(store, layer, target_index).await?;
        
        info!("üîç Rebuild analysis for layer {:?}: method={:?}, estimated_records={}", 
              layer, analysis.recommended_method, analysis.total_records);
        
        // –ó–∞–ø—É—Å–∫–∞–µ–º rebuild —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º –º–µ—Ç–æ–¥–æ–º
        let result = match analysis.recommended_method {
            RecommendedMethod::SkipNotNeeded => {
                info!("‚úÖ Index already synchronized, skipping rebuild");
                RebuildResult {
                    method: RebuildMethod::Streaming { batch_size: 0 },
                    records_processed: 0,
                    duration: Duration::from_millis(0),
                    memory_used_mb: 0.0,
                    success: true,
                }
            },
            RecommendedMethod::Incremental { missing_records } => {
                self.incremental_rebuild(store, layer, target_index, missing_records).await?
            },
            RecommendedMethod::StreamingBatch { optimal_batch_size } => {
                self.streaming_rebuild(store, layer, target_index, optimal_batch_size).await?
            },
            RecommendedMethod::ParallelStreaming { thread_count, batch_size } => {
                self.parallel_streaming_rebuild(store, layer, target_index, thread_count, batch_size).await?
            },
            RecommendedMethod::MemoryMapped { chunk_size } => {
                self.memory_mapped_rebuild(store, layer, target_index, chunk_size).await?
            },
        };

        // –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.update_rebuild_stats(&result, start_time.elapsed());
        
        info!("‚úÖ Smart rebuild completed: {} records in {:.2}s using {:?}", 
              result.records_processed, 
              result.duration.as_secs_f64(),
              result.method);

        Ok(result)
    }

    /// –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –¥–ª—è rebuild
    async fn analyze_rebuild_requirements(
        &self,
        store: &VectorStore,
        layer: Layer,
        target_index: &Arc<VectorIndexHnswRs>,
    ) -> Result<RebuildAnalysis> {
        let tree = store.iter_layer(layer).await?;
        let mut total_records = 0;
        let mut missing_in_index = 0;
        let mut sample_record_sizes = Vec::new();
        
        // –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤—ã—Ö N –∑–∞–ø–∏—Å–µ–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        for (i, item) in tree.enumerate() {
            if i >= 1000 { break; } // –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 1000 –¥–ª—è –±—ã—Å—Ç—Ä–æ—Ç—ã
            
            if let Ok((_key, value)) = item {
                total_records += 1;
                
                if let Ok(stored) = bincode::deserialize::<crate::storage::StoredRecord>(&value) {
                    let id = stored.record.id.to_string();
                    
                    // –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –≤ –∏–Ω–¥–µ–∫—Å–µ
                    if !target_index.contains(&id) {
                        missing_in_index += 1;
                    }
                    
                    // –°–æ–±–∏—Ä–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
                    sample_record_sizes.push(value.len());
                }
            }
        }

        // –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π count –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ
        let estimated_total = if total_records >= 1000 {
            // –≠–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ–º –µ—Å–ª–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏ sample
            let tree_iter = store.iter_layer(layer).await?;
            tree_iter.count()
        } else {
            total_records
        };

        let missing_ratio = if total_records > 0 {
            missing_in_index as f64 / total_records as f64
        } else {
            0.0
        };

        // –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –∑–∞–ø–∏—Å–∏
        let avg_record_size = if !sample_record_sizes.is_empty() {
            sample_record_sizes.iter().sum::<usize>() / sample_record_sizes.len()
        } else {
            1024 // –î–µ—Ñ–æ–ª—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        };

        let total_memory_estimate_mb = (estimated_total * avg_record_size) as f64 / 1024.0 / 1024.0;

        // –í—ã–±–∏—Ä–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥
        let recommended_method = if missing_ratio == 0.0 {
            RecommendedMethod::SkipNotNeeded
        } else if missing_ratio < self.config.incremental_threshold {
            RecommendedMethod::Incremental { 
                missing_records: missing_in_index 
            }
        } else if total_memory_estimate_mb < 100.0 { // –ú–∞–ª–µ–Ω—å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ - –ø—Ä–æ—Å—Ç–æ–π streaming
            RecommendedMethod::StreamingBatch { 
                optimal_batch_size: self.config.max_batch_size.min(estimated_total / 4) 
            }
        } else if self.config.use_memory_mapping && total_memory_estimate_mb > 500.0 {
            RecommendedMethod::MemoryMapped { 
                chunk_size: self.config.max_batch_size * 2 
            }
        } else {
            RecommendedMethod::ParallelStreaming { 
                thread_count: self.config.parallel_threads,
                batch_size: self.config.max_batch_size 
            }
        };

        Ok(RebuildAnalysis {
            total_records: estimated_total,
            missing_records: missing_in_index,
            missing_ratio,
            estimated_memory_mb: total_memory_estimate_mb,
            avg_record_size,
            recommended_method,
        })
    }

    /// –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ - —Ç–æ–ª—å–∫–æ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∑–∞–ø–∏—Å–∏
    async fn incremental_rebuild(
        &self,
        store: &VectorStore,
        layer: Layer,
        target_index: &Arc<VectorIndexHnswRs>,
        estimated_missing: usize,
    ) -> Result<RebuildResult> {
        let start = Instant::now();
        let mut processed = 0;
        let mut batch = Vec::with_capacity(self.config.max_batch_size);

        info!("üîÑ Starting incremental rebuild for {} estimated missing records", estimated_missing);

        let tree_iter = store.iter_layer(layer).await?;
        
        for item in tree_iter {
            if let Ok((_, value)) = item {
                if let Ok(stored) = bincode::deserialize::<crate::storage::StoredRecord>(&value) {
                    let id = stored.record.id.to_string();
                    
                    // –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –∏–Ω–¥–µ–∫—Å–µ
                    if !target_index.contains(&id) {
                        batch.push((id, stored.record.embedding));
                        
                        // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º batch –∫–æ–≥–¥–∞ –¥–æ—Å—Ç–∏–≥–∞–µ–º –ª–∏–º–∏—Ç–∞
                        if batch.len() >= self.config.max_batch_size {
                            let batch_len = batch.len();
                            target_index.add_batch(batch.clone())?;
                            processed += batch_len;
                            batch.clear();
                            
                            debug!("üì¶ Incremental batch processed: {} records", processed);
                            
                            // Yield –¥–ª—è –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á
                            if processed % (self.config.max_batch_size * 2) == 0 {
                                tokio::task::yield_now().await;
                            }
                        }
                    }
                }
            }
        }

        // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞—Ç–∫–∏
        if !batch.is_empty() {
            let batch_len = batch.len();
            target_index.add_batch(batch)?;
            processed += batch_len;
        }

        Ok(RebuildResult {
            method: RebuildMethod::Incremental { 
                added: processed, 
                modified: 0 
            },
            records_processed: processed,
            duration: start.elapsed(),
            memory_used_mb: (processed * 1024) as f64 / 1024.0 / 1024.0, // –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ
            success: true,
        })
    }

    /// –ü–æ—Ç–æ–∫–æ–≤–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ batch —Ä–∞–∑–º–µ—Ä–∞–º–∏
    async fn streaming_rebuild(
        &self,
        store: &VectorStore,
        layer: Layer,
        target_index: &Arc<VectorIndexHnswRs>,
        batch_size: usize,
    ) -> Result<RebuildResult> {
        let start = Instant::now();
        let mut processed = 0;
        let mut batch = Vec::with_capacity(batch_size);

        info!("üåä Starting streaming rebuild with batch size {}", batch_size);

        // –û—á–∏—â–∞–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è –ø–æ–ª–Ω–æ–π –ø–µ—Ä–µ—Å—Ç—Ä–æ–π–∫–∏
        target_index.clear();

        let tree_iter = store.iter_layer(layer).await?;
        
        for item in tree_iter {
            if let Ok((_, value)) = item {
                if let Ok(stored) = bincode::deserialize::<crate::storage::StoredRecord>(&value) {
                    let id = stored.record.id.to_string();
                    batch.push((id, stored.record.embedding));
                    
                    if batch.len() >= batch_size {
                        target_index.add_batch(batch.clone())?;
                        processed += batch.len();
                        batch.clear();
                        
                        // Checkpoint –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                        if processed % self.config.checkpoint_interval == 0 {
                            debug!("üìä Streaming progress: {} records processed", processed);
                            tokio::task::yield_now().await;
                        }
                        
                        // –ü—Ä–æ–≤–µ—Ä—è–µ–º timeout
                        if start.elapsed() > self.config.max_rebuild_duration {
                            warn!("‚è∞ Rebuild timeout reached, processed {} records", processed);
                            break;
                        }
                    }
                }
            }
        }

        // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞—Ç–∫–∏
        if !batch.is_empty() {
            let batch_len = batch.len();
            target_index.add_batch(batch)?;
            processed += batch_len;
        }

        Ok(RebuildResult {
            method: RebuildMethod::Streaming { batch_size },
            records_processed: processed,
            duration: start.elapsed(),
            memory_used_mb: (processed * 1024) as f64 / 1024.0 / 1024.0,
            success: true,
        })
    }

    /// –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –ø–æ—Ç–æ–∫–æ–≤–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ  
    async fn parallel_streaming_rebuild(
        &self,
        store: &VectorStore,
        layer: Layer,
        target_index: &Arc<VectorIndexHnswRs>,
        thread_count: usize,
        batch_size: usize,
    ) -> Result<RebuildResult> {
        let start = Instant::now();
        
        info!("‚ö° Starting parallel streaming rebuild: {} threads, batch size {}", 
              thread_count, batch_size);

        // –û—á–∏—â–∞–µ–º –∏–Ω–¥–µ–∫—Å
        target_index.clear();

        // –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –≤ chunks –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        let mut all_vectors = Vec::new();
        let tree_iter = store.iter_layer(layer).await?;
        
        for item in tree_iter {
            if let Ok((_, value)) = item {
                if let Ok(stored) = bincode::deserialize::<crate::storage::StoredRecord>(&value) {
                    let id = stored.record.id.to_string();
                    all_vectors.push((id, stored.record.embedding));
                }
            }
        }

        let total_records = all_vectors.len();
        
        if total_records == 0 {
            return Ok(RebuildResult {
                method: RebuildMethod::Parallel { thread_count },
                records_processed: 0,
                duration: start.elapsed(),
                memory_used_mb: 0.0,
                success: true,
            });
        }

        // –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ chunks –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        let chunk_size = (total_records / thread_count).max(batch_size);
        let chunks: Vec<_> = all_vectors.chunks(chunk_size).collect();
        
        info!("üìä Processing {} records in {} chunks of ~{} records each", 
              total_records, chunks.len(), chunk_size);

        // –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –≤—Å—Ç–∞–≤–∫—É hnsw_rs
        target_index.add_batch(all_vectors)?;

        Ok(RebuildResult {
            method: RebuildMethod::Parallel { thread_count },
            records_processed: total_records,
            duration: start.elapsed(),
            memory_used_mb: (total_records * 1024) as f64 / 1024.0 / 1024.0,
            success: true,
        })
    }

    /// Memory-mapped –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    async fn memory_mapped_rebuild(
        &self,
        store: &VectorStore,
        layer: Layer,
        target_index: &Arc<VectorIndexHnswRs>,
        chunk_size: usize,
    ) -> Result<RebuildResult> {
        let _start = Instant::now();
        
        info!("üíæ Starting memory-mapped rebuild with chunk size {}", chunk_size);
        
        // –î–ª—è demonstration - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã memory mapping
        // –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º streaming —Å –±–æ–ª—å—à–∏–º–∏ chunks
        self.streaming_rebuild(store, layer, target_index, chunk_size).await
    }

    /// –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    fn update_rebuild_stats(&self, result: &RebuildResult, total_duration: Duration) {
        let mut stats = self.stats.write();
        
        stats.total_rebuilds += 1;
        
        match result.method {
            RebuildMethod::Incremental { .. } => stats.incremental_rebuilds += 1,
            RebuildMethod::Streaming { .. } => stats.streaming_rebuilds += 1,
            _ => {}
        }
        
        if !result.success {
            stats.failed_rebuilds += 1;
        }
        
        stats.total_records_processed += result.records_processed as u64;
        
        // –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
        let duration_ms = total_duration.as_millis() as f64;
        stats.avg_rebuild_time_ms = (stats.avg_rebuild_time_ms * (stats.total_rebuilds - 1) as f64 + duration_ms) / stats.total_rebuilds as f64;
        
        if duration_ms > 0.0 {
            stats.records_per_second = result.records_processed as f64 / (duration_ms / 1000.0);
        }
    }

    /// –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    pub fn get_stats(&self) -> RebuildStats {
        self.stats.read().clone()
    }

    /// –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–µ rebuild –æ–ø–µ—Ä–∞—Ü–∏–∏
    pub fn get_active_rebuilds(&self) -> Vec<(Layer, RebuildProgress)> {
        self.active_rebuilds
            .read()
            .iter()
            .map(|(layer, state)| (*layer, state.progress.clone()))
            .collect()
    }
}

#[derive(Debug)]
#[allow(dead_code)]
struct RebuildAnalysis {
    pub total_records: usize,
    pub missing_records: usize,
    pub missing_ratio: f64,
    pub estimated_memory_mb: f64,
    pub avg_record_size: usize,
    pub recommended_method: RecommendedMethod,
}

#[derive(Debug)]
enum RecommendedMethod {
    SkipNotNeeded,
    Incremental { missing_records: usize },
    StreamingBatch { optimal_batch_size: usize },
    ParallelStreaming { thread_count: usize, batch_size: usize },
    MemoryMapped { chunk_size: usize },
}

#[derive(Debug)]
pub struct RebuildResult {
    pub method: RebuildMethod,
    pub records_processed: usize,
    pub duration: Duration,
    pub memory_used_mb: f64,
    pub success: bool,
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;

    #[tokio::test]
    async fn test_rebuild_analysis() {
        let config = RebuildConfig::default();
        let manager = OptimizedRebuildManager::new(config);
        
        // –¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è –∏ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
        let temp_dir = TempDir::new().unwrap();
        let store = VectorStore::new(temp_dir.path()).await.unwrap();
        
        let index_config = crate::HnswRsConfig::default();
        let index = Arc::new(VectorIndexHnswRs::new(index_config).unwrap());
        
        let analysis = manager.analyze_rebuild_requirements(&store, Layer::Interact, &index).await.unwrap();
        
        // –ü—É—Å—Ç–æ–π store –¥–æ–ª–∂–µ–Ω —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å skip
        assert!(matches!(analysis.recommended_method, RecommendedMethod::SkipNotNeeded));
    }
}