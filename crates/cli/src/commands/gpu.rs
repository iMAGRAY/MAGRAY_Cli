#[cfg(feature = "gpu")]
use ai::{
    gpu_detector::GpuDetector, gpu_memory_pool::GPU_MEMORY_POOL, tensorrt_cache::TENSORRT_CACHE,
};
use anyhow::Result;
use clap::{Args, Subcommand};
use tracing::{info, warn};

#[derive(Debug, Args)]
pub struct GpuCommand {
    #[command(subcommand)]
    command: GpuSubcommand,
}

#[derive(Debug, Subcommand)]
enum GpuSubcommand {
    /// –ü–æ–∫–∞–∑–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö GPU
    #[command(visible_alias = "i")]
    Info,

    /// –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å GPU
    #[command(visible_alias = "b")]
    Benchmark {
        /// –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –±–∞—Ç—á–∞
        #[arg(short, long, default_value = "100")]
        batch_size: usize,

        /// –°—Ä–∞–≤–Ω–∏—Ç—å —Å CPU
        #[arg(short, long)]
        compare: bool,
    },

    /// –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫—ç—à–µ–º
    Cache {
        #[command(subcommand)]
        action: CacheAction,
    },

    /// –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é GPU
    Memory {
        #[command(subcommand)]
        action: MemoryAction,
    },

    /// –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ GPU
    #[command(visible_alias = "o")]
    Optimize {
        /// –ò–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        #[arg(default_value = "qwen3emb")]
        model: String,
    },
}

#[derive(Debug, Subcommand)]
enum CacheAction {
    /// –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞
    Stats,

    /// –û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à TensorRT
    Clear,

    /// –ü–æ–∫–∞–∑–∞—Ç—å —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
    Size,
}

#[derive(Debug, Subcommand)]
enum MemoryAction {
    /// –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–º—è—Ç–∏
    Stats,

    /// –û—á–∏—Å—Ç–∏—Ç—å –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –±—É—Ñ–µ—Ä—ã
    Clear,
}

impl GpuCommand {
    pub async fn execute(self) -> Result<()> {
        match self.command {
            GpuSubcommand::Info => self.show_info(),
            GpuSubcommand::Benchmark {
                batch_size,
                compare,
            } => self.run_benchmark(batch_size, compare).await,
            GpuSubcommand::Cache { ref action } => self.handle_cache(action).await,
            GpuSubcommand::Memory { ref action } => self.handle_memory(action),
            GpuSubcommand::Optimize { ref model } => self.optimize_model(model).await,
        }
    }

    /// –ü–æ–∫–∞–∑–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU
    fn show_info(&self) -> Result<()> {
        #[cfg(feature = "gpu")]
        {
            let detector = GpuDetector::detect();
            detector.print_detailed_info();

            if !detector.available {
                warn!("üí° –ü–æ–¥—Å–∫–∞–∑–∫–∞: –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è GPU –ø–æ–¥–¥–µ—Ä–∂–∫–∏:");
                warn!("  1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ NVIDIA –¥—Ä–∞–π–≤–µ—Ä—ã –∏ CUDA Toolkit");
                warn!("  2. –ü–µ—Ä–µ—Å–æ–±–µ—Ä–∏—Ç–µ —Å: cargo build --release --features gpu");
                warn!("  3. –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ nvidia-smi –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ PATH");
            }

            Ok(())
        }

        #[cfg(not(feature = "gpu"))]
        {
            warn!("GPU —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –°–æ–±–µ—Ä–∏—Ç–µ —Å --features gpu");
            Ok(())
        }
    }

    /// –ó–∞–ø—É—Å—Ç–∏—Ç—å –±–µ–Ω—á–º–∞—Ä–∫
    async fn run_benchmark(&self, _batch_size: usize, _compare: bool) -> Result<()> {
        info!("üèÉ –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ GPU —Å batch_size={}", _batch_size);

        #[cfg(feature = "gpu")]
        {
            let detector = GpuDetector::detect();
            if detector.available {
                // TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –ø–æ—Å–ª–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ GPU
                info!("üöß –ë–µ–Ω—á–º–∞—Ä–∫ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ");
            } else {
                warn!("‚ùå GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ 'magray gpu info' –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏.");
            }
        }

        #[cfg(not(feature = "gpu"))]
        {
            warn!("GPU —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –°–æ–±–µ—Ä–∏—Ç–µ —Å --features gpu");
        }

        Ok(())
    }

    /// –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫—ç—à–µ–º
    async fn handle_cache(&self, _action: &CacheAction) -> Result<()> {
        #[cfg(feature = "gpu")]
        {
            match _action {
                CacheAction::Stats => {
                    let stats = TENSORRT_CACHE.get_stats()?;
                    stats.print();
                }
                CacheAction::Clear => {
                    TENSORRT_CACHE.clear_cache()?;
                    info!("‚úÖ –ö—ç—à TensorRT –æ—á–∏—â–µ–Ω");
                }
                CacheAction::Size => {
                    let stats = TENSORRT_CACHE.get_stats()?;
                    info!(
                        "üì¶ –†–∞–∑–º–µ—Ä –∫—ç—à–∞ TensorRT: {:.2} GB",
                        stats.total_size as f64 / 1024.0 / 1024.0 / 1024.0
                    );
                }
            }
        }

        #[cfg(not(feature = "gpu"))]
        {
            warn!("GPU —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –°–æ–±–µ—Ä–∏—Ç–µ —Å --features gpu");
        }

        Ok(())
    }

    /// –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é
    fn handle_memory(&self, _action: &MemoryAction) -> Result<()> {
        #[cfg(feature = "gpu")]
        {
            match _action {
                MemoryAction::Stats => {
                    let _ = GPU_MEMORY_POOL.print_stats();
                }
                MemoryAction::Clear => {
                    let _ = GPU_MEMORY_POOL.clear_unused();
                    info!("‚úÖ –ù–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –±—É—Ñ–µ—Ä—ã GPU –æ—á–∏—â–µ–Ω—ã");
                }
            }
        }

        #[cfg(not(feature = "gpu"))]
        {
            warn!("GPU —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –°–æ–±–µ—Ä–∏—Ç–µ —Å --features gpu");
        }

        Ok(())
    }

    /// –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å
    async fn optimize_model(&self, _model_name: &str) -> Result<()> {
        #[cfg(feature = "gpu")]
        {
            info!("üîß –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ {} –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ GPU...", _model_name);

            let detector = GpuDetector::detect();
            if !detector.available {
                warn!("‚ùå GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω!");
                return Ok(());
            }

            // –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
            info!("üì• –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –º–æ–¥–µ–ª–∏...");
            use ai::model_downloader::MODEL_DOWNLOADER;
            let model_path = MODEL_DOWNLOADER.ensure_model(_model_name).await?;
            info!("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {:?}", model_path);

            // –°–æ–∑–¥–∞—ë–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å
            use ai::EmbeddingConfig;
            let config = EmbeddingConfig {
                model_name: _model_name.to_string(),
                use_gpu: true,
                ..Default::default()
            };

            info!("üöÄ –°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞...");
            use ai::auto_device_selector::SmartEmbeddingFactory;
            let (service, decision) = SmartEmbeddingFactory::create_optimized(config).await?;

            info!("‚úÖ –ú–æ–¥–µ–ª—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞!");
            info!(
                "  - –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {}",
                if decision.use_gpu { "GPU" } else { "CPU" }
            );
            info!("  - Batch size: {}", decision.recommended_batch_size);

            // –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫
            info!("\nüß™ –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫...");
            let test_texts = vec!["Hello, world!".to_string()];
            let start = std::time::Instant::now();
            let _ = service.embed_batch(test_texts).await?;
            let elapsed = start.elapsed();

            info!("‚úÖ –¢–µ—Å—Ç —É—Å–ø–µ—à–µ–Ω! –í—Ä–µ–º—è: {:.2} –º—Å", elapsed.as_millis());
        }

        #[cfg(not(feature = "gpu"))]
        {
            warn!("GPU —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –°–æ–±–µ—Ä–∏—Ç–µ —Å --features gpu");
        }

        Ok(())
    }
}

/// –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞ —Ä–µ—à–µ–Ω–∏—è
#[allow(dead_code)]
trait DecisionExt {
    fn print_decision(&self);
}

impl DecisionExt for ai::auto_device_selector::DeviceDecision {
    fn print_decision(&self) {
        info!("\nü§ñ –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–±–æ—Ä–∞:");
        info!(
            "  - –í—ã–±—Ä–∞–Ω–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {}",
            if self.use_gpu { "GPU üéÆ" } else { "CPU üíª" }
        );
        info!("  - –ü—Ä–∏—á–∏–Ω–∞: {}", self.reason);
        info!(
            "  - CPU –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {:.1} items/sec",
            self.cpu_score
        );
        if let Some(gpu_score) = self.gpu_score {
            info!("  - GPU –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {:.1} items/sec", gpu_score);
            let speedup = gpu_score / self.cpu_score;
            info!(
                "  - –£—Å–∫–æ—Ä–µ–Ω–∏–µ GPU: {:.1}x {}",
                speedup,
                match speedup {
                    x if x > 10.0 => "üöÄüöÄüöÄ",
                    x if x > 5.0 => "üöÄüöÄ",
                    x if x > 2.0 => "üöÄ",
                    _ => "‚ö°",
                }
            );
        }
        info!(
            "  - –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π batch size: {}",
            self.recommended_batch_size
        );
    }
}
