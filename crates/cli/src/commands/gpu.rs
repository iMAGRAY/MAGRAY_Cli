use anyhow::Result;
use clap::{Args, Subcommand};
use ai::{
    gpu_detector::GpuDetector,
    gpu_memory_pool::GPU_MEMORY_POOL,
    tensorrt_cache::TENSORRT_CACHE,
    model_downloader::MODEL_DOWNLOADER,
    auto_device_selector::{AutoDeviceSelector, SmartEmbeddingFactory},
    EmbeddingConfig,
};
use tracing::{info, warn, error};

/// @component: {"k":"C","id":"gpu_commands","t":"GPU management CLI","m":{"cur":95,"tgt":100,"u":"%"}}
#[derive(Debug, Args)]
pub struct GpuCommand {
    #[command(subcommand)]
    command: GpuSubcommand,
}

#[derive(Debug, Subcommand)]
enum GpuSubcommand {
    /// –ü–æ–∫–∞–∑–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö GPU
    #[command(visible_alias = "i")]
    Info,
    
    /// –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å GPU
    #[command(visible_alias = "b")]
    Benchmark {
        /// –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –±–∞—Ç—á–∞
        #[arg(short, long, default_value = "100")]
        batch_size: usize,
        
        /// –°—Ä–∞–≤–Ω–∏—Ç—å —Å CPU
        #[arg(short, long)]
        compare: bool,
    },
    
    /// –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫—ç—à–µ–º
    Cache {
        #[command(subcommand)]
        action: CacheAction,
    },
    
    /// –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é GPU
    Memory {
        #[command(subcommand)]
        action: MemoryAction,
    },
    
    /// –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ GPU
    #[command(visible_alias = "o")]
    Optimize {
        /// –ò–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        #[arg(default_value = "bge-m3")]
        model: String,
    },
}

#[derive(Debug, Subcommand)]
enum CacheAction {
    /// –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞
    Stats,
    
    /// –û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à TensorRT
    Clear,
    
    /// –ü–æ–∫–∞–∑–∞—Ç—å —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
    Size,
}

#[derive(Debug, Subcommand)]
enum MemoryAction {
    /// –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–º—è—Ç–∏
    Stats,
    
    /// –û—á–∏—Å—Ç–∏—Ç—å –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –±—É—Ñ–µ—Ä—ã
    Clear,
}

impl GpuCommand {
    pub async fn execute(self) -> Result<()> {
        match self.command {
            GpuSubcommand::Info => self.show_info(),
            GpuSubcommand::Benchmark { batch_size, compare } => {
                self.run_benchmark(batch_size, compare).await
            }
            GpuSubcommand::Cache { ref action } => self.handle_cache(action).await,
            GpuSubcommand::Memory { ref action } => self.handle_memory(action),
            GpuSubcommand::Optimize { ref model } => self.optimize_model(model).await,
        }
    }
    
    /// –ü–æ–∫–∞–∑–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU
    fn show_info(&self) -> Result<()> {
        let detector = GpuDetector::detect();
        detector.print_detailed_info();
        
        if !detector.available {
            warn!("üí° –ü–æ–¥—Å–∫–∞–∑–∫–∞: –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è GPU –ø–æ–¥–¥–µ—Ä–∂–∫–∏:");
            warn!("  1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ NVIDIA –¥—Ä–∞–π–≤–µ—Ä—ã –∏ CUDA Toolkit");
            warn!("  2. –ü–µ—Ä–µ—Å–æ–±–µ—Ä–∏—Ç–µ —Å: cargo build --release --features gpu");
            warn!("  3. –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ nvidia-smi –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ PATH");
        }
        
        Ok(())
    }
    
    /// –ó–∞–ø—É—Å—Ç–∏—Ç—å –±–µ–Ω—á–º–∞—Ä–∫
    async fn run_benchmark(&self, batch_size: usize, compare: bool) -> Result<()> {
        info!("üèÉ –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ GPU —Å batch_size={}", batch_size);
        
        let detector = GpuDetector::detect();
        if !detector.available {
            error!("‚ùå GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ 'magray gpu info' –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏.");
            return Ok(());
        }
        
        // –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        let test_texts: Vec<String> = (0..batch_size)
<<<<<<< HEAD
            .map(|i| format!("This is test text number {i} for benchmarking embedding performance on our optimized service with GPU acceleration."))
=======
            .map(|i| format!("This is test text number {} for benchmarking embedding performance on our optimized service with GPU acceleration.", i))
>>>>>>> cdac5c55f689e319aa18d538b93d7c8f8759a52c
            .collect();
        
        // –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        let config = EmbeddingConfig {
            model_name: "bge-m3".to_string(),
            use_gpu: true,
            batch_size,
            ..Default::default()
        };
        
        if compare {
            info!("\nüìä –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ CPU vs GPU");
            
            // –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
            let mut selector = AutoDeviceSelector::new();
            let decision = selector.select_device(&config).await?;
            decision.print_decision();
            
            info!("\nüèÜ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å {}", 
                if decision.use_gpu { "GPU" } else { "CPU" }
            );
        } else {
            // –¢–æ–ª—å–∫–æ GPU —Ç–µ—Å—Ç
            use ai::embeddings_gpu::GpuEmbeddingService;
            use std::time::Instant;
            
            info!("‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...");
            let service = GpuEmbeddingService::new(config).await?;
            
            // –ü—Ä–æ–≥—Ä–µ–≤
            info!("üî• –ü—Ä–æ–≥—Ä–µ–≤ GPU...");
            let warmup_batch = test_texts.iter().take(10).cloned().collect();
            let _ = service.embed_batch(warmup_batch).await?;
            
            // –ë–µ–Ω—á–º–∞—Ä–∫
            info!("‚ö° –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞...");
            let start = Instant::now();
            let embeddings = service.embed_batch(test_texts.clone()).await?;
            let elapsed = start.elapsed();
            
            // –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
            info!("\nüìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞ GPU:");
            info!("  - –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤: {}", batch_size);
            info!("  - –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {:.2} —Å–µ–∫", elapsed.as_secs_f64());
            info!("  - –°–∫–æ—Ä–æ—Å—Ç—å: {:.1} —Ç–µ–∫—Å—Ç–æ–≤/—Å–µ–∫", batch_size as f64 / elapsed.as_secs_f64());
            info!("  - –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {:.2} –º—Å/—Ç–µ–∫—Å—Ç", elapsed.as_millis() as f64 / batch_size as f64);
            info!("  - –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {}", embeddings[0].len());
            
            // –ú–µ—Ç—Ä–∏–∫–∏
            service.print_metrics();
        }
        
        Ok(())
    }
    
    /// –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫—ç—à–µ–º
    async fn handle_cache(&self, action: &CacheAction) -> Result<()> {
        match action {
            CacheAction::Stats => {
                let stats = TENSORRT_CACHE.get_stats()?;
                stats.print();
            }
            CacheAction::Clear => {
                TENSORRT_CACHE.clear_cache()?;
                info!("‚úÖ –ö—ç—à TensorRT –æ—á–∏—â–µ–Ω");
            }
            CacheAction::Size => {
                let stats = TENSORRT_CACHE.get_stats()?;
                info!("üì¶ –†–∞–∑–º–µ—Ä –∫—ç—à–∞ TensorRT: {:.2} GB", 
                    stats.total_size as f64 / 1024.0 / 1024.0 / 1024.0
                );
            }
        }
        Ok(())
    }
    
    /// –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é
    fn handle_memory(&self, action: &MemoryAction) -> Result<()> {
        match action {
            MemoryAction::Stats => {
                GPU_MEMORY_POOL.print_stats();
            }
            MemoryAction::Clear => {
                GPU_MEMORY_POOL.clear_unused();
                info!("‚úÖ –ù–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –±—É—Ñ–µ—Ä—ã GPU –æ—á–∏—â–µ–Ω—ã");
            }
        }
        Ok(())
    }
    
    /// –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å
    async fn optimize_model(&self, model_name: &String) -> Result<()> {
        info!("üîß –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ {} –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ GPU...", model_name);
        
        let detector = GpuDetector::detect();
        if !detector.available {
            error!("‚ùå GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω!");
            return Ok(());
        }
        
        // –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
        info!("üì• –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –º–æ–¥–µ–ª–∏...");
<<<<<<< HEAD
        let model_path = MODEL_DOWNLOADER.ensure_model(model_name).await?;
=======
        let model_path = MODEL_DOWNLOADER.ensure_model(&model_name).await?;
>>>>>>> cdac5c55f689e319aa18d538b93d7c8f8759a52c
        info!("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {:?}", model_path);
        
        // –°–æ–∑–¥–∞—ë–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å
        let config = EmbeddingConfig {
            model_name: model_name.clone(),
            use_gpu: true,
            ..Default::default()
        };
        
        info!("üöÄ –°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞...");
        let (service, decision) = SmartEmbeddingFactory::create_optimized(config).await?;
        
        info!("‚úÖ –ú–æ–¥–µ–ª—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞!");
        info!("  - –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {}", if decision.use_gpu { "GPU" } else { "CPU" });
        info!("  - Batch size: {}", decision.recommended_batch_size);
        
        // –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫
        info!("\nüß™ –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫...");
        let test_texts = vec!["Hello, world!".to_string()];
        let start = std::time::Instant::now();
        let _ = service.embed_batch(test_texts).await?;
        let elapsed = start.elapsed();
        
        info!("‚úÖ –¢–µ—Å—Ç —É—Å–ø–µ—à–µ–Ω! –í—Ä–µ–º—è: {:.2} –º—Å", elapsed.as_millis());
        
        Ok(())
    }
}

/// –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞ —Ä–µ—à–µ–Ω–∏—è
trait DecisionExt {
    fn print_decision(&self);
}

impl DecisionExt for ai::auto_device_selector::DeviceDecision {
    fn print_decision(&self) {
        info!("\nü§ñ –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–±–æ—Ä–∞:");
        info!("  - –í—ã–±—Ä–∞–Ω–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {}", if self.use_gpu { "GPU üéÆ" } else { "CPU üíª" });
        info!("  - –ü—Ä–∏—á–∏–Ω–∞: {}", self.reason);
        info!("  - CPU –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {:.1} items/sec", self.cpu_score);
        if let Some(gpu_score) = self.gpu_score {
            info!("  - GPU –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {:.1} items/sec", gpu_score);
            let speedup = gpu_score / self.cpu_score;
            info!("  - –£—Å–∫–æ—Ä–µ–Ω–∏–µ GPU: {:.1}x {}", 
                speedup,
                match speedup {
                    x if x > 10.0 => "üöÄüöÄüöÄ",
                    x if x > 5.0 => "üöÄüöÄ",
                    x if x > 2.0 => "üöÄ",
                    _ => "‚ö°",
                }
            );
        }
        info!("  - –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π batch size: {}", self.recommended_batch_size);
    }
}