//! LLM Communication Service - –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
//!
//! –°–µ—Ä–≤–∏—Å –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–µ–π —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏:
//! - OpenAI API
//! - Anthropic Claude API  
//! - –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
//!
//! –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback –º–µ–∂–¥—É –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏,
//! —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º —Ç–æ–∫–µ–Ω–∞–º–∏, –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –æ—Ç–≤–µ—Ç–æ–≤ –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.

use super::types::{OperationResult, RequestContext};
use anyhow::Result;
use llm::LlmClient;
use std::sync::Arc;
use std::time::Duration;
use tracing::{debug, info, warn};

/// Trait –¥–ª—è —Å–µ—Ä–≤–∏—Å–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å LLM
#[async_trait::async_trait]
pub trait LlmCommunicationService: Send + Sync {
    /// –û—Ç–ø—Ä–∞–≤–∏—Ç—å –ø—Ä–æ—Å—Ç–æ–π —á–∞—Ç –∑–∞–ø—Ä–æ—Å –∫ LLM
    async fn chat(&self, context: &RequestContext) -> Result<OperationResult<String>>;

    /// –û—Ç–ø—Ä–∞–≤–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    async fn chat_with_options(
        &self,
        context: &RequestContext,
        options: &ChatOptions,
    ) -> Result<OperationResult<String>>;

    /// –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
    async fn health_check(&self) -> LlmHealthStatus;

    /// –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–µ
    async fn get_provider_info(&self) -> ProviderInfo;

    /// –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    async fn get_usage_stats(&self) -> LlmUsageStats;

    /// –°–±—Ä–æ—Å–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É (–¥–ª—è —Ç–µ—Å—Ç–æ–≤)
    async fn reset_stats(&self);
}

/// –û–ø—Ü–∏–∏ –¥–ª—è —á–∞—Ç –∑–∞–ø—Ä–æ—Å–∞
#[derive(Debug, Clone)]
pub struct ChatOptions {
    /// –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
    pub max_tokens: Option<u32>,

    /// –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (0.0 - 2.0)
    pub temperature: Option<f32>,

    /// Top-p sampling parameter (0.0 - 1.0)  
    pub top_p: Option<f32>,

    /// –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
    pub system_prompt: Option<String>,

    /// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –∫—ç—à –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
    pub use_cache: bool,

    /// –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
    pub preferred_provider: Option<String>,

    /// Timeout –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
    pub timeout: Option<Duration>,
}

impl Default for ChatOptions {
    fn default() -> Self {
        Self {
            max_tokens: Some(4000),
            temperature: Some(0.7),
            top_p: Some(0.9),
            system_prompt: None,
            use_cache: true,
            preferred_provider: None,
            timeout: Some(Duration::from_secs(30)),
        }
    }
}

/// –°—Ç–∞—Ç—É—Å –∑–¥–æ—Ä–æ–≤—å—è LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
#[derive(Debug, Clone)]
pub struct LlmHealthStatus {
    pub primary_provider_healthy: bool,
    pub fallback_providers_available: u32,
    pub last_successful_request: Option<chrono::DateTime<chrono::Utc>>,
    pub current_error_rate: f64,
    pub estimated_response_time: Duration,
}

/// –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–µ–∫—É—â–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–µ
#[derive(Debug, Clone)]
pub struct ProviderInfo {
    pub name: String,
    pub model: String,
    pub version: Option<String>,
    pub rate_limits: RateLimits,
    pub capabilities: Vec<String>,
}

/// –õ–∏–º–∏—Ç—ã —Å–∫–æ—Ä–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
#[derive(Debug, Clone)]
pub struct RateLimits {
    pub requests_per_minute: u32,
    pub tokens_per_minute: u32,
    pub current_usage_rpm: u32,
    pub current_usage_tpm: u32,
}

/// –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LLM
#[derive(Debug, Clone)]
pub struct LlmUsageStats {
    pub total_requests: u64,
    pub successful_requests: u64,
    pub failed_requests: u64,
    pub cached_responses: u64,
    pub total_tokens_consumed: u64,
    pub total_tokens_generated: u64,
    pub avg_response_time_ms: f64,
    pub provider_distribution: std::collections::HashMap<String, u64>,
}

/// –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å LLM –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
pub struct DefaultLlmCommunicationService {
    /// –û—Å–Ω–æ–≤–Ω–æ–π LLM –∫–ª–∏–µ–Ω—Ç
    llm_client: LlmClient,

    /// –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    stats: parking_lot::RwLock<LlmUsageStats>,

    /// –ö—ç—à –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –∑–∞–ø—Ä–æ—Å–æ–≤
    response_cache: parking_lot::RwLock<std::collections::HashMap<String, CachedResponse>>,

    /// –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞
    config: LlmCommunicationConfig,
}

/// –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
#[derive(Debug, Clone)]
struct CachedResponse {
    response: String,
    timestamp: chrono::DateTime<chrono::Utc>,
    tokens_used: u32,
    provider: String,
}

/// –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å LLM
#[derive(Debug, Clone)]
pub struct LlmCommunicationConfig {
    /// –í—Ä–µ–º—è –∂–∏–∑–Ω–∏ –∫—ç—à–∞ –æ—Ç–≤–µ—Ç–æ–≤
    pub cache_ttl: Duration,

    /// –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π)
    pub max_cache_size: usize,

    /// –í–∫–ª—é—á–∏—Ç—å –ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π fallback –º–µ–∂–¥—É –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏
    pub enable_provider_fallback: bool,

    /// –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
    pub max_retries: u32,

    /// –ë–∞–∑–æ–≤—ã–π timeout –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤
    pub default_timeout: Duration,

    /// –í–∫–ª—é—á–∏—Ç—å –ª–∏ –¥–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    pub enable_detailed_metrics: bool,
}

impl Default for LlmCommunicationConfig {
    fn default() -> Self {
        Self {
            cache_ttl: Duration::from_secs(30 * 60),
            max_cache_size: 1000,
            enable_provider_fallback: true,
            max_retries: 3,
            default_timeout: Duration::from_secs(30),
            enable_detailed_metrics: true,
        }
    }
}

impl DefaultLlmCommunicationService {
    /// –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
    pub fn new(llm_client: LlmClient) -> Self {
        Self::with_config(llm_client, LlmCommunicationConfig::default())
    }

    /// –°–æ–∑–¥–∞—Ç—å —ç–∫–∑–µ–º–ø–ª—è—Ä —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
    pub fn with_config(llm_client: LlmClient, config: LlmCommunicationConfig) -> Self {
        Self {
            llm_client,
            stats: parking_lot::RwLock::new(LlmUsageStats::default()),
            response_cache: parking_lot::RwLock::new(std::collections::HashMap::new()),
            config,
        }
    }

    /// –°–æ–∑–¥–∞—Ç—å –∫–ª—é—á –∫—ç—à–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–ø—Ä–æ—Å–∞
    fn create_cache_key(&self, message: &str, options: &ChatOptions) -> String {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};

        let mut hasher = DefaultHasher::new();
        message.hash(&mut hasher);
        options.max_tokens.hash(&mut hasher);
        options
            .temperature
            .map(|t| (t * 1000.0) as u32)
            .hash(&mut hasher);
        options.system_prompt.hash(&mut hasher);

        format!("llm_cache_{:x}", hasher.finish())
    }

    /// –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫—ç—à –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –æ—Ç–≤–µ—Ç–∞
    fn check_cache(&self, cache_key: &str) -> Option<String> {
        if !self.config.enable_detailed_metrics {
            return None;
        }

        let cache = self.response_cache.read();
        if let Some(cached) = cache.get(cache_key) {
            let age = chrono::Utc::now() - cached.timestamp;
            if age
                < chrono::Duration::from_std(self.config.cache_ttl)
                    .expect("Operation failed - converted from unwrap()")
            {
                debug!("üì¶ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –∫—ç—à –¥–ª—è LLM –∑–∞–ø—Ä–æ—Å–∞");
                return Some(cached.response.clone());
            }
        }
        None
    }

    /// –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç–≤–µ—Ç –≤ –∫—ç—à
    fn store_in_cache(&self, cache_key: String, response: &str, tokens_used: u32, provider: &str) {
        if !self.config.enable_detailed_metrics {
            return;
        }

        let mut cache = self.response_cache.write();

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞ –∏ –æ—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        if cache.len() >= self.config.max_cache_size {
            let entries: Vec<_> = cache
                .iter()
                .map(|(k, v)| (k.clone(), v.timestamp))
                .collect();
            let mut sorted_entries = entries;
            sorted_entries.sort_by_key(|(_, timestamp)| *timestamp);

            // –£–¥–∞–ª—è–µ–º 20% —Å–∞–º—ã—Ö —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π
            let to_remove = cache.len() / 5;
            for (key, _) in sorted_entries.iter().take(to_remove) {
                cache.remove(key);
            }
        }

        cache.insert(
            cache_key,
            CachedResponse {
                response: response.to_string(),
                timestamp: chrono::Utc::now(),
                tokens_used,
                provider: provider.to_string(),
            },
        );
    }

    /// –û—á–∏—Å—Ç–∏—Ç—å —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –∑–∞–ø–∏—Å–∏ –∫—ç—à–∞
    fn cleanup_cache(&self) {
        let mut cache = self.response_cache.write();
        let now = chrono::Utc::now();
        let ttl = chrono::Duration::from_std(self.config.cache_ttl)
            .expect("Operation failed - converted from unwrap()");

        cache.retain(|_, cached| {
            let age = now - cached.timestamp;
            age < ttl
        });
    }

    /// –û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    fn update_stats(
        &self,
        success: bool,
        from_cache: bool,
        duration: Duration,
        tokens_used: u32,
        provider: &str,
    ) {
        let mut stats = self.stats.write();
        stats.total_requests += 1;

        if success {
            stats.successful_requests += 1;
        } else {
            stats.failed_requests += 1;
        }

        if from_cache {
            stats.cached_responses += 1;
        } else {
            stats.total_tokens_consumed += tokens_used as u64;
        }

        // –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞
        let duration_ms = duration.as_millis() as f64;
        let total = stats.total_requests as f64;
        stats.avg_response_time_ms =
            ((stats.avg_response_time_ms * (total - 1.0)) + duration_ms) / total;

        // –û–±–Ω–æ–≤–ª—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º
        *stats
            .provider_distribution
            .entry(provider.to_string())
            .or_insert(0) += 1;
    }
}

#[async_trait::async_trait]
impl LlmCommunicationService for DefaultLlmCommunicationService {
    async fn chat(&self, context: &RequestContext) -> Result<OperationResult<String>> {
        self.chat_with_options(context, &ChatOptions::default())
            .await
    }

    async fn chat_with_options(
        &self,
        context: &RequestContext,
        options: &ChatOptions,
    ) -> Result<OperationResult<String>> {
        use std::time::Instant;
        use tokio::time::timeout;

        let start_time = Instant::now();
        let cache_key = self.create_cache_key(&context.message, options);

        debug!("üí¨ LLM –∑–∞–ø—Ä–æ—Å: {} —Å–∏–º–≤–æ–ª–æ–≤", context.message.len());

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –µ—Å–ª–∏ –≤–∫–ª—é—á—ë–Ω
        if options.use_cache {
            if let Some(cached_response) = self.check_cache(&cache_key) {
                let duration = start_time.elapsed();
                self.update_stats(true, true, duration, 0, "cache");
                return Ok(OperationResult {
                    result: Ok(cached_response),
                    duration,
                    retries: 0,
                    from_cache: true,
                });
            }
        }

        // –û—á–∏—â–∞–µ–º –∫—ç—à –æ—Ç —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π
        self.cleanup_cache();

        let request_timeout = options.timeout.unwrap_or(self.config.default_timeout);
        let mut attempts = 0;
        let max_attempts = self.config.max_retries + 1;

        while attempts < max_attempts {
            attempts += 1;

            debug!(
                "üîÑ –ü–æ–ø—ã—Ç–∫–∞ {} –∏–∑ {} –æ—Ç–ø—Ä–∞–≤–∫–∏ LLM –∑–∞–ø—Ä–æ—Å–∞",
                attempts, max_attempts
            );

            let llm_future = self.llm_client.chat_simple(&context.message);
            let llm_result = timeout(request_timeout, llm_future).await;

            match llm_result {
                Ok(Ok(response)) => {
                    let duration = start_time.elapsed();

                    // –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)
                    let estimated_tokens = (context.message.len() + response.len()) as u32 / 4;

                    // –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à –µ—Å–ª–∏ –≤–∫–ª—é—á—ë–Ω
                    if options.use_cache {
                        self.store_in_cache(cache_key, &response, estimated_tokens, "llm");
                    }

                    self.update_stats(true, false, duration, estimated_tokens, "llm");

                    info!(
                        "‚úÖ LLM –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –∑–∞ {:?} ({} —Å–∏–º–≤–æ–ª–æ–≤)",
                        duration,
                        response.len()
                    );

                    return Ok(OperationResult {
                        result: Ok(response),
                        duration,
                        retries: attempts - 1,
                        from_cache: false,
                    });
                }
                Ok(Err(e)) => {
                    warn!("‚ö†Ô∏è LLM –∑–∞–ø—Ä–æ—Å failed (–ø–æ–ø—ã—Ç–∫–∞ {}): {}", attempts, e);

                    if attempts >= max_attempts {
                        let duration = start_time.elapsed();
                        self.update_stats(false, false, duration, 0, "llm");
                        return Ok(OperationResult {
                            result: Err(e),
                            duration,
                            retries: attempts - 1,
                            from_cache: false,
                        });
                    }

                    // Exponential backoff –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
                    let delay = Duration::from_millis(1000 * attempts as u64);
                    tokio::time::sleep(delay).await;
                }
                Err(_) => {
                    warn!("‚ö†Ô∏è LLM –∑–∞–ø—Ä–æ—Å timeout (–ø–æ–ø—ã—Ç–∫–∞ {})", attempts);

                    if attempts >= max_attempts {
                        let duration = start_time.elapsed();
                        let error =
                            anyhow::anyhow!("LLM request timeout after {} attempts", max_attempts);
                        self.update_stats(false, false, duration, 0, "llm");
                        return Ok(OperationResult {
                            result: Err(error),
                            duration,
                            retries: attempts - 1,
                            from_cache: false,
                        });
                    }

                    // –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º timeout –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–∏
                    let delay = Duration::from_millis(2000 * attempts as u64);
                    tokio::time::sleep(delay).await;
                }
            }
        }

        // –≠—Ç–æ—Ç –∫–æ–¥ –Ω–µ –¥–æ–ª–∂–µ–Ω –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è, –Ω–æ –¥–ª—è –∫–æ–º–ø–∏–ª—è—Ç–æ—Ä–∞
        unreachable!("Loop should have returned earlier")
    }

    async fn health_check(&self) -> LlmHealthStatus {
        // –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è - –ø–æ–ø—Ä–æ–±—É–µ–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        let test_context = RequestContext {
            message: "ping".to_string(),
            user_id: None,
            session_id: None,
            timestamp: chrono::Utc::now(),
            metadata: std::collections::HashMap::new(),
        };

        let test_options = ChatOptions {
            max_tokens: Some(10),
            use_cache: false,
            timeout: Some(Duration::from_secs(5)),
            ..Default::default()
        };

        let start_time = std::time::Instant::now();
        let health_result = self.chat_with_options(&test_context, &test_options).await;
        let response_time = start_time.elapsed();

        let stats = self.stats.read();
        let error_rate = if stats.total_requests > 0 {
            stats.failed_requests as f64 / stats.total_requests as f64
        } else {
            0.0
        };

        LlmHealthStatus {
            primary_provider_healthy: health_result.is_ok()
                && health_result
                    .expect("Operation failed - converted from unwrap()")
                    .result
                    .is_ok(),
            fallback_providers_available: if self.config.enable_provider_fallback {
                1
            } else {
                0
            },
            last_successful_request: if stats.successful_requests > 0 {
                Some(chrono::Utc::now())
            } else {
                None
            },
            current_error_rate: error_rate,
            estimated_response_time: response_time,
        }
    }

    async fn get_provider_info(&self) -> ProviderInfo {
        ProviderInfo {
            name: "LlmClient".to_string(),
            model: "gpt-3.5-turbo".to_string(), // –≠—Ç–æ –Ω—É–∂–Ω–æ –ø–æ–ª—É—á–∞—Ç—å –∏–∑ LlmClient
            version: Some("1.0".to_string()),
            rate_limits: RateLimits {
                requests_per_minute: 3000,
                tokens_per_minute: 90000,
                current_usage_rpm: 0, // TODO: —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ
                current_usage_tpm: 0,
            },
            capabilities: vec![
                "chat".to_string(),
                "function_calling".to_string(),
                "streaming".to_string(),
            ],
        }
    }

    async fn get_usage_stats(&self) -> LlmUsageStats {
        let stats = self.stats.read();
        stats.clone()
    }

    async fn reset_stats(&self) {
        let mut stats = self.stats.write();
        *stats = LlmUsageStats::default();

        let mut cache = self.response_cache.write();
        cache.clear();

        debug!("üîÑ LLM stats and cache reset");
    }
}

impl Default for LlmUsageStats {
    fn default() -> Self {
        Self {
            total_requests: 0,
            successful_requests: 0,
            failed_requests: 0,
            cached_responses: 0,
            total_tokens_consumed: 0,
            total_tokens_generated: 0,
            avg_response_time_ms: 0.0,
            provider_distribution: std::collections::HashMap::new(),
        }
    }
}

/// Factory —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è DI –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
pub fn create_llm_communication_service(llm_client: LlmClient) -> Arc<dyn LlmCommunicationService> {
    Arc::new(DefaultLlmCommunicationService::new(llm_client))
}

/// Factory —Ñ—É–Ω–∫—Ü–∏—è —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
pub fn create_llm_communication_service_with_config(
    llm_client: LlmClient,
    config: LlmCommunicationConfig,
) -> Arc<dyn LlmCommunicationService> {
    Arc::new(DefaultLlmCommunicationService::with_config(
        llm_client, config,
    ))
}

#[cfg(test)]
mod tests {
    use super::*;
    use chrono::Utc;

    fn create_test_context(message: &str) -> RequestContext {
        RequestContext {
            message: message.to_string(),
            user_id: Some("test_user".to_string()),
            session_id: Some("test_session".to_string()),
            timestamp: Utc::now(),
            metadata: std::collections::HashMap::new(),
        }
    }

    #[test]
    fn test_cache_key_generation() {
        let llm_client = LlmClient::from_env().expect("Operation failed - converted from unwrap()");
        let service = DefaultLlmCommunicationService::new(llm_client);

        let options1 = ChatOptions::default();
        let options2 = ChatOptions {
            temperature: Some(0.5),
            ..Default::default()
        };

        let key1 = service.create_cache_key("test message", &options1);
        let key2 = service.create_cache_key("test message", &options2);
        let key3 = service.create_cache_key("test message", &options1);

        // –û–¥–∏–Ω–∞–∫–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–æ–ª–∂–Ω—ã –¥–∞–≤–∞—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π –∫–ª—é—á
        assert_eq!(key1, key3);

        // –†–∞–∑–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–æ–ª–∂–Ω—ã –¥–∞–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –∫–ª—é—á–∏
        assert_ne!(key1, key2);
    }

    #[test]
    fn test_cache_cleanup() {
        let llm_client = LlmClient::from_env().expect("Operation failed - converted from unwrap()");
        let config = LlmCommunicationConfig {
            cache_ttl: Duration::from_millis(1), // –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–π TTL –¥–ª—è —Ç–µ—Å—Ç–∞
            ..Default::default()
        };
        let service = DefaultLlmCommunicationService::with_config(llm_client, config);

        // –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å –≤ –∫—ç—à
        service.store_in_cache(
            "test_key".to_string(),
            "test_response",
            100,
            "test_provider",
        );

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∑–∞–ø–∏—Å—å –µ—Å—Ç—å
        assert!(service.check_cache("test_key").is_some());

        // –ñ–¥—ë–º –∏—Å—Ç–µ—á–µ–Ω–∏—è TTL
        std::thread::sleep(Duration::from_millis(5));

        // –ó–∞–ø—É—Å–∫–∞–µ–º –æ—á–∏—Å—Ç–∫—É –∫—ç—à–∞
        service.cleanup_cache();

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∑–∞–ø–∏—Å—å —É–¥–∞–ª–µ–Ω–∞
        assert!(service.check_cache("test_key").is_none());
    }

    #[tokio::test]
    async fn test_stats_tracking() {
        let llm_client = LlmClient::from_env().expect("Operation failed - converted from unwrap()");
        let service = DefaultLlmCommunicationService::new(llm_client);

        // –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        service.reset_stats().await;

        // –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑
        service.update_stats(true, false, Duration::from_millis(100), 50, "test_provider");
        service.update_stats(true, true, Duration::from_millis(50), 0, "cache");
        service.update_stats(false, false, Duration::from_millis(200), 0, "test_provider");

        let stats = service.get_usage_stats().await;

        assert_eq!(stats.total_requests, 3);
        assert_eq!(stats.successful_requests, 2);
        assert_eq!(stats.failed_requests, 1);
        assert_eq!(stats.cached_responses, 1);
        assert_eq!(stats.total_tokens_consumed, 50);

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è
        let expected_avg = (100.0 + 50.0 + 200.0) / 3.0;
        assert!((stats.avg_response_time_ms - expected_avg).abs() < 1.0);

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
        assert_eq!(stats.provider_distribution.get("test_provider"), Some(&2));
        assert_eq!(stats.provider_distribution.get("cache"), Some(&1));
    }

    #[tokio::test]
    async fn test_health_check() {
        let llm_client = LlmClient::from_env().expect("Operation failed - converted from unwrap()");
        let service = DefaultLlmCommunicationService::new(llm_client);

        let health = service.health_check().await;

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–∑–æ–≤—ã–µ –ø–æ–ª—è
        assert!(health.estimated_response_time > Duration::from_millis(0));
        assert!(health.current_error_rate >= 0.0);
        assert!(health.current_error_rate <= 1.0);
    }
}
