---
description: "LLM Integration: КРИТИЧЕСКИЙ БЛОКЕР - подключение к OpenAI для step 'think'"
globs: ["**/llm/**/*.rs", "**/config/**/*.toml"]
alwaysApply: true
---

## БЛОКЕР: НЕТ подключения к LLM!

### Приоритет 1: OpenAI клиент
```rust
// llm/src/openai_client.rs
use reqwest::Client;
use serde_json::{json, Value};

pub struct OpenAIClient {
    client: Client,
    api_key: String,
    base_url: String,
}

impl OpenAIClient {
    pub fn new(api_key: String) -> Self {
        Self {
            client: Client::new(),
            api_key,
            base_url: "https://api.openai.com/v1".to_string(),
        }
    }
    
    pub async fn chat_completion(&self, messages: Vec<Value>) -> Result<String> {
        let response = self.client
            .post(&format!("{}/chat/completions", self.base_url))
            .header("Authorization", format!("Bearer {}", self.api_key))
            .json(&json!({
                "model": "gpt-4",
                "messages": messages,
                "temperature": 0.7,
                "max_tokens": 2000
            }))
            .send()
            .await?;
            
        let result: Value = response.json().await?;
        Ok(result["choices"][0]["message"]["content"]
            .as_str()
            .unwrap_or("")
            .to_string())
    }
}
```

### Приоритет 2: Think Step реализация
```rust
// llm/src/think_service.rs
pub struct ThinkService {
    client: OpenAIClient,
    context_window: usize,
}

impl ThinkService {
    pub async fn think(&self, input: &Value) -> Result<Value> {
        let task = input["task"].as_str().unwrap_or("");
        let context = input["context"].as_str().unwrap_or("");
        
        let messages = vec![
            json!({
                "role": "system",
                "content": "Ты - планирующий агент CLI системы. Анализируй задачу и предложи конкретные шаги."
            }),
            json!({
                "role": "user", 
                "content": format!("Задача: {}\nКонтекст: {}", task, context)
            })
        ];
        
        let reasoning = self.client.chat_completion(messages).await?;
        
        Ok(json!({
            "reasoning": reasoning,
            "confidence": 0.8,
            "tokens_used": reasoning.len() / 4, // примерная оценка
            "model": "gpt-4"
        }))
    }
}
```

### Приоритет 3: Интеграция с Executor
```rust
// В executor/src/lib.rs - ЗАМЕНИТЬ ЗАГЛУШКУ!
Some("think") => {
    // БЫЛО: заглушка
    // let thought = serde_json::json!({"reasoning": "Размышление о задаче"});
    
    // СТАЛО: реальный LLM вызов
    let think_result = self.llm_service.think(&node.input).await?;
    
    // Сохранить результат в память
    self.memory.store_result(&node.id, &think_result)?;
    
    // Обновить контекст для следующих шагов
    self.context.insert("last_reasoning".to_string(), think_result);
}
```

### Приоритет 4: Конфигурация
```toml
# config/config.example.toml
[llm]
provider = "openai"
api_key = "${OPENAI_API_KEY}"
model = "gpt-4"
temperature = 0.7
max_tokens = 2000
timeout_seconds = 30

[llm.fallback]
provider = "anthropic" 
model = "claude-3-sonnet"
```

## Обработка ошибок
```rust
pub enum LLMError {
    ApiKeyMissing,
    RateLimitExceeded,
    ContextTooLong,
    NetworkError(reqwest::Error),
    ParseError(serde_json::Error),
}

impl ThinkService {
    async fn think_with_retry(&self, input: &Value, max_retries: u32) -> Result<Value> {
        // Exponential backoff для rate limits
        // Fallback на другую модель при ошибках
    }
}
```

## Тестирование
```rust
#[tokio::test]
async fn test_openai_integration() {
    let client = OpenAIClient::new("test-key".to_string());
    // Mock HTTP responses для тестирования
}

#[tokio::test] 
async fn test_think_service() {
    let service = ThinkService::new(mock_client);
    let input = json!({"task": "analyze code", "context": "rust project"});
    let result = service.think(&input).await.unwrap();
    assert!(result["reasoning"].is_string());
}
```

## Критерии готовности
- ✅ OpenAI клиент подключается и делает запросы
- ✅ Think step возвращает реальные рассуждения
- ✅ Executor использует LLM вместо заглушки
- ✅ Конфигурация через environment variables
- ✅ Обработка ошибок и retry логика
- ✅ Покрытие тестами ≥75%

## Мониторинг
- Количество LLM вызовов
- Средняя latency
- Стоимость токенов
- Rate limit hits
- Качество ответов (через feedback)